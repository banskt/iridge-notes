{
  
    
        "post0": {
            "title": "Comparison of prediction accuracy of EM-VAMP, Mr.ASH and EBMR",
            "content": "About . Here, I compare the prediction accuracy of EM-VAMP and EM-iRidge with existing penalized regression method. The simulation scenarios were used earlier for comparing few well-known penalized regression methods. . EM-VAMP. Proposed by Fletcher and Schniter, 2017, this algorithm combines Vector Approximate Message Passing (VAMP) and Expectation Maximization and is well suited for sparse linear regression. I used vampyre for the implementation. . | EM-iRidge. Proposed by Matthew Stephens, 2020, this algorithm uses iterative ridge regression to solve linear regression where the prior of the coefficients is given by a product of two normal distributions, which has sparsity inducing properties. I implemented an Expectation Maximization algorithm for iRidge in ebmrPy. . | EBMR (DExp). Proposed by Matthew Stephens, 2020, this algorithm uses a hierarchical Empirical Bayes regression, where the prior variance of the coefficient depends hierarchically on another distribution. . | . The simulation pipeline is implemented using Dynamic Statistical Comparisons (DSC). . Importing packages and DSC results . Non-standard packages include DSC and PyMir. The simulation repository needs to be in path for importing some of the utilities. . import pandas as pd import numpy as np import math import os import sys import collections srcdir = &quot;/home/saikat/Documents/work/ebmr/simulation/eb-linreg-dsc&quot; sys.path.append(os.path.join(srcdir, &quot;analysis&quot;)) import dscrutils2py as dscrutils import methodprops import methodplots import matplotlib.pyplot as plt import matplotlib.gridspec as gridspec from pymir import mpl_stylesheet from pymir import mpl_utils from pymir import pd_utils mpl_stylesheet.banskt_presentation() . . I have run the simulations using the following settings. . highdims = (100, 200) lowdims = (500, 200) # fraction of non-zero predictors, sfrac = p_causal / p sfracs = [0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1.0] # PVE pve_list = [0.5, 0.95] # rho rho_list = [0, 0.95] # Output directory dsc_outdir = os.path.join(srcdir, &quot;dsc/dsc_result&quot;) . Read the results of the simulation and store it in a DataFrame . targets = [&quot;simulate&quot;, &quot;simulate.dims&quot;, &quot;simulate.se&quot;, &quot;simulate.rho&quot;, &quot;simulate.sfrac&quot;, &quot;simulate.pve&quot;, &quot;fit&quot;, &quot;fit.DSC_TIME&quot;, &quot;mse.err&quot;] dscout = dscrutils.dscquery(dsc_outdir, targets) dscout[&#39;score1&#39;] = np.sqrt(dscout[&#39;mse.err&#39;])/dscout[&#39;simulate.se&#39;] . . Calling: dsc-query /home/saikat/Documents/work/ebmr/simulation/eb-linreg-dsc/dsc/dsc_result -o /tmp/RtmpIuxNqU/file66a2136fde94.csv --target &#34;simulate simulate.dims simulate.se simulate.rho simulate.sfrac simulate.pve fit fit.DSC_TIME mse.err&#34; --force Loaded dscquery output table with 21280 rows and 12 columns. . dscout . DSC simulate simulate.dims simulate.se simulate.rho simulate.sfrac simulate.pve fit fit.DSC_TIME mse.err score1 . 0 1 | indepgauss | (500,200) | 3.462448 | 0.00 | 0.010 | 0.50 | l0learn | 0.417 | 12.563997 | 1.023719 | . 1 1 | indepgauss | (100,200) | 0.861522 | 0.00 | 0.010 | 0.50 | l0learn | 0.872 | 0.654125 | 0.938780 | . 2 1 | indepgauss | (500,200) | 1.905777 | 0.00 | 0.025 | 0.50 | l0learn | 0.407 | 3.574130 | 0.992003 | . 3 1 | indepgauss | (100,200) | 1.249624 | 0.00 | 0.025 | 0.50 | l0learn | 0.800 | 1.806580 | 1.075596 | . 4 1 | indepgauss | (500,200) | 3.063747 | 0.00 | 0.050 | 0.50 | l0learn | 0.372 | 9.284629 | 0.994556 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 21275 20 | equicorrgauss | (100,200) | 2.003817 | 0.95 | 0.250 | 0.95 | mcp | 0.839 | 10.202592 | 1.594033 | . 21276 20 | equicorrgauss | (500,200) | 0.856860 | 0.95 | 0.500 | 0.95 | mcp | 17.814 | 1.525528 | 1.441453 | . 21277 20 | equicorrgauss | (100,200) | 0.841918 | 0.95 | 0.500 | 0.95 | mcp | 0.866 | 6.398867 | 3.004566 | . 21278 20 | equicorrgauss | (500,200) | 0.964054 | 0.95 | 1.000 | 0.95 | mcp | 18.341 | 5.848087 | 2.508451 | . 21279 20 | equicorrgauss | (100,200) | 2.148883 | 0.95 | 1.000 | 0.95 | mcp | 0.789 | 15.720326 | 1.845092 | . 21280 rows × 11 columns . Select the methods to be displayed in the figures. . whichmethods = [#&quot;l0learn&quot;, &quot;lasso&quot;, &quot;ridge&quot;, &quot;elastic_net&quot;, #&quot;scad&quot;, #&quot;mcp&quot;, #&quot;blasso&quot;, #&quot;bayesb&quot;, #&quot;susie&quot;, #&quot;varbvs&quot;, #&quot;varbvsmix&quot;, &quot;mr_ash&quot;, &quot;em_vamp&quot;, &quot;em_iridge&quot;, &quot;ebmr_lasso&quot;, #&quot;ebmr_ash&quot;, ] . High dimension setting (p &gt; n) . Simulations were performed with n = 100, p = 200. Some of the EM-VAMP optimizations did not converge. Therefore, I used median of the prediction scores (instead of mean). . highdim_condition = [f&quot;$(simulate.dims) == &#39;({highdims[0]},{highdims[1]})&#39;&quot;] resdf1 = pd_utils.select_dfrows(dscout, highdim_condition) methodplots.create_figure_prediction_error(whichmethods, resdf1, highdims, rho_list, pve_list, sfracs, use_median = True) . . Low dimension setting (p &lt; n) . Simulations were performed with n = 500, p = 200. . lowdim_condition = [f&quot;$(simulate.dims) == &#39;({lowdims[0]},{lowdims[1]})&#39;&quot;] resdf2 = pd_utils.select_dfrows(dscout, lowdim_condition) methodplots.create_figure_prediction_error(whichmethods, resdf2, lowdims, rho_list, pve_list, sfracs, use_median = True) . . Convergence of EM-VAMP . EM-VAMP fails to converge in some simulations. Here, I look at the distribution of prediction errors of EM-VAMP over all the 20 simulations at different settings. . method = &quot;em_vamp&quot; ncol = 4 nrow = 2 wspace = 0.2 hspace = 1.5 axwidth = 3 aspect = 0.7 xscale = &#39;log10&#39; yscale = &#39;log10&#39; nan_pos = -1.5 figw = ncol * axwidth axheight = axwidth * aspect figh = nrow * axheight + hspace * (nrow - 1) * axheight fig = plt.figure(figsize = (figw, figh)) gs = gridspec.GridSpec(nrow, ncol) gs.update(wspace = wspace, hspace = hspace) pm = methodprops.plot_metainfo()[method] figtitle = f&quot;{pm.label}&quot; xlab_offset = - wspace / 2 if ncol % 2 == 0 else 0.5 ylab_offset = (1 + hspace / 2) if nrow % 2 == 0 else 0.5 axrow = list() for i, dim in enumerate([highdims, lowdims]): xvals = [max(1, int(x * dim[1])) for x in sfracs] axlist = list() allscores = list() resdf = pd_utils.select_dfrows(dscout, [f&quot;$(simulate.dims) == &#39;({dim[0]},{dim[1]})&#39;&quot;]) for j, rho in enumerate(rho_list): for k, pve in enumerate(pve_list): colnum = j * 2 + k if len(axlist) == 0: ax = fig.add_subplot(gs[i, colnum]) ax.text(0, 1.3, f&quot;n = {dim[0]}&quot;, va = &#39;bottom&#39;, ha = &#39;left&#39;, transform = ax.transAxes) else: ax = fig.add_subplot(gs[i, colnum], sharey = axlist[0]) ax.text(0.5, 1.05, f&quot;pve = {pve:g}, &quot; + r&quot;$ rho$ = {:g}&quot;.format(rho), va = &#39;bottom&#39;, ha = &#39;center&#39;, transform = ax.transAxes) # Main plot mconditions = [f&quot;$(fit) == {method}&quot;] mconditions += [f&quot;$(simulate.rho) == {rho}&quot;] mconditions += [f&quot;$(simulate.pve) == {pve}&quot;] for s, sfrac in enumerate(sfracs): scondition = [f&quot;$(simulate.sfrac) == {sfrac}&quot;] dfselect = pd_utils.select_dfrows(resdf, mconditions + scondition) scores = dfselect[&#39;score1&#39;].to_numpy() num_nan = np.sum(np.isnan(scores)) xpos = mpl_utils.scale_list([xvals[s]], scale = xscale) yvals = mpl_utils.scale_array(scores[~np.isnan(scores)], scale = yscale) ax.scatter(xpos * len(yvals), yvals, alpha = 0.5) ax.text(xpos[0], nan_pos, f&quot;{num_nan}&quot;, ha=&#39;center&#39;, va=&#39;bottom&#39;) # Tick marks and axes decoration mpl_utils.set_xticks(ax, scale = xscale, tickmarks = xvals, rotation = 90) ax.tick_params(labelcolor = &quot;#333333&quot;, left = False) if len(axlist) &gt; 0: ax.tick_params(labelleft = False) mpl_utils.decorate_axes(ax, hide = [&quot;left&quot;, &quot;right&quot;, &quot;top&quot;], ticklimits = True, pads = [34, 10]) mpl_utils.set_xticks(ax, scale = xscale, tickmarks = xvals, rotation = 90) for side, border in ax.spines.items(): if side == &quot;top&quot;: border.set_visible(True) ax.grid(which = &#39;major&#39;, axis = &#39;y&#39;, ls = &#39;dotted&#39;) axlist.append(ax) &#39;&#39;&#39; Following indices are now hard-coded &#39;&#39;&#39; axlist[2].set_xlabel(r&quot;Number of non-zero coefficients (s)&quot;, x = xlab_offset) mpl_utils.set_yticks(axlist[0], scale = yscale, spacing = &#39;log10&#39;) axlist[0].text(0, nan_pos, f&#39;nan&#39;, ha=&#39;right&#39;, va=&#39;bottom&#39;) axrow.append(axlist) axrow[1][0].set_ylabel(r&quot;Prediction Error (RMSE / $ sigma$)&quot;, y = ylab_offset) axrow[0][2].set_title(figtitle, x = xlab_offset, pad = 40) plt.show() . .",
            "url": "https://banskt.github.io/iridge-notes/2021/03/29/compare-prediction-accuracy-vamp-mrash-ebmr.html",
            "relUrl": "/2021/03/29/compare-prediction-accuracy-vamp-mrash-ebmr.html",
            "date": " • Mar 29, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Comparison of prediction accuracy of linear regression methods",
            "content": "About . Here, I compare the prediction accuracy of several linear regression methods using simulation examples of Kim, Wang, Carbonetto and Stephens (KWCS 2020). It reproduces four simulation scenarios (see below) from Figure 2 and 3 of their manuscript and adds four other scenarios. I compare the combinations of signal strength (PVE $= 0.5$ and $0.95$) and correlation of the predictors ($ rho = 0$ and $0.95$) with sample size $n = 100$ ($&lt; p = 200$) and $n = 500$ ($&gt; p$), which gives the 8 different scenarios presented here. . The four simulation scenarios which are reproduced from (KWCS 2020) are: . Low-dimension ($n = 500$, $ rho = 0$, PVE $= 0.5$) | High-dimension ($n = 100$, $ rho = 0$, PVE $= 0.5$) | Strong-signal ($n = 500$, $ rho = 0$, PVE $= 0.95$) | EquiCorrGauss ($n = 100$, $ rho = 0.95$, PVE $= 0.5$) | The setting where Fabio Morgante found Lasso to be better than Mr.ASH would be equivalent to ($n = 100$, $ rho = 0$, PVE $= 0.95$). . The simulation pipeline is implemented using Dynamic Statistical Comparisons (DSC). . Importing packages . Non-standard packages include DSC and PyMir. The simulation repository needs to be in path for importing some of the utilities. . import pandas as pd import numpy as np import math import os import sys import collections srcdir = &quot;/home/saikat/Documents/work/ebmr/simulation/eb-linreg-dsc&quot; sys.path.append(os.path.join(srcdir, &quot;analysis&quot;)) import dscrutils2py as dscrutils import methodprops import methodplots import matplotlib.pyplot as plt import matplotlib.gridspec as gridspec from pymir import mpl_stylesheet from pymir import mpl_utils from pymir import pd_utils mpl_stylesheet.banskt_presentation() . . DSC results . I have run the simulations using the following settings. . highdims = (100, 200) lowdims = (500, 200) # fraction of non-zero predictors, sfrac = p_causal / p sfracs = [0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1.0] # PVE pve_list = [0.5, 0.95] # rho rho_list = [0, 0.95] # Output directory dsc_outdir = os.path.join(srcdir, &quot;dsc/dsc_result&quot;) . Read the results of the simulation and store it in a DataFrame . targets = [&quot;simulate&quot;, &quot;simulate.dims&quot;, &quot;simulate.se&quot;, &quot;simulate.rho&quot;, &quot;simulate.sfrac&quot;, &quot;simulate.pve&quot;, &quot;fit&quot;, &quot;fit.DSC_TIME&quot;, &quot;mse.err&quot;] dscout = dscrutils.dscquery(dsc_outdir, targets) dscout[&#39;score1&#39;] = np.sqrt(dscout[&#39;mse.err&#39;])/dscout[&#39;simulate.se&#39;] . . Calling: dsc-query /home/saikat/Documents/work/ebmr/simulation/eb-linreg-dsc/dsc/dsc_result -o /tmp/RtmpRh1NxB/file66416d28fb15.csv --target &#34;simulate simulate.dims simulate.se simulate.rho simulate.sfrac simulate.pve fit fit.DSC_TIME mse.err&#34; --force Loaded dscquery output table with 21280 rows and 12 columns. . dscout . DSC simulate simulate.dims simulate.se simulate.rho simulate.sfrac simulate.pve fit fit.DSC_TIME mse.err score1 . 0 1 | indepgauss | (500,200) | 3.462448 | 0.00 | 0.010 | 0.50 | l0learn | 0.417 | 12.563997 | 1.023719 | . 1 1 | indepgauss | (100,200) | 0.861522 | 0.00 | 0.010 | 0.50 | l0learn | 0.872 | 0.654125 | 0.938780 | . 2 1 | indepgauss | (500,200) | 1.905777 | 0.00 | 0.025 | 0.50 | l0learn | 0.407 | 3.574130 | 0.992003 | . 3 1 | indepgauss | (100,200) | 1.249624 | 0.00 | 0.025 | 0.50 | l0learn | 0.800 | 1.806580 | 1.075596 | . 4 1 | indepgauss | (500,200) | 3.063747 | 0.00 | 0.050 | 0.50 | l0learn | 0.372 | 9.284629 | 0.994556 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 21275 20 | equicorrgauss | (100,200) | 2.003817 | 0.95 | 0.250 | 0.95 | mcp | 0.839 | 10.202592 | 1.594033 | . 21276 20 | equicorrgauss | (500,200) | 0.856860 | 0.95 | 0.500 | 0.95 | mcp | 17.814 | 1.525528 | 1.441453 | . 21277 20 | equicorrgauss | (100,200) | 0.841918 | 0.95 | 0.500 | 0.95 | mcp | 0.866 | 6.398867 | 3.004566 | . 21278 20 | equicorrgauss | (500,200) | 0.964054 | 0.95 | 1.000 | 0.95 | mcp | 18.341 | 5.848087 | 2.508451 | . 21279 20 | equicorrgauss | (100,200) | 2.148883 | 0.95 | 1.000 | 0.95 | mcp | 0.789 | 15.720326 | 1.845092 | . 21280 rows × 11 columns . Select the methods to be displayed in the figures. . whichmethods = [&quot;l0learn&quot;, &quot;lasso&quot;, #&quot;lasso_1se&quot;, &quot;ridge&quot;, &quot;elastic_net&quot;, #&quot;elastic_net_1se&quot;, &quot;scad&quot;, &quot;mcp&quot;, &quot;blasso&quot;, &quot;bayesb&quot;, &quot;susie&quot;, &quot;varbvs&quot;, &quot;varbvsmix&quot;, &quot;mr_ash&quot;] . High dimension setting (p &gt; n) . Simulations were performed with n = 100, p = 200. . highdim_condition = [f&quot;$(simulate.dims) == &#39;({highdims[0]},{highdims[1]})&#39;&quot;] resdf1 = pd_utils.select_dfrows(dscout, highdim_condition) methodplots.create_figure_prediction_error(whichmethods, resdf1, highdims, rho_list, pve_list, sfracs) . . Low dimension setting (p &lt; n) . Simulations were performed with n = 500, p = 200. . lowdim_condition = [f&quot;$(simulate.dims) == &#39;({lowdims[0]},{lowdims[1]})&#39;&quot;] resdf2 = pd_utils.select_dfrows(dscout, lowdim_condition) methodplots.create_figure_prediction_error(whichmethods, resdf2, lowdims, rho_list, pve_list, sfracs) . . Computational time . Shown for one particular simulation scenario. Needs to be extended for varying $n$ and $p$. . fig = plt.figure() ax1 = fig.add_subplot(111) # Choose settings dims = lowdims pve = 0.5 rho = 0 sfrac = 0.5 data = resdf2.copy() if dims == lowdims else resdf1.copy() s = max(1, int(sfrac * dims[1])) # Plot methodplots.single_plot_computational_time(ax1, data, &quot;fit.DSC_TIME&quot;, whichmethods, pve, rho, dims, sfrac) ax1.set_xlabel(&quot;Computational Time (in seconds)&quot;) ax1.set_title(r&quot;($n=$&quot; + f&quot;{dims[0]}, &quot; + r&quot;$p=$&quot; + f&quot;{dims[1]}, &quot; + r&quot;$s=$&quot; + f&quot;{s}, &quot; + f&quot;PVE = {pve}, &quot; + r&quot;$ rho = $&quot; + f&quot;{rho})&quot;, pad = 40) plt.show() . .",
            "url": "https://banskt.github.io/iridge-notes/2021/03/24/compare-prediction-accuracy-linear-regression-methods-dsc.html",
            "relUrl": "/2021/03/24/compare-prediction-accuracy-linear-regression-methods-dsc.html",
            "date": " • Mar 24, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Multiple regression with product of normals",
            "content": "About . I am trying to understand the problem in the VEB implementation of multiple regression with product of normals. I checked the analysis with single predictor ($p=1$) (link) and with two predictors ($p=2$) (link). Here, I will check with $n$ predictors with effect sizes sampled from a known prior. . import numpy as np import pandas as pd from scipy import linalg as sc_linalg from scipy import special as sc_special import matplotlib.pyplot as plt import mpl_stylesheet import mpl_utils from matplotlib import cm from matplotlib import ticker as plticker from mpl_toolkits.axes_grid1 import make_axes_locatable mpl_stylesheet.banskt_presentation(fontfamily = &#39;latex-clearsans&#39;, fontsize = 18, colors = &#39;banskt&#39;, dpi = 72) . . Toy model . I defined a toy model with 50 predictors and 100 samples. I sampled the predictor from the standard normal distribution $ mathcal{N}(0, 1)$. The model used is described in the writeup. . I chose $ sigma = 50.0$, $ sigma_b = 2.0$ and $ sigma_w = 4.0$. . def variance_explained(y, ypred): ss_err = np.sum(np.square(y - ypred)) ss_tot = np.sum(np.square(y - np.mean(y))) r2 = 1 - (ss_err / ss_tot) return r2 def prod_norm_prior_pdf(z, s1, s2): x = np.abs(z) / s1 / s2 prob = sc_special.kn(0, x) / (np.pi * s1 * s2) return prob def normal_logdensity_onesample(z, m, s2): logdensity = - 0.5 * np.log(2 * np.pi * s2) - 0.5 * (z - m) * (z - m) / s2 return logdensity def normal_logdensity(y, mean, sigma2): n = y.shape[0] logdensity = - 0.5 * n * np.log(2 * np.pi * sigma2) - 0.5 * np.sum(np.square(y - mean)) / sigma2 return logdensity def simulate_data(n, p, s, sb, sw, seed = 200): np.random.seed(seed) b = np.random.normal(0, sb, p) w = np.random.normal(0, sw, p) bw = np.multiply(b, w) X = np.zeros((n, p)) for i in range(p): X[:, i] = np.random.normal(0, 1, n) err = np.random.normal(0, s, n) y = np.dot(X, bw) + err return X, y, b, w, bw def print_matrix(matrix, fmt): print(&#39; n&#39;.join([&#39; t&#39;.join([fmt.format(cell) for cell in row]) for row in matrix])) . . nsample = 50 npred = 100 sigbtrue = 2.0 sigwtrue = 4.0 sigtrue = 50.0 X, y, btrue, wtrue, bwtrue = simulate_data(nsample, npred, sigtrue, sigbtrue, sigwtrue, seed = 300) ypredtrue = np.dot(X, bwtrue) r2 = variance_explained(y, ypredtrue) bwprior = np.multiply(np.random.normal(0, sigbtrue, 1000), np.random.normal(0, sigwtrue, 1000)) print(f&quot;Fraction of variance explained by X: {r2:.2f}&quot;) . . Fraction of variance explained by X: 0.77 . fig = plt.figure(figsize = (12, 6)) ax1 = fig.add_subplot(121) ax2 = fig.add_subplot(122) bwvals = np.linspace(-40, 40, 1000) numerical_prior_1d = prod_norm_prior_pdf(bwvals, sigbtrue, sigwtrue) ax1.scatter(ypredtrue, y) mpl_utils.plot_diag(ax1) ax1.text(0.02, 0.94, f&quot;s_b = {sigbtrue}, s_w = {sigwtrue:.2f}&quot;, transform = ax1.transAxes) #ax1.set_title(&quot;y = Xbw + e&quot;, pad = 20.0) ax1.set_xlabel(&quot;Xbw&quot;) ax1.set_ylabel(&quot;y&quot;) ax2.hist(bwprior, bins = 50, density = True, alpha = 0.4, label=&quot;Simulation&quot;) ax2.plot(bwvals, numerical_prior_1d, label = &quot;Analytical&quot;) ax2.legend() ax2.set_title(&quot;Prior distribution&quot;, pad = 20.0) ax2.set_xlabel(&quot;bw&quot;) ax2.set_ylabel(&quot;Density&quot;) plt.tight_layout() plt.show() . . VEB optimization . When I fix the hyperparameters $ sigma$, $ sigma_b$ and $ sigma_w$ to true values, then the variational parameters $m_b$, $s_b$, $m_w$ and $s_w$ are reasonably obtained. Further, if I only fix $ sigma$ then the results are slightly worse but still we recover the signal. However, there is significant underfitting if I try to optimize both the hyperparameters and variational parameters. . I also wanted to check what happens if I use $q(b, w) = q(b mid w) q(w)$ instead of mean field approximation. As a quick check, I hold $b$ and $w$ fixed to their means $m_b$ and $m_w$ while updating the alternate parameter. There is significant overfitting. . Note: This is not exact, so I have to check and write the equations for $q(b, w) = q(b mid w) q(w)$. . def get_elbo(X, XTX, XTy, yTy, s2, sb2, sw2, mb, mw, covb, covw): n, p = X.shape elbo = cfunc(n, p, yTy, s2, sb2, sw2, covb, covw) - hfunc(mb, covb, sb2) - efunc(XTX, XTy, mb, mw, covb, covw, s2, sw2) return elbo def cfunc(n, p, yTy, s2, sb2, sw2, covb, covw): sign, det_covb = np.linalg.slogdet(covb) sign, det_covw = np.linalg.slogdet(covw) val = 0.5 * p val += - 0.5 * p * np.log(sb2) - 0.5 * p * np.log(sw2) val += - 0.5 * n * np.log(2.0 * np.pi * s2) val += 0.5 * det_covb + 0.5 * det_covw val += 0.5 * yTy / s2 return val def hfunc(m, cov, s2): h = np.sum(np.square(m)) + cov.trace() return 0.5 * h / s2 def efunc(XTX, XTy, mb, mw, covb, covw, sigma2, sigmaw2): p = mb.shape[0] mmTb = np.einsum(&#39;i,j-&gt;ij&#39;, mb, mb) EBTXTXB = np.multiply(XTX, mmTb + covb) Rb = (EBTXTXB / sigma2) + (np.eye(p) / sigmaw2) vb = np.einsum(&#39;i,i-&gt;i&#39;, mb, XTy) / sigma2 t1 = 0.5 * np.linalg.multi_dot([mw.T, Rb, mw]) t2 = np.dot(mw.T, vb) t3 = 0.5 * np.dot(Rb, covw).trace() return t1 - t2 + t3 def veb_ridge_step(XTX, XTy, sigma2, sigmab2, mw, covw, use_emstep = False): p = XTy.shape[0] mmT = np.einsum(&#39;i,j-&gt;ij&#39;, mw, mw) if use_emstep: EWTXTXW = np.multiply(XTX, mmT) else: EWTXTXW = np.multiply(XTX, mmT + covw) covbinv = (EWTXTXW / sigma2) + (np.eye(p) / sigmab2) covb = np.linalg.inv(covbinv) mb = np.linalg.multi_dot([covb, np.diag(mw).T, XTy]) / sigma2 return mb, covb def get_sigma_updates(X, y, XTX, XTy, yTy, mb, mw, covb, covw, use_emstep = False): n, p = X.shape mmTw = np.einsum(&#39;i,j-&gt;ij&#39;, mw, mw) mmTb = np.einsum(&#39;i,j-&gt;ij&#39;, mb, mb) mbmwXTy = np.dot(mb, np.einsum(&#39;i,i-&gt;i&#39;, mw, XTy)) if use_emstep: EWTXTXW = np.multiply(XTX, mmTw) trace_cov = np.dot(EWTXTXW, mmTb + covb).trace() else: EWTXTXW = np.multiply(XTX, mmTw + covw) trace_cov = np.dot(EWTXTXW, mmTb + covb).trace() sigma2 = (yTy - 2 * mbmwXTy + trace_cov) / n sigmab2 = np.sum(np.square(mb) + np.diag(covb)) / p sigmaw2 = np.sum(np.square(mw) + np.diag(covw)) / p return sigma2, sigmab2, sigmaw2 def veb_iridge(X, y, tol = 1e-8, max_iter = 10000, init_sigma = 1.0, init_sigmab = 1.0, init_sigmaw = 1.0, init_mb = 1.0, init_mw = 1.0, init_sb = 1.0, init_sw = 1.0, use_convergence = True, update_sigma = True, update_sigmab = True, update_sigmaw = True, use_emstep = False, debug = True ): n, p = X.shape XTX = np.dot(X.T, X) XTy = np.dot(X.T, y) yTy = np.dot(y.T, y) elbo_path = np.zeros(max_iter + 1) # Initialize hyperparameters sigma = init_sigma sigmab = init_sigmab sigmaw = init_sigmaw sigma2 = sigma * sigma sigmab2 = sigmab * sigmab sigmaw2 = sigmaw * sigmaw # Initialize variational parameters covb = np.eye(p) * init_sb * init_sb covw = np.eye(p) * init_sw * init_sw mb = np.repeat(init_mb, p) mw = np.repeat(init_mw, p) niter = 0 elbo_path[0] = -np.inf for itn in range(1, max_iter + 1): &#39;&#39;&#39; Update &#39;&#39;&#39; mb, covb = veb_ridge_step(XTX, XTy, sigma2, sigmab2, mw, covw, use_emstep) mw, covw = veb_ridge_step(XTX, XTy, sigma2, sigmaw2, mb, covb, use_emstep) if update_sigma or update_sigmab or update_sigmaw: _sigma2, _sigmab2, _sigmaw2 = get_sigma_updates(X, y, XTX, XTy, yTy, mb, mw, covb, covw, use_emstep) if update_sigma: sigma2 = _sigma2 if update_sigmab: sigmab2 = _sigmab2 if update_sigmaw: sigmaw2 = _sigmaw2 sigma = np.sqrt(sigma2) sigmab = np.sqrt(sigmab2) sigmaw = np.sqrt(sigmaw2) if debug: print(f&quot;Iteration {itn}&quot;) print(sigma2, sigmab2, sigmaw2) &#39;&#39;&#39; Convergence &#39;&#39;&#39; niter += 1 elbo_path[itn] = get_elbo(X, XTX, XTy, yTy, sigma2, sigmab2, sigmaw2, mb, mw, covb, covw) if use_convergence: if elbo_path[itn] - elbo_path[itn - 1] &lt; tol: break return mb, mw, covb, covw, niter, elbo_path[:niter + 1], sigma, sigmab, sigmaw . . vebres = dict() vebres[&#39;fix_sall&#39;] = veb_iridge(X, y, init_sigma = sigtrue, init_sigmab = sigbtrue, init_sigmaw = sigwtrue, update_sigma = False, update_sigmab = False, update_sigmaw = False, debug = False) vebres[&#39;fix_sbsw&#39;] = veb_iridge(X, y, init_sigma = sigtrue, init_sigmab = sigbtrue, init_sigmaw = sigwtrue, update_sigma = True, update_sigmab = False, update_sigmaw = False, debug = False) vebres[&#39;fix_s&#39;] = veb_iridge(X, y, init_sigma = sigtrue, init_sigmab = sigbtrue, init_sigmaw = sigwtrue, update_sigma = False, update_sigmab = True, update_sigmaw = True, debug = False) vebres[&#39;fix_none&#39;] = veb_iridge(X, y, init_sigma = sigtrue, init_sigmab = sigbtrue, init_sigmaw = sigwtrue, update_sigma = True, update_sigmab = True, update_sigmaw = True, debug = False) vebres[&#39;fix_b/w&#39;] = veb_iridge(X, y, init_sigma = sigtrue, init_sigmab = sigbtrue, init_sigmaw = sigwtrue, update_sigma = True, update_sigmab = True, update_sigmaw = True, debug = False, use_emstep = True) . . def list_sigmas_vebres(res): sigma = res[6] sigmab = res[7] sigmaw = res[8] return [sigma, sigmab, sigmaw] def list_params_vebres(res): niter = res[4] elbo = res[5][-1] return [niter, elbo] data = [[sigtrue, sigbtrue, sigwtrue] + [0, 0]] rownames = [&#39;True&#39;] for key, val in vebres.items(): data.append(list_sigmas_vebres(val) + list_params_vebres(val)) rownames.append(key) colnames = [&#39;s&#39;, &#39;sb&#39;, &#39;sw&#39;, &#39;niter&#39;, &#39;ELBO&#39;] #rownames = [&#39;True&#39;, &#39;VEB-fix&#39;, &#39;VEB-full&#39;, &#39;VEB-fix-s&#39;, &#39;VEB-fix-sb-sw&#39;, &#39;VEB-high-s&#39;] df = pd.DataFrame.from_records(data, columns = colnames, index = rownames) #df.style.format(&quot;{:.3f}&quot;) df . . s sb sw niter ELBO . True 50.000000 | 2.000000 | 4.000000 | 0 | 0.000000e+00 | . fix_sall 50.000000 | 2.000000 | 4.000000 | 140 | -2.109103e+02 | . fix_sbsw 98.965709 | 2.000000 | 4.000000 | 2 | -3.190882e+02 | . fix_s 50.000000 | 1.671161 | 2.725174 | 92 | -2.047158e+02 | . fix_none 95.628828 | 1.934922 | 2.915587 | 2 | -3.120212e+02 | . fix_b/w 0.097264 | 2.345863 | 4.170464 | 1128 | 3.304583e+07 | . def plot_predictions(ax, X, y, btrue, wtrue, res, key): mb = res[0] mw = res[1] bwpred = np.multiply(mb, mw) ypred = np.dot(X, bwpred) ypredtrue = np.dot(X, np.multiply(btrue, wtrue)) ax.scatter(y, ypred) mpl_utils.plot_diag(ax) ax.text(0.1, 0.9, f&quot;{key}&quot;, transform = ax.transAxes) ax.set_xlabel(&#39;y&#39;) ax.set_ylabel(&#39;VEB ypred&#39;) return fig = plt.figure(figsize = (18, 12)) i = 0 for key, res in vebres.items(): ax = fig.add_subplot(2, 3, i+1) plot_predictions(ax, X, y, btrue, wtrue, res, key) i += 1 plt.tight_layout() plt.show() . .",
            "url": "https://banskt.github.io/iridge-notes/2021/02/19/multiple-regression-known-prior-with-product-of-normals.html",
            "relUrl": "/2021/02/19/multiple-regression-known-prior-with-product-of-normals.html",
            "date": " • Feb 19, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Multiple regression (two predictors) with product of normals",
            "content": "About . I am trying to understand the problem in the VEB implementation of multiple regression with product of normals. Here, I will numerically check the underlying distributions for a special case with only two predictors. For the single predictor ($p=1$) analysis, check here. . import numpy as np import pandas as pd from scipy import linalg as sc_linalg from scipy import special as sc_special import matplotlib.pyplot as plt import mpl_stylesheet import mpl_utils from matplotlib import cm from matplotlib import ticker as plticker from mpl_toolkits.axes_grid1 import make_axes_locatable mpl_stylesheet.banskt_presentation(fontfamily = &#39;latex-clearsans&#39;, fontsize = 18, colors = &#39;banskt&#39;, dpi = 72) . . Toy model . I defined a toy model with two predictors and 10 samples. I sampled the predictor from the standard normal distribution $ mathcal{N}(0, 1)$. The model used is described in the writeup. . I chose $ sigma = 10.0$, $ sigma_b = 2.0$ and $ sigma_w = 4.0$. Let the true values for $b$ and $w$ be denoted as $ hat{b}$ and $ hat{w}$. I calculated the log posterior of $bw$ from the sum of the log prior and the log likelihood and normalized it using a discrete grid of $bw$. . def variance_explained(y, ypred): ss_err = np.sum(np.square(y - ypred)) ss_tot = np.sum(np.square(y - np.mean(y))) r2 = 1 - (ss_err / ss_tot) return r2 def prod_norm_prior_pdf(z, s1, s2): x = np.abs(z) / s1 / s2 prob = sc_special.kn(0, x) / (np.pi * s1 * s2) return prob def prod_norm_prior_gridpdf(b1, b2, s1, s2): n1 = b1.shape[0] n2 = b2.shape[0] prob = np.zeros((n2, n1)) pdf2 = prod_norm_prior_pdf(b2, s1, s2) for i, _b1 in enumerate(b1): pdf1 = prod_norm_prior_pdf(_b1, s1, s2) prob[:, i] = pdf1 * pdf2 return prob def normal_logdensity_onesample(z, m, s2): logdensity = - 0.5 * np.log(2 * np.pi * s2) - 0.5 * (z - m) * (z - m) / s2 return logdensity def normal_logdensity(y, mean, sigma2): n = y.shape[0] logdensity = - 0.5 * n * np.log(2 * np.pi * sigma2) - 0.5 * np.sum(np.square(y - mean)) / sigma2 return logdensity def data_log_likelihood(X, y, b1, b2, sigma): n1 = b1.shape[0] n2 = b2.shape[0] ll = np.zeros((n2, n1)) for i, _b1 in enumerate(b1): for j, _b2 in enumerate(b2): mean = np.dot(X, np.array([_b1, _b2])) ll[j, i]= normal_logdensity(y, mean, sigma * sigma) return ll def normalize_by_volume(x, y, z): &#39;&#39;&#39; y is the row, x is the column of z &#39;&#39;&#39; grid_area = np.einsum(&#39;i, j -&gt; ij&#39;, np.diff(y), np.diff(x)) volume = np.sum(np.multiply(z[1:, 1:], grid_area)) return z / volume def normalize_logdensity_by_volume(x, y, lnz): z = np.exp(lnz) grid_area = np.einsum(&#39;i, j -&gt; ij&#39;, np.diff(y), np.diff(x)) V = np.sum(np.multiply(z[1:, 1:], grid_area)) return lnz - np.log(V) # Use Green&#39;s theorem to compute the area # enclosed by a given contour. def area_greens(vs): xvs = vs[:, 0] yvs = vs[:, 1] area = 0.5 * np.sum(xvs[:-1] * np.diff(yvs) - yvs[:-1] * np.diff(xvs)) return np.abs(area) def plot_contours(ax, X, Y, Z, beta, norm, cstep = 10, xlabel = &quot;&quot;, ylabel = &quot;&quot;, zlabel = &quot;&quot;, nlevels = 5, nstd = 0, showContourArea=False, showbeta=False, showZmax=False, showColbar=False): zmin = np.min(Z) - nstd * np.std(Z) zmax = np.max(Z) + nstd * np.std(Z) ind = np.unravel_index(np.argmax(Z, axis=None), Z.shape) levels = np.linspace(zmin, zmax, 200) clevels = np.linspace(zmin, zmax, nlevels) cmap = cm.YlOrRd if norm: cset1 = ax.contourf(X, Y, Z, levels, norm = norm, cmap=cm.get_cmap(cmap, len(levels) - 1)) else: cset1 = ax.contourf(X, Y, Z, levels, cmap=cm.get_cmap(cmap, len(levels) - 1)) cset2 = ax.contour(X, Y, Z, clevels, colors=&#39;k&#39;) if showContourArea: for i, contour in enumerate(cset2.collections): cpaths = contour.get_paths() if len(cpaths) &gt; 0: vs = contour.get_paths()[0].vertices # Compute area enclosed by vertices. a = area_greens(vs) print (&quot;r = &quot; + str(clevels[i]) + &quot;: a =&quot; + str(a)) ax.clabel(cset2, inline=1, fontsize=10) for c in cset2.collections: c.set_linestyle(&#39;solid&#39;) ax.set_aspect(&quot;equal&quot;) if showbeta: ax.scatter(beta[0], beta[1], color = &#39;blue&#39;, s = 100) if showZmax: ax.scatter(X[ind[1]], Y[ind[0]], color = &#39;k&#39;, s = 100) if xlabel: ax.set_xlabel(xlabel) if ylabel: ax.set_ylabel(ylabel) if showColbar: divider = make_axes_locatable(ax) cax = divider.append_axes(&quot;right&quot;, size=&quot;5%&quot;, pad=0.2) cbar = plt.colorbar(cset1, cax=cax) ytickpos = np.arange(int(zmin / cstep) * cstep, zmax, cstep) cbar.set_ticks(ytickpos) if zlabel: cax.set_ylabel(zlabel) return cset1, cset2, clevels def simulate_data(n, p, s, sb, sw, seed = 200): np.random.seed(seed) b = np.random.normal(0, sb, p) w = np.random.normal(0, sw, p) bw = np.multiply(b, w) X = np.zeros((n, p)) for i in range(p): X[:, i] = np.random.normal(0, 1, n) err = np.random.normal(0, s, n) y = np.dot(X, bw) + err return X, y, b, w, bw def print_matrix(matrix, fmt): print(&#39; n&#39;.join([&#39; t&#39;.join([fmt.format(cell) for cell in row]) for row in matrix])) . . nsample = 50 npred = 2 sigbtrue = 2.0 sigwtrue = 4.0 sigtrue = 15.0 X, y, btrue, wtrue, bwtrue = simulate_data(nsample, npred, sigtrue, sigbtrue, sigwtrue, seed = 300) ypredtrue = np.dot(X, bwtrue) r2 = variance_explained(y, ypredtrue) bwprior = np.multiply(np.random.normal(0, sigbtrue, 1000), np.random.normal(0, sigwtrue, 1000)) # mcmc_trace, mcmc_posterior = gibbs(X, y, sigtrue, sigbtrue, sigwtrue, # burn_iter = 1000, max_iter = 10000) print(f&quot;Fraction of variance explained by X: {r2:.2f}&quot;) . . Fraction of variance explained by X: 0.66 . bwvals = np.linspace(-40, 40, 1000) bwvals1 = np.linspace(13, 23, 100) bwvals2 = np.linspace(-3, 7, 80) numerical_prior_1d = prod_norm_prior_pdf(bwvals, sigbtrue, sigwtrue) numerical_prior = prod_norm_prior_gridpdf(bwvals1, bwvals2, sigbtrue, sigwtrue) numerical_logll = data_log_likelihood(X, y, bwvals1, bwvals2, sigtrue) numerical_logll = normalize_logdensity_by_volume(bwvals1, bwvals2, numerical_logll) numerical_logpost = numerical_logll + np.log(numerical_prior) numerical_logpost = normalize_logdensity_by_volume(bwvals1, bwvals2, numerical_logpost) fig = plt.figure(figsize = (12, 12)) ax1 = fig.add_subplot(221) ax2 = fig.add_subplot(222) ax3 = fig.add_subplot(223) ax4 = fig.add_subplot(224) ax1.scatter(ypredtrue, y) mpl_utils.plot_diag(ax1) ax1.text(0.02, 0.94, f&quot;bw = ({bwtrue[0]:.2f}, {bwtrue[1]:.2f})&quot;, transform = ax1.transAxes) #ax1.set_title(&quot;y = Xbw + e&quot;, pad = 20.0) ax1.set_xlabel(&quot;Xbw&quot;) ax1.set_ylabel(&quot;y&quot;) ax2.hist(bwprior, bins = 50, density = True, alpha = 0.4, label=&quot;Simulation&quot;) ax2.plot(bwvals, numerical_prior_1d, label = &quot;Analytical&quot;) ax2.legend() ax2.set_title(&quot;Prior distribution&quot;, pad = 20.0) ax2.set_xlabel(&quot;bw&quot;) ax2.set_ylabel(&quot;Density&quot;) zdata = np.exp(numerical_logll) norm = cm.colors.Normalize(vmin=np.min(zdata), vmax=np.max(zdata)) cs1, cs2, zlevels = plot_contours(ax3, bwvals1, bwvals2, zdata, bwtrue, norm = norm, cstep = 10, nlevels = 5, showbeta = True, showZmax = False, xlabel = r&quot;$b_1 w_1$&quot;, ylabel = r&quot;$b_2 w_2$&quot;, zlabel = &quot;Likelihood&quot;, ) ax3.set_title(&quot;Likelihood&quot;, pad = 20.0) zdata = np.exp(numerical_logpost) norm = cm.colors.Normalize(vmin=np.min(zdata), vmax=np.max(zdata)) cs1, cs2, zlevels = plot_contours(ax4, bwvals1, bwvals2, zdata, bwtrue, norm = norm, cstep = 10, nlevels = 5, showbeta = True, showZmax = False, xlabel = r&quot;$b_1 w_1$&quot;, ylabel = r&quot;$b_2 w_2$&quot;, zlabel = &quot;Posterior&quot;, ) ax4.set_title(&quot;Posterior&quot;, pad = 20.0) plt.tight_layout() plt.show() . . Variational approximation with fixed $ sigma$, $ sigma_b$ and $ sigma_w$ . Here, I obtain the variational parameters using coordinate ascent updates of $m_b$, $s_b$, $m_w$ and $s_w$, while the hyperparameters are fixed at their true values, $ sigma = 15.0$, $ sigma_b = 2.0$ and $ sigma_w = 4.0$. . def get_elbo(X, XTX, XTy, yTy, s2, sb2, sw2, mb, mw, covb, covw): n, p = X.shape elbo = cfunc(n, p, yTy, s2, sb2, sw2, covb, covw) - hfunc(mb, covb, sb2) - efunc(XTX, XTy, mb, mw, covb, covw, s2, sw2) return elbo def get_elbo_grid(X, y, s, sb, sw, mb, mw, covb, covw, bwvals1, bwvals2, debug = False): k1 = bwvals1.shape[0] k2 = bwvals2.shape[0] elbo_grid = np.zeros((k2, k1)) s2 = s * s sb2 = sb * sb sw2 = sw * sw XTX = np.dot(X.T, X) XTy = np.dot(X.T, y) yTy = np.dot(y.T, y) for i in range(k1): for j in range(k2): _mb = np.array([bwvals1[i] / mw[0], bwvals2[j] / mw[1]]) if debug: print(np.multiply(_mb, mw)) elbo_grid[j, i] = get_elbo(X, XTX, XTy, yTy, s2, sb2, sw2, _mb, mw, covb, covw) return elbo_grid def cfunc(n, p, yTy, s2, sb2, sw2, covb, covw): sign, det_covb = np.linalg.slogdet(covb) sign, det_covw = np.linalg.slogdet(covw) val = 0.5 * p val += - 0.5 * p * np.log(sb2) - 0.5 * p * np.log(sw2) val += - 0.5 * n * np.log(2.0 * np.pi * s2) val += 0.5 * det_covb + 0.5 * det_covw val += 0.5 * yTy / s2 return val def hfunc(m, cov, s2): h = np.sum(np.square(m)) + cov.trace() return 0.5 * h / s2 def efunc(XTX, XTy, mb, mw, covb, covw, sigma2, sigmaw2): p = mb.shape[0] mmTb = np.einsum(&#39;i,j-&gt;ij&#39;, mb, mb) EBTXTXB = np.multiply(XTX, mmTb + covb) Rb = (EBTXTXB / sigma2) + (np.eye(p) / sigmaw2) vb = np.einsum(&#39;i,i-&gt;i&#39;, mb, XTy) / sigma2 t1 = 0.5 * np.linalg.multi_dot([mw.T, Rb, mw]) t2 = np.dot(mw.T, vb) t3 = 0.5 * np.dot(Rb, covw).trace() return t1 - t2 + t3 def qposterior_hist2d(mb, mw, covb, covw, bwvals1, bwvals2, n=1e8, density=True): &#39;&#39;&#39; calculate n samples of b and w, from which create an array of bw For the 2d histogram, bwvals1 := bin edges on the x-axis bwvals2 := bin edges on the y-axis Returns bw_counts := bin counts (or density) on a grid of bwvals1 on the x-axis (columns) and bwvals2 on the y-axis (rows) &#39;&#39;&#39; bvals = np.random.multivariate_normal(mb, covb, int(n)) wvals = np.random.multivariate_normal(mw, covw, int(n)) bw_samples = np.multiply(bvals, wvals) xbins = np.digitize(bw_samples[:, 0], bwvals1) ybins = np.digitize(bw_samples[:, 1], bwvals2) bwbins = np.array([xbins, ybins]).T k1 = bwvals1.shape[0] k2 = bwvals2.shape[0] bw_counts = np.zeros((k2, k1)) for i in range(1, k1): for j in range(1, k2): bw_counts[j, i] = np.sum((bwbins[:, None] == np.array([i, j])).all(-1)) if density: bw_counts = normalize_by_volume(bwvals1, bwvals2, bw_counts) &#39;&#39;&#39; Another way to do this would be: bw_hist2d, _, _ = np.histogram2d(bw_samples[:, 0], bw_samples[:, 1], [bwvals1, bwvals2], density = density) Implement after debugging &#39;&#39;&#39; return bw_counts def veb_ridge_step(XTX, XTy, sigma2, sigmab2, mw, covw): p = XTy.shape[0] mmT = np.einsum(&#39;i,j-&gt;ij&#39;, mw, mw) EWTXTXW = np.multiply(XTX, mmT + covw) covbinv = (EWTXTXW / sigma2) + (np.eye(p) / sigmab2) covb = np.linalg.inv(covbinv) mb = np.linalg.multi_dot([covb, np.diag(mw).T, XTy]) / sigma2 return mb, covb def get_sigma_updates(X, y, XTX, XTy, yTy, mb, mw, covb, covw): n, p = X.shape mmTw = np.einsum(&#39;i,j-&gt;ij&#39;, mw, mw) mmTb = np.einsum(&#39;i,j-&gt;ij&#39;, mb, mb) EWTXTXW = np.multiply(XTX, mmTw + covw) mbmwXTy = np.dot(mb, np.einsum(&#39;i,i-&gt;i&#39;, mw, XTy)) trace_cov = np.dot(EWTXTXW, mmTb + covb).trace() sigma2 = (yTy - 2 * mbmwXTy + trace_cov) / n sigmab2 = np.sum(np.square(mb) + np.diag(covb)) / p sigmaw2 = np.sum(np.square(mw) + np.diag(covw)) / p return sigma2, sigmab2, sigmaw2 def veb_iridge(X, y, tol = 1e-8, max_iter = 10000, init_sigma = 1.0, init_sigmab = 1.0, init_sigmaw = 1.0, init_mb = 1.0, init_mw = 1.0, init_sb = 1.0, init_sw = 1.0, use_convergence = True, update_sigma = True, update_sigmab = True, update_sigmaw = True, debug = True ): n, p = X.shape XTX = np.dot(X.T, X) XTy = np.dot(X.T, y) yTy = np.dot(y.T, y) elbo_path = np.zeros(max_iter + 1) # Initialize hyperparameters sigma = init_sigma sigmab = init_sigmab sigmaw = init_sigmaw sigma2 = sigma * sigma sigmab2 = sigmab * sigmab sigmaw2 = sigmaw * sigmaw # Initialize variational parameters covb = np.eye(p) * init_sb * init_sb covw = np.eye(p) * init_sw * init_sw mb = np.repeat(init_mb, p) mw = np.repeat(init_mw, p) niter = 0 elbo_path[0] = -np.inf for itn in range(1, max_iter + 1): &#39;&#39;&#39; Update &#39;&#39;&#39; mb, covb = veb_ridge_step(XTX, XTy, sigma2, sigmab2, mw, covw) mw, covw = veb_ridge_step(XTX, XTy, sigma2, sigmaw2, mb, covb) if update_sigma or update_sigmab or update_sigmaw: _sigma2, _sigmab2, _sigmaw2 = get_sigma_updates(X, y, XTX, XTy, yTy, mb, mw, covb, covw) if update_sigma: sigma2 = _sigma2 if update_sigmab: sigmab2 = _sigmab2 if update_sigmaw: sigmaw2 = _sigmaw2 sigma = np.sqrt(sigma2) sigmab = np.sqrt(sigmab2) sigmaw = np.sqrt(sigmaw2) if debug: print(f&quot;Iteration {itn}&quot;) print(sigma2, sigmab2, sigmaw2) &#39;&#39;&#39; Convergence &#39;&#39;&#39; niter += 1 elbo_path[itn] = get_elbo(X, XTX, XTy, yTy, sigma2, sigmab2, sigmaw2, mb, mw, covb, covw) if use_convergence: if elbo_path[itn] - elbo_path[itn - 1] &lt; tol: break return mb, mw, covb, covw, niter, elbo_path[:niter + 1], sigma, sigmab, sigmaw . . def veb_show_result(X, y, vebres, ax1, ax2, ax3, ax4, bwvals1, bwvals2, true_ypred, true_posterior, use_logscale_elbo = True): mb, mw, covb, covw, niter, elbo_path, sigma, sigmab, sigmaw = vebres bw = np.multiply(mb, mw) ypred = np.dot(X, bw) veb_posterior = qposterior_hist2d(mb, mw, covb, covw, bwvals1, bwvals2, n = 1e6) _sigma2, _sigmab2, _sigmaw2 = get_sigma_updates(X, y, np.dot(X.T, X), np.dot(X.T, y), np.dot(y.T, y), mb, mw, covb, covw) sigma = np.sqrt(_sigma2) sigmab = np.sqrt(_sigmab2) sigmaw = np.sqrt(_sigmaw2) print(f&quot;Optimized in {niter} iterations. Final ELBO: {elbo_path[-1]:.3f}&quot;) print(&quot;Optimum parameters:&quot;) print(f&quot;mb = {mb[0]:g}, {mb[1]:g}&quot;) print(f&quot;mw = {mw[0]:g}, {mw[1]:g}&quot;) print(f&quot;sb&quot;) print_matrix(covb, &quot;{:g}&quot;) print(&quot;&quot;) print(f&quot;sw&quot;) print_matrix(covw, &quot;{:g}&quot;) print(&quot;&quot;) max_elbo = np.max(elbo_path) elbo_diff = max_elbo - elbo_path[1:] min_diff = np.min(elbo_diff[elbo_diff!=0]) elbo_diff[elbo_diff == 0] = min_diff ax1.scatter(np.arange(niter - 1), elbo_diff[:-1]) ax1.plot(np.arange(niter - 1), elbo_diff[:-1]) if use_logscale_elbo: ax1.set_yscale(&#39;log&#39;) ax1.set_xlabel(&#39;Iteration&#39;) ax1.set_ylabel(&#39;Distance to &quot;best&quot; ELBO&#39;) ax2.scatter(ypred, ypredtrue) mpl_utils.plot_diag(ax2) ax2.text(0.02, 0.94, f&quot;bw = ({bw[0]:.2f}, {bw[1]:.2f})&quot;, transform = ax2.transAxes) ax2.set_xlabel(&#39;VEB $y_{ mathrm{pred}}$&#39;) ax2.set_ylabel(&#39;True $y_{ mathrm{pred}}$&#39;) zdata = true_posterior norm = cm.colors.Normalize(vmin=np.min(zdata), vmax=np.max(zdata)) cs1, cs2, zlevels = plot_contours(ax4, bwvals1, bwvals2, zdata, bwtrue, norm = norm, cstep = 10, nlevels = 5, showbeta = True, showZmax = False, xlabel = r&quot;$b_1 w_1$&quot;, ylabel = r&quot;$b_2 w_2$&quot;, zlabel = &quot;Posterior&quot;, ) ax4.set_title(&quot;True Posterior&quot;, pad = 20.0) zdata = veb_posterior cs1, cs2, zlevels = plot_contours(ax3, bwvals1, bwvals2, zdata, bwtrue, norm = norm, cstep = 10, nlevels = 5, showbeta = True, showZmax = False, xlabel = r&quot;$b_1 w_1$&quot;, ylabel = r&quot;$b_2 w_2$&quot;, zlabel = &quot;Posterior&quot;, ) ax3.set_title(&quot;Variational Posterior&quot;, pad = 20.0) return sigma, sigmab, sigmaw . veb1_res = veb_iridge(X, y, init_sigma = sigtrue, init_sigmab = sigbtrue, init_sigmaw = sigwtrue, update_sigma = False, update_sigmab = False, update_sigmaw = False, debug = False) . . fig = plt.figure(figsize=(12, 12)) ax1 = fig.add_subplot(221) ax2 = fig.add_subplot(222) ax3 = fig.add_subplot(223) ax4 = fig.add_subplot(224) veb1_sigma, veb1_sigmab, veb1_sigmaw = veb_show_result(X, y, veb1_res, ax1, ax2, ax3, ax4, bwvals1, bwvals2, ypredtrue, np.exp(numerical_logpost) ) plt.tight_layout() plt.show() . . Optimized in 91 iterations. Final ELBO: -66.106 Optimum parameters: mb = 3.04132, 0.349786 mw = 6.08147, 0.69957 sb 0.0964886 -0.0023612 -0.0023612 0.807325 sw 0.385802 -0.00944298 -0.00944298 3.2293 . VEB optimization of all parameters . The problem starts when I try to obtain both the variational parameters $m_b$, $s_b$, $m_w$ and $s_w$, and the hyperparameters $ sigma$, $ sigma_b$ and $ sigma_w$ using the update scheme described in the writeup. I have switched off the convergence criteria to check how the ELBO evolves over time. . I found that the convergence depends on the initialization of $ sigma$, $ sigma_b$ and $ sigma_w$. Initializing at the true values of the parameters lead to &quot;unnatural&quot; ELBO updates. (I had observed similar ELBO updates in the trend filtering example). . veb2_res = veb_iridge(X, y, init_sigma = sigtrue, init_sigmab = sigbtrue, init_sigmaw = sigwtrue, debug = False, use_convergence = False, max_iter = 100) . . fig = plt.figure(figsize=(12, 12)) ax1 = fig.add_subplot(221) ax2 = fig.add_subplot(222) ax3 = fig.add_subplot(223) ax4 = fig.add_subplot(224) veb2_sigma, veb2_sigmab, veb2_sigmaw = veb_show_result(X, y, veb2_res, ax1, ax2, ax3, ax4, bwvals1, bwvals2, ypredtrue, np.exp(numerical_logpost), use_logscale_elbo = True ) plt.tight_layout() plt.show() . . Optimized in 100 iterations. Final ELBO: -68.941 Optimum parameters: mb = 6.4937, 0.836119 mw = 2.86254, 0.368576 sb 0.446534 -0.0123073 -0.0123073 3.74056 sw 0.0867704 -0.00239156 -0.00239156 0.726864 . Variational approximation with fixed $ sigma$ . If I fix $ sigma$ and allow update of $ sigma_b$ and $ sigma_w$, then the ELBO keeps increasing, provided $ sigma$ is initialized at the true value. . veb3_res = veb_iridge(X, y, init_sigma = sigtrue, init_sigmab = sigbtrue, init_sigmaw = sigwtrue, debug = False, update_sigma = False, use_convergence = False, max_iter = 100) . . fig = plt.figure(figsize=(12, 12)) ax1 = fig.add_subplot(221) ax2 = fig.add_subplot(222) ax3 = fig.add_subplot(223) ax4 = fig.add_subplot(224) veb3_sigma, veb3_sigmab, veb3_sigmaw = veb_show_result(X, y, veb3_res, ax1, ax2, ax3, ax4, bwvals1, bwvals2, ypredtrue, np.exp(numerical_logpost), use_logscale_elbo = True ) plt.tight_layout() plt.show() . . Optimized in 100 iterations. Final ELBO: -66.035 Optimum parameters: mb = 6.49685, 0.888305 mw = 2.86272, 0.391416 sb 0.437943 -0.0128269 -0.0128269 3.66886 sw 0.0850294 -0.00249043 -0.00249043 0.712332 . Variational approximation with fixed $ sigma_b$ and $ sigma_w$ . Here I will allow update of $ sigma$, while keeping $ sigma_b$ and $ sigma_w$ fixed. . veb4_res = veb_iridge(X, y, init_sigma = sigtrue, init_sigmab = sigbtrue, init_sigmaw = sigwtrue, debug = False, update_sigma = True, update_sigmab = False, update_sigmaw = False, use_convergence = False, max_iter = 1000) . . fig = plt.figure(figsize=(12, 12)) ax1 = fig.add_subplot(221) ax2 = fig.add_subplot(222) ax3 = fig.add_subplot(223) ax4 = fig.add_subplot(224) veb4_sigma, veb4_sigmab, veb4_sigmaw = veb_show_result(X, y, veb4_res, ax1, ax2, ax3, ax4, bwvals1, bwvals2, ypredtrue, np.exp(numerical_logpost), use_logscale_elbo = True ) plt.tight_layout() plt.show() . . Optimized in 1000 iterations. Final ELBO: -69.151 Optimum parameters: mb = 3.03998, 0.317298 mw = 6.07996, 0.634596 sb 0.0984972 -0.00218552 -0.00218552 0.824218 sw 0.393989 -0.00874209 -0.00874209 3.29687 . Initialization with very high $ sigma$ leads to well-behaved optimization . If I initialize $ sigma = 100.0$ and fix the values of $ sigma_b = 2.0$ and $ sigma_w = 4.0$, then the ELBO is well-behaved (always increasing). The optimized values are the same as the previous one, where we initialize with $ sigma = 10.0$ and allowed updates for 1000 iterations (without stopping the iterations with any convergence criteria). . veb5_res = veb_iridge(X, y, init_sigma = 100.0, init_sigmab = sigbtrue, init_sigmaw = sigwtrue, debug = False, update_sigma = True, update_sigmab = False, update_sigmaw = False, use_convergence = False, max_iter = 250) . . fig = plt.figure(figsize=(12, 12)) ax1 = fig.add_subplot(221) ax2 = fig.add_subplot(222) ax3 = fig.add_subplot(223) ax4 = fig.add_subplot(224) veb5_sigma, veb5_sigmab, veb5_sigmaw = veb_show_result(X, y, veb5_res, ax1, ax2, ax3, ax4, bwvals1, bwvals2, ypredtrue, np.exp(numerical_logpost), use_logscale_elbo = True ) plt.tight_layout() plt.show() . . Optimized in 250 iterations. Final ELBO: -69.151 Optimum parameters: mb = 3.03998, 0.317298 mw = 6.07996, 0.634596 sb 0.0984972 -0.00218552 -0.00218552 0.824218 sw 0.393989 -0.00874209 -0.00874209 3.29687 . What is the expectation of $ sigma$, $ sigma_b$ and $ sigma_w$? . data = [[sigtrue, sigbtrue, sigwtrue], [veb1_sigma, veb1_sigmab, veb1_sigmaw], [veb2_sigma, veb2_sigmab, veb2_sigmaw], [veb3_sigma, veb3_sigmab, veb3_sigmaw], [veb4_sigma, veb4_sigmab, veb4_sigmaw], [veb5_sigma, veb5_sigmab, veb5_sigmaw] ] colnames = [&#39;s&#39;, &#39;sb&#39;, &#39;sw&#39;] rownames = [&#39;True&#39;, &#39;VEB-fix&#39;, &#39;VEB-full&#39;, &#39;VEB-fix-s&#39;, &#39;VEB-fix-sb-sw&#39;, &#39;VEB-high-s&#39;] df = pd.DataFrame.from_records(data, columns = colnames, index = rownames) df.style.format(&quot;{:.3f}&quot;) . s sb sw . True 15.000 | 2.000 | 4.000 | . VEB-fix 15.144 | 2.267 | 4.533 | . VEB-full 15.150 | 4.850 | 2.138 | . VEB-fix-s 15.138 | 4.853 | 2.138 | . VEB-fix-sb-sw 15.157 | 2.265 | 4.531 | . VEB-high-s 15.157 | 2.265 | 4.531 | .",
            "url": "https://banskt.github.io/iridge-notes/2021/01/28/multiple-regression-two-predictors.html",
            "relUrl": "/2021/01/28/multiple-regression-two-predictors.html",
            "date": " • Jan 28, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Simple regression with product of normals",
            "content": "About . I am trying to understand the underfitting in the VEB implementation of multiple regression with product of normals. Here, I will numerically check the underlying distributions for the special case of simple regression model. . import numpy as np import pandas as pd from scipy import linalg as sc_linalg from scipy import special as sc_special from scipy import integrate as sc_integrate import matplotlib.pyplot as plt import mpl_stylesheet import mpl_utils from matplotlib import cm from matplotlib import ticker as plticker from mpl_toolkits.axes_grid1 import make_axes_locatable mpl_stylesheet.banskt_presentation(fontfamily = &#39;latex-clearsans&#39;, fontsize = 18, colors = &#39;banskt&#39;, dpi = 72) . . def variance_explained(y, ypred): ss_err = np.sum(np.square(y - ypred)) ss_tot = np.sum(np.square(y - np.mean(y))) r2 = 1 - (ss_err / ss_tot) return r2 def prod_norm_prior_pdf(z, s1, s2): x = np.abs(z) / s1 / s2 prob = sc_special.kn(0, x) / (np.pi * s1 * s2) return prob def normal_logdensity_onesample(z, m, s2): logdensity = - 0.5 * np.log(2 * np.pi * s2) - 0.5 * (z - m) * (z - m) / s2 return logdensity def prod_norm_prior2_pdf(z, s1, s2): w = 3.5 b = z / w p1 = np.exp(normal_logdensity_onesample(b, 0, s1 * s1)) p2 = np.exp(normal_logdensity_onesample(w, 0, s2 * s2)) return p1 * p2 def normal_logdensity(y, mean, sigma2): n = y.shape[0] logdensity = - 0.5 * n * np.log(2 * np.pi * sigma2) - 0.5 * np.sum(np.square(y - mean)) / sigma2 return logdensity def data_log_likelihood(X, y, b, sigma): ll = np.zeros_like(b) for i, _b in enumerate(b): ll[i] = normal_logdensity(y, X * _b, sigma * sigma) return ll def normalize_by_grid(y, x): unnorm_sum = sc_integrate.simps(y, x) return y / unnorm_sum def normalize_logdensity_by_grid(lny, x): y = np.exp(lny) M = sc_integrate.simps(y, x) return lny - np.log(M) &#39;&#39;&#39; Implement a simple Gibbs sampling algorithm to calculate the posterior distribution &#39;&#39;&#39; def sample_b(xtx, xty, w, sigma2, sigmab2): sb2inv = (1 / sigmab2) + (xtx * w * w / sigma2) sb2 = 1 / sb2inv mean = w * xty * sb2 / sigma2 return np.random.normal(mean, np.sqrt(sb2)) def gibbs(X, y, sigma, sigmab, sigmaw, binit = 1.0, winit = 1.0, burn_iter = 1000, max_iter = 1000): xtx = np.sum(np.square(X)) xty = np.dot(X, y) sigma2 = sigma * sigma sigmab2 = sigmab * sigmab sigmaw2 = sigmaw * sigmaw _b = binit _w = winit trace = np.zeros((max_iter, 2)) # trace to store values of b and w bwvals = np.zeros(max_iter) for itn in range(burn_iter): _b = sample_b(xtx, xty, _w, sigma2, sigmab2) _w = sample_b(xtx, xty, _b, sigma2, sigmaw2) for itn in range(max_iter): _b = sample_b(xtx, xty, _w, sigma2, sigmab2) _w = sample_b(xtx, xty, _b, sigma2, sigmaw2) trace[itn, :] = np.array([_b, _w]) bwvals[itn] = _b * _w return trace, bwvals . . Toy model . I defined a toy model with one predictor (or independent variable) and 10 samples. I sampled the predictor from the standard normal distribution $ mathcal{N}(0, 1)$. The model used is described in the Appendix A of the writeup. In short, the likelihood of the model is, . $ displaystyle p left( y mid x, b, w, sigma^2 right) = prod_{n = 1}^{N} mathcal{N} left(y_n mid x_n b w, sigma^2 right)$ . We used normal priors with zero means for $b sim mathcal{N}(0, sigma_b^2)$ and $w sim mathcal{N}(0, sigma_w^2)$, which gives a prior probability of $bw$ as, . $ displaystyle p left(bw mid sigma_b^2, sigma_w^2 right) = frac{K_0{ displaystyle left( frac{ lvert bw rvert}{ sigma_b sigma_w} right)}}{ pi sigma_b sigma_w}$. . I chose $ sigma = 10.0$, $ sigma_b = 2.0$ and $ sigma_w = 4.0$. Let the true values for $b$ and $w$ be denoted as $ hat{b}$ and $ hat{w}$. The sum of the log prior and the log likelihood is proportional to the log posterior of $bw$. I called this the &quot;numerical&quot; posterior. The posterior is normalized using a discrete grid of $bw$. The prior, likelihood and posterior depends on the product $bw$ and hence will have the same densities along the line $b = 1 / w$. . I can also use Gibbs sampling technique to obtain the posterior distribution, and is labeled as &#39;Gibbs&#39;. . nsample = 10 sigbtrue = 2.0 sigwtrue = 4.0 sigtrue = 10.0 np.random.seed(200) btrue = np.random.normal(0, sigbtrue) wtrue = np.random.normal(0, sigwtrue) bwtrue = btrue * wtrue X = np.random.normal(0, 1, nsample) y = X * btrue * wtrue + np.random.normal(0, sigtrue, nsample) r2 = variance_explained(y, X * btrue * wtrue) bwprior = np.multiply(np.random.normal(0, sigbtrue, 1000), np.random.normal(0, sigwtrue, 1000)) mcmc_trace, mcmc_posterior = gibbs(X, y, sigtrue, sigbtrue, sigwtrue, burn_iter = 1000, max_iter = 10000) print(f&quot;Fraction of variance explained by X: {r2:.2f}&quot;) . . Fraction of variance explained by X: 0.41 . #bwvals = mpl_utils.x_linspace(ax2, n = 1000) #bwvals = np.linspace(-40, 40, 1000) bwvals = np.linspace(-30, 10, 1000) xlims = [-30, 10] numerical_prior = prod_norm_prior_pdf(bwvals, sigbtrue, sigwtrue) numerical_logll = data_log_likelihood(X, y, bwvals, sigtrue) numerical_likelihood = normalize_by_grid(np.exp(numerical_logll), bwvals) numerical_logpost_unnorm = np.log(numerical_prior) + numerical_logll numerical_logpost = normalize_logdensity_by_grid(numerical_logpost_unnorm, bwvals) numerical_posterior = np.exp(numerical_logpost) numerical_bwopt = bwvals[np.argmax(numerical_posterior)] fig = plt.figure(figsize = (12, 12)) ax1 = fig.add_subplot(221) ax2 = fig.add_subplot(222) ax3 = fig.add_subplot(223) ax4 = fig.add_subplot(224) ax1.scatter(X * bwtrue, y) mpl_utils.plot_diag(ax1) ax1.text(0.1, 0.9, f&quot;bw = {bwtrue:.3f}&quot;, transform = ax1.transAxes) #ax1.set_title(&quot;y = Xbw + e&quot;, pad = 20.0) ax1.set_xlabel(&quot;Xbw&quot;) ax1.set_ylabel(&quot;y&quot;) ax2.hist(bwprior, bins = 50, density = True, alpha = 0.4, label=&quot;Simulation&quot;) ax2.plot(bwvals, numerical_prior, label = &quot;Analytical&quot;) ax2.legend() ax2.set_title(&quot;Prior distribution&quot;, pad = 20.0) ax2.set_xlabel(&quot;bw&quot;) ax2.set_ylabel(&quot;Density&quot;) ax3.plot(bwvals, numerical_likelihood) ax3.set_xlabel(&quot;bw&quot;) ax3.set_ylabel(&quot;Likelihood p(y | b, w)&quot;) ax3.set_xlim(xlims) ax4.hist(mcmc_posterior, density = True, alpha = 0.2, label = &#39;Gibbs&#39;) ax4.plot(bwvals, numerical_posterior, alpha = 1.0, label = &#39;Numerical&#39;) ax4.axvline(numerical_bwopt, color = &#39;gray&#39;, linestyle = &#39;dotted&#39;) ax4.text(0.5, 0.2, f&quot;{numerical_bwopt:.3f}&quot;, transform = ax4.transAxes) ax4.legend() ax4.set_title(&quot;Posterior distribution&quot;, pad = 20.0) ax4.set_xlabel(&quot;bw&quot;) ax4.set_ylabel(&quot;Density&quot;) ax4.set_xlim(xlims) plt.tight_layout() plt.show() . . ELBO and variational posterior . We used the mean field approximation, . $q(b, w) = q(b) q(w)$, with $q(b) = mathcal{N} left(b mid m_b, s_b^2 right)$ and $q(w) = mathcal{N} left(w mid m_w, s_w^2 right)$. . Poor man&#39;s numerical optimization . Top left: I fixed $m_w = hat{w}$, $m_b = hat{b}$ and $s_w = sigma_w$. We then calculated the ELBO for a sequence of values of $s_b$ and the optimum (maximum) ELBO was observed at $ hat{s}_b$. . Top right: I fixed $m_w = hat{w}$, $m_b = hat{b}$ and $s_b = hat{s}_b$. We then calculated the ELBO for a sequence of values of $s_w$ and the optimum (maximum) ELBO was observed at $ hat{s}_w$. . Bottom left: I fixed $m_w = hat{w}$, $s_b = hat{s}_b$ and $s_w = hat{s}_w$. We then calculated the ELBO for a sequence of values of $m_b$ and the optimum (maximum) ELBO was observed at $ hat{m}_b$. . Variational Posterior . The variational posterior for different values of $bw$ on the bottom right plot is calculated with $m_b = hat{m}_b$, $m_w = hat{w}$, $s_b = hat{s}_b$ and $s_w = hat{s}_w$. . def get_elbo(X, y, sigma, sigmab, sigmaw, mb, mw, sb, sw): sigma2 = sigma * sigma sigmab2 = sigmab * sigmab sigmaw2 = sigmaw * sigmaw sb2 = sb * sb sw2 = sw * sw return get_elbo_sq(X, y, sigma2, sigmab2, sigmaw2, mb, mw, sb2, sw2) def get_elbo_sq(X, y, sigma2, sigmab2, sigmaw2, mb, mw, sb2, sw2): elbo = Eqlnproby(X, y, mb, mw, sb2, sw2, sigma2) - KLqp(0, mb, sigmab2, sb2) - KLqp(0, mw, sigmaw2, sw2) return elbo def KLqp(m1, m2, s1sq, s2sq): KLqp = 0.5 * (np.log(s1sq / s2sq) + ((s2sq + (m1 - m2) ** 2) / s1sq) - 1) return KLqp def Eqlnproby(X, y, mb, mw, sb2, sw2, sigma2): mb2 = mb * mb mw2 = mw * mw t1 = - 0.5 * np.log(2 * np.pi * sigma2) t2 = - 0.5 * np.sum(np.square(y - X * mb * mw)) / sigma2 t3 = - 0.5 * np.sum(np.square(X)) * (mw2 * sb2 + mb2 * sw2 + sb2 * sw2) / sigma2 return t1 + t2 + t3 def normal_logdensity_onesample(z, m, s2): logdensity = - 0.5 * np.log(2 * np.pi * s2) - 0.5 * (z - m) * (z - m) / s2 return logdensity def get_elbo_bw(X, y, sigma, sigmab, sigmaw, mbvals, mw, sb, sw): elbo = np.zeros_like(mbvals) for i, mb in enumerate(mbvals): elbo[i] = get_elbo(X, y, sigma, sigmab, sigmaw, mb, mw, sb, sw) return elbo def get_elbo_sb(X, y, sigma, sigmab, sigmaw, mb, mw, sbvals, sw): elbo = np.zeros_like(sbvals) for i, sb in enumerate(sbvals): elbo[i] = get_elbo(X, y, sigma, sigmab, sigmaw, mb, mw, sb, sw) return elbo def get_elbo_sw(X, y, sigma, sigmab, sigmaw, mb, mw, sb, swvals): elbo = np.zeros_like(swvals) for i, sw in enumerate(swvals): elbo[i] = get_elbo(X, y, sigma, sigmab, sigmaw, mb, mw, sb, sw) return elbo def get_qbw(b, w, mb, mw, sb2, sw2): lnqb = normal_logdensity_onesample(b, mb, sb2) lnqw = normal_logdensity_onesample(w, mw, sw2) return lnqb + lnqw def get_qbw_hist(mb, mw, sb, sw, nsample = 1000): bvals = np.random.normal(mb, sb, nsample) wvals = np.random.normal(mw, sw, nsample) return np.multiply(bvals, wvals) def get_qbw_bw(bwvals, mb, mw, sb, sw): w = 6.2 sb2 = sb * sb sw2 = sw * sw lnq = np.zeros_like(bwvals) for i, bw in enumerate(bwvals): b = bw / w lnq[i] = get_qbw(b, w, mb, mw, sb2, sw2) return normalize_by_grid(np.exp(lnq), bwvals) . . fig = plt.figure(figsize = (14, 12)) ax1 = fig.add_subplot(221) ax2 = fig.add_subplot(222) ax3 = fig.add_subplot(223) ax4 = fig.add_subplot(224) sbvals = np.logspace(-4, 0.6, 100) elbo_sb = get_elbo_sb(X, y, sigtrue, sigbtrue, sigwtrue, btrue, wtrue, sbvals, sigwtrue) elbo_sbopt = sbvals[np.argmax(elbo_sb)] ax1.plot(np.log10(sbvals), elbo_sb) ax1.axvline(np.log10(elbo_sbopt), color = &#39;gray&#39;, linestyle = &#39;dotted&#39;) ax1.text(0.5, 0.2, f&quot;{elbo_sbopt:.3f}&quot;, transform = ax1.transAxes) ax1.set_xlabel(&quot;$ log({s_b})$&quot;) ax1.set_ylabel(&quot;ELBO&quot;) swvals = np.logspace(-1, 0.6, 100) elbo_sw = get_elbo_sw(X, y, sigtrue, sigbtrue, sigwtrue, btrue, wtrue, elbo_sbopt, swvals) elbo_swopt = swvals[np.argmax(elbo_sw)] ax2.plot(np.log10(swvals), elbo_sw) ax2.axvline(np.log10(elbo_swopt), color = &#39;gray&#39;, linestyle = &#39;dotted&#39;) ax2.text(0.5, 0.2, f&quot;{elbo_swopt:.3f}&quot;, transform = ax2.transAxes) ax2.set_xlabel(&quot;$ log({s_w})$&quot;) ax2.set_ylabel(&quot;ELBO&quot;) elbo_bw = get_elbo_bw(X, y, sigtrue, sigbtrue, sigwtrue, bwvals / wtrue, wtrue, elbo_sbopt, elbo_swopt) elbo_bwopt = bwvals[np.argmax(elbo_bw)] numerical_evidence = numerical_logll + np.log(numerical_prior) - numerical_logpost ax3.plot(bwvals, elbo_bw, label = &quot;F(q)&quot;) #ax3.plot(bwvals, numerical_evidence, ls = &#39;dotted&#39;, label = &quot;log p(y)&quot;) ax3.axvline(elbo_bwopt, color = &#39;gray&#39;, linestyle = &#39;dotted&#39;) ax3.text(0.5, 0.2, f&quot;{elbo_bwopt:.3f}&quot;, transform = ax3.transAxes) ax3.legend() ax3.set_xlabel(r&quot;$m_b hat{w}$&quot;) ax3.set_ylabel(&quot;ELBO&quot;) # variational_posterior = get_qbw_bw(bwvals, elbo_bwopt / wtrue, wtrue, # elbo_sbopt, elbo_swopt) # variational_bwopt = bwvals[np.argmax(variational_posterior)] variational_posterior = get_qbw_hist(elbo_bwopt / wtrue, wtrue, elbo_sbopt, elbo_swopt) ax4.plot(bwvals, numerical_posterior, label = &quot;Numerical&quot;, color = &#39;dodgerblue&#39;) ax4.hist(variational_posterior, density = True, label = &quot;bw from q(b)q(w)&quot;, color = &#39;salmon&#39;, alpha = 0.4) #ax4.axvline(variational_bwopt, linestyle = &#39;dashed&#39;, color = &#39;salmon&#39;) #ax4.axvline(numerical_bwopt, linestyle = &#39;dotted&#39;, color = &#39;dodgerblue&#39;) #ax4.text(0.5, 0.5, f&quot;{variational_bwopt:.3f}&quot;, transform = ax4.transAxes) ax4.legend() ax4.set_title(&quot;Posterior&quot;, pad = 20.0) ax4.set_xlabel(&quot;bw&quot;) ax4.set_ylabel(&quot;Density&quot;) plt.tight_layout() plt.show() . . VEB optimization with fixed $ sigma$, $ sigma_b$ and $ sigma_w$ . Here, I obtain the variational parameters using coordinate ascent updates of $m_b$, $s_b$, $m_w$ and $s_w$, while the hyperparameters are fixed at their true values, $ sigma = 10.0$, $ sigma_b = 2.0$ and $ sigma_w = 4.0$. . def veb_ridge_step(xtx, xty, sigma2, sigmab2, mw, sw2): mw2 = mw * mw sb2inv = (1 / sigmab2) + (xtx * (mw2 + sw2) / sigma2) sb2 = 1 / sb2inv mb = sb2 * mw * xty / sigma2 return mb, sb2 def get_sigma_updates(X, y, xtx, mb, mw, sb2, sw2): n = y.shape[0] mb2 = mb * mb mw2 = mw * mw yresidual2 = np.sum(np.square(y - X * mb * mw)) sigma2 = (yresidual2 + (xtx * (mw2 * sb2 + mb2 * sw2 + sw2 * sb2))) / n sigmab2 = mb2 + sb2 sigmaw2 = mw2 + sw2 return sigma2, sigmab2, sigmaw2 def veb1(X, y, tol = 1e-8, max_iter = 10000, init_sigma = 1.0, init_sigmab = 1.0, init_sigmaw = 1.0, init_mb = 1.0, init_mw = 1.0, init_sb = 1.0, init_sw = 1.0, update_sigmas = True, debug = False, use_convergence = True ): xtx = np.sum(np.square(X)) xty = np.dot(X, y) elbo_path = np.zeros(max_iter + 1) # Initialize hyperparameters sigma2 = init_sigma * init_sigma sigmab2 = init_sigmab * init_sigmab sigmaw2 = init_sigmaw * init_sigmaw # Initialize variational parameters sb2 = init_sb * init_sb sw2 = init_sw * init_sw mb = init_mb mw = init_mw niter = 0 elbo_path[0] = -np.inf for itn in range(1, max_iter + 1): &#39;&#39;&#39; Update &#39;&#39;&#39; mb, sb2 = veb_ridge_step(xtx, xty, sigma2, sigmab2, mw, sw2) mw, sw2 = veb_ridge_step(xtx, xty, sigma2, sigmaw2, mb, sb2) if update_sigmas: sigma2, sigmab2, sigmaw2 = get_sigma_updates(X, y, xtx, mb, mw, sb2, sw2) &#39;&#39;&#39; Convergence &#39;&#39;&#39; niter += 1 elbo_path[itn] = get_elbo_sq(X, y, sigma2, sigmab2, sigmaw2, mb, mw, sb2, sw2) if use_convergence: if elbo_path[itn] - elbo_path[itn - 1] &lt; tol: break sb = np.sqrt(sb2) sw = np.sqrt(sw2) sigma = np.sqrt(sigma2) sigmab = np.sqrt(sigmab2) sigmaw = np.sqrt(sigmaw2) return mb, mw, sb, sw, niter, elbo_path[:niter + 1], sigma, sigmab, sigmaw . . veb_res = veb1(X, y, init_sigma = sigtrue, init_sigmab = sigbtrue, init_sigmaw = sigwtrue, update_sigmas = False) veb_mb, veb_mw, veb_sb, veb_sw, veb_niter, veb_elbo, veb_sigma, veb_sigmab, veb_sigmaw = veb_res veb_posterior = get_qbw_hist(veb_mb, veb_mw, veb_sb, veb_sw) veb_bwopt = veb_mb * veb_mw veb_ypred = X * veb_bwopt print(f&quot;Optimized in {veb_niter} iterations. Final ELBO: {veb_elbo[-1]:.3f}&quot;) print(&quot;Optimum parameters:&quot;) print(f&quot;mb = {veb_mb:.3f}&quot;) print(f&quot;mw = {veb_mw:.3f}&quot;) print(f&quot;sb = {veb_sb:.3f}&quot;) print(f&quot;sw = {veb_sw:.3f}&quot;) print(&quot;&quot;) print(f&quot;bw = {veb_bwopt:.3f}&quot;) . . Optimized in 12 iterations. Final ELBO: -12.821 Optimum parameters: mb = -2.459 mw = 4.918 sb = 0.883 sw = 1.765 bw = -12.093 . The ELBO increases continuously and the distribution of $bw$ obtained from $q(b)q(w)$ matches closely with the true posterior distribution. . fig = plt.figure(figsize = (12, 12)) ax1 = fig.add_subplot(221) ax2 = fig.add_subplot(222) ax3 = fig.add_subplot(223) max_elbo = veb_elbo[-1] elbo_diff = max_elbo - veb_elbo[1:] ax1.scatter(np.arange(veb_niter - 1), elbo_diff[:-1]) ax1.plot(np.arange(veb_niter - 1), elbo_diff[:-1]) ax1.set_yscale(&#39;log&#39;) ax1.set_xlabel(&#39;Iteration&#39;) ax1.set_ylabel(&#39;Distance to &quot;best&quot; ELBO&#39;) ax2.scatter(veb_ypred, y) mpl_utils.plot_diag(ax2) ax2.text(0.1, 0.9, f&quot;bw = {veb_bwopt:.3f}&quot;, transform = ax2.transAxes) ax2.set_xlabel(&#39;VEB $y_{ mathrm{pred}}$&#39;) ax2.set_ylabel(&#39;True $y_{ mathrm{pred}}$&#39;) ax3.plot(bwvals, numerical_posterior, label = &quot;Numerical&quot;, color = &#39;dodgerblue&#39;) ax3.axvline(numerical_bwopt, linestyle = &#39;dotted&#39;, color = &#39;dodgerblue&#39;) ax3.hist(veb_posterior, density = True, label = &quot;bw from q(b)q(w)&quot;, color = &#39;salmon&#39;, alpha = 0.4) ax3.axvline(veb_bwopt, linestyle = &#39;dashed&#39;, color = &#39;salmon&#39;) ax3.legend() ax3.set_title(&quot;Posterior&quot;, pad = 20.0) ax3.set_xlabel(&quot;bw&quot;) ax3.set_ylabel(&quot;Log density&quot;) plt.tight_layout() plt.show() . . np.sqrt(get_sigma_updates(X, y, np.sum(np.square(X)), veb_mb, veb_mw, veb_sb**2, veb_sw**2)) . array([12.04714593, 2.61259987, 5.2250033 ]) . VEB optimization of all parameters . Here, I obtain both the variational parameters $m_b$, $s_b$, $m_w$ and $s_w$, and the hyperparameters $ sigma$, $ sigma_b$ and $ sigma_w$ using the update scheme described in the appendix of the writeup. I have switched off the convergence criteria to check how the ELBO evolves over time. . I found that the convergence depends on the initialization of $ sigma$, $ sigma_b$ and $ sigma_w$. Initializing at the true values of the parameters lead to decreasing ELBO. . If we set the parameters to extremely high values, e.g. $ sigma = 100.0$, $ sigma_b = 20.0$ and $ sigma_w = 40.0$, then we recover correct parameters after optimization. . veb2_res = veb1(X, y, init_sigma = sigtrue, init_sigmab = sigbtrue, init_sigmaw = sigwtrue, update_sigmas = True, use_convergence = False, max_iter = 30) veb2_mb, veb2_mw, veb2_sb, veb2_sw, veb2_niter, veb2_elbo, veb2_sigma, veb2_sigmab, veb2_sigmaw = veb2_res veb2_posterior = get_qbw_hist(veb2_mb, veb2_mw, veb2_sb, veb2_sw) veb2_bwopt = veb2_mb * veb2_mw veb2_ypred = X * veb2_bwopt print(f&quot;Optimized in {veb2_niter} iterations. Final ELBO: {veb2_elbo[-1]:.3f}&quot;) print(&quot;Optimum parameters:&quot;) print(f&quot;mb = {veb2_mb:.3f}&quot;) print(f&quot;mw = {veb2_mw:.3f}&quot;) print(f&quot;sb = {veb2_sb:.3f}&quot;) print(f&quot;sw = {veb2_sw:.3f}&quot;) print(f&quot;sigma_b = {veb2_sigmab:.3f}&quot;) print(f&quot;sigma_w = {veb2_sigmaw:.3f}&quot;) print(&quot;&quot;) print(f&quot;bw = {veb2_bwopt:.3f}&quot;) . . Optimized in 30 iterations. Final ELBO: -10.199 Optimum parameters: mb = -2.816 mw = 4.123 sb = 1.285 sw = 1.882 sigma_b = 3.095 sigma_w = 4.532 bw = -11.609 . fig = plt.figure(figsize = (12, 12)) ax1 = fig.add_subplot(221) ax2 = fig.add_subplot(222) ax3 = fig.add_subplot(223) max_elbo = veb2_elbo[-1] elbo_diff = max_elbo - veb2_elbo[1:] ax1.scatter(np.arange(veb2_niter), elbo_diff) #ax1.set_yscale(&#39;log&#39;) ax1.set_xlabel(&#39;Iteration&#39;) ax1.set_ylabel(&#39;Distance to &quot;best&quot; ELBO&#39;) ax2.scatter(veb2_ypred, y) mpl_utils.plot_diag(ax2) ax2.text(0.1, 0.9, f&quot;bw = {veb2_bwopt:.3f}&quot;, transform = ax2.transAxes) ax2.set_xlabel(&#39;VEB $y_{ mathrm{pred}}$&#39;) ax2.set_ylabel(&#39;True $y_{ mathrm{pred}}$&#39;) ax3.plot(bwvals, numerical_posterior, label = &quot;Numerical&quot;, color = &#39;dodgerblue&#39;) ax3.axvline(numerical_bwopt, linestyle = &#39;dotted&#39;, color = &#39;dodgerblue&#39;) ax3.hist(veb2_posterior, density = True, label = &quot;bw from q(b)q(w)&quot;, color = &#39;salmon&#39;, alpha = 0.4) ax3.axvline(veb2_bwopt, linestyle = &#39;dashed&#39;, color = &#39;salmon&#39;) ax3.legend() ax3.set_title(&quot;Posterior&quot;, pad = 20.0) ax3.set_xlabel(&quot;bw&quot;) ax3.set_ylabel(&quot;Log density&quot;) plt.tight_layout() plt.show() . .",
            "url": "https://banskt.github.io/iridge-notes/2021/01/27/simple-regression-with-product-of-normals.html",
            "relUrl": "/2021/01/27/simple-regression-with-product-of-normals.html",
            "date": " • Jan 27, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "Effect of $\sigma_b$ and $\sigma_w$ in EBMR with product of normals",
            "content": "About . Here, I look at the dependence of ELBO on $ sigma_b$ and $ sigma_w$. . import numpy as np import pandas as pd from scipy import linalg as sc_linalg import matplotlib.pyplot as plt import sys sys.path.append(&quot;../../ebmrPy/&quot;) from inference.ebmr import EBMR from inference import f_elbo from inference import f_sigma from inference import penalized_em from utils import log_density sys.path.append(&quot;../../utils/&quot;) import mpl_stylesheet from matplotlib import cm from matplotlib import ticker as plticker from mpl_toolkits.axes_grid1 import make_axes_locatable mpl_stylesheet.banskt_presentation(fontfamily = &#39;latex-clearsans&#39;, fontsize = 18, colors = &#39;banskt&#39;, dpi = 72) . . Toy example . The same trend-filtering data as used previously. . def standardize(X): Xnorm = (X - np.mean(X, axis = 0)) #Xstd = Xnorm / np.std(Xnorm, axis = 0) Xstd = Xnorm / np.sqrt((Xnorm * Xnorm).sum(axis = 0)) return Xstd def trend_data(n, p, bval = 1.0, sd = 1.0, seed=100): np.random.seed(seed) X = np.zeros((n, p)) for i in range(p): X[i:n, i] = np.arange(1, n - i + 1) #X = standardize(X) btrue = np.zeros(p) idx = int(n / 3) btrue[idx] = bval btrue[idx + 1] = -bval y = np.dot(X, btrue) + np.random.normal(0, sd, n) # y = y / np.std(y) return X, y, btrue . . n = 100 p = 200 bval = 8.0 sd = 2.0 X, y, btrue = trend_data(n, p, bval = bval, sd = sd) fig = plt.figure() ax1 = fig.add_subplot(111) ax1.plot(np.arange(n), np.dot(X, btrue), label = &quot;Xb&quot;) ax1.scatter(np.arange(n), y, edgecolor = &#39;black&#39;, facecolor=&#39;white&#39;, label = &quot;Xb + e&quot;) ax1.legend() ax1.set_xlabel(&quot;Sample index&quot;) ax1.set_ylabel(&quot;y&quot;) plt.show() . . Fix initial values of $ sigma^2$, $ sigma_b^2$ and $ sigma_w^2$ . First, I will get an optimal fit, using point estimates of $ mathbf{b}$ and $ mathbf{w}$ (without using the contribution of their variance). And then initialize the second optimization (including the contribution of the variances of $ mathbf{b}$ and $ mathbf{w}$) from the optimal initial values of all parameters. In this second optimization, I will fix the parameters $ sigma^2$, $ sigma_b^2$ and $ sigma_w^2$. . def ridge_mll(X, y, s2, sb2, W): n, p = X.shape Xscale = np.dot(X, np.diag(W)) XWWtXt = np.dot(Xscale, Xscale.T) sigmay = s2 * (np.eye(n) + sb2 * XWWtXt) muy = np.zeros((n, 1)) return log_density.mgauss(y.reshape(-1,1), muy, sigmay) def grr_step(X, y, s2, sb2, muW, varW, XTX, XTy, useVW=True): n, p = X.shape W = np.diag(muW) WtXtXW = np.linalg.multi_dot([W.T, XTX, W]) VW = np.diag(XTX) * np.diag(varW) if useVW else np.zeros(p) sigmabinv = (WtXtXW + np.diag(VW) + np.eye(p) * s2 / sb2) / s2 sigmab = np.linalg.inv(sigmabinv) mub = np.linalg.multi_dot([sigmab, W.T, XTy]) / s2 XWmu = np.linalg.multi_dot([X, W, mub]) mub2 = np.square(mub) s2 = (np.sum(np.square(y - XWmu)) + np.dot((WtXtXW + np.diag(VW)), sigmab).trace() + np.sum(mub2 * VW)) / n sb2 = (np.sum(mub2) + sigmab.trace()) / p return s2, sb2, mub, sigmab def elbo(X, y, s2, sb2, sw2, mub, sigmab, Wbar, varW, XTX, useVW=True): &#39;&#39;&#39; Wbar is a vector which contains the diagonal elements of the diagonal matrix W W = diag_matrix(Wbar) Wbar = diag(W) -- VW is a vector which contains the diagonal elements of the diagonal matrix V_w &#39;&#39;&#39; n, p = X.shape VW = np.diag(XTX) * np.diag(varW) if useVW else np.zeros(p) elbo = c_func(n, p, s2, sb2, sw2) + h1_func(X, y, s2, sb2, sw2, mub, Wbar, VW) + h2_func(p, s2, sb2, sw2, XTX, Wbar, sigmab, varW, VW) return elbo def c_func(n, p, s2, sb2, sw2): val = p val += - 0.5 * n * np.log(2.0 * np.pi * s2) val += - 0.5 * p * np.log(sb2) val += - 0.5 * p * np.log(sw2) return val def h1_func(X, y, s2, sb2, sw2, mub, Wbar, VW): XWmu = np.linalg.multi_dot([X, np.diag(Wbar), mub]) val1 = - (0.5 / s2) * np.sum(np.square(y - XWmu)) val2 = - 0.5 * np.sum(np.square(mub) * ((VW / s2) + (1 / sb2))) val3 = - 0.5 * np.sum(np.square(Wbar)) / sw2 val = val1 + val2 + val3 return val def h2_func(p, s2, sb2, sw2, XTX, Wbar, sigmab, sigmaw, VW): (sign, logdetS) = np.linalg.slogdet(sigmab) (sign, logdetV) = np.linalg.slogdet(sigmaw) W = np.diag(Wbar) WtXtXW = np.linalg.multi_dot([W.T, XTX, W]) val = 0.5 * logdetS + 0.5 * logdetV val += - 0.5 * np.trace(sigmab) / sb2 - 0.5 * np.trace(sigmaw) / sw2 val += - 0.5 * np.dot(WtXtXW + np.diag(VW), sigmab).trace() / s2 return val def ebmr_WB2(X, y, s2_init = 1.0, sb2_init = 1.0, sw2_init = 1.0, binit = None, winit = None, sigmab_init = None, sigmaw_init = None, use_wb_variance = True, ignore_b_update = False, ignore_w_update = False, ignore_s2_update = False, ignore_s2_sb2_update = False, ignore_sw2_update = False, max_iter = 10000, tol = 1e-8 ): XTX = np.dot(X.T, X) XTy = np.dot(X.T, y) n_samples, n_features = X.shape elbo_path = np.zeros(max_iter + 1) mll_path = np.zeros(max_iter + 1) &#39;&#39;&#39; Iteration 0 &#39;&#39;&#39; niter = 0 s2 = s2_init sb2 = sb2_init sw2 = sw2_init mub = np.ones(n_features) if binit is None else binit muw = np.ones(n_features) if winit is None else winit sigmab = np.zeros((n_features, n_features)) if sigmab_init is None else sigmab_init sigmaw = np.zeros((n_features, n_features)) if sigmaw_init is None else sigmaw_init elbo_path[0] = -np.inf mll_path[0] = -np.inf for itn in range(1, max_iter + 1): &#39;&#39;&#39; GRR for b &#39;&#39;&#39; if not ignore_b_update: if ignore_s2_update: #print (&quot;Updating b without s2&quot;) __, sb2, mub, sigmab = grr_step(X, y, s2, sb2, muw, sigmaw, XTX, XTy, useVW=use_wb_variance) elif ignore_s2_sb2_update: #print (&quot;Updating b without s2 and sb2&quot;) __, ___, mub, sigmab = grr_step(X, y, s2, sb2, muw, sigmaw, XTX, XTy, useVW=use_wb_variance) else: #print (&quot;Updating b&quot;) s2, sb2, mub, sigmab = grr_step(X, y, s2, sb2, muw, sigmaw, XTX, XTy, useVW=use_wb_variance) &#39;&#39;&#39; GRR for W &#39;&#39;&#39; if not ignore_w_update: if ignore_sw2_update: #print (&quot;Updating w without sw2&quot;) __, ___, muw, sigmaw = grr_step(X, y, s2, sw2, mub, sigmab, XTX, XTy, useVW=use_wb_variance) else: #print (&quot;Updating w&quot;) __, sw2, muw, sigmaw = grr_step(X, y, s2, sw2, mub, sigmab, XTX, XTy, useVW=use_wb_variance) &#39;&#39;&#39; Convergence &#39;&#39;&#39; niter += 1 elbo_path[itn] = elbo(X, y, s2, sb2, sw2, mub, sigmab, muw, sigmaw, XTX, useVW=use_wb_variance) mll_path[itn] = ridge_mll(X, y, s2, sb2, muw) if elbo_path[itn] - elbo_path[itn - 1] &lt; tol: break #if mll_path[itn] - mll_path[itn - 1] &lt; tol: break return s2, sb2, sw2, mub, sigmab, muw, sigmaw, niter, elbo_path[:niter + 1], mll_path[:niter + 1] . . m1 = ebmr_WB2(X, y, use_wb_variance = False) # s2, sb2, sw2, mub, sigmab, W, sigmaW, niter, elbo_path, mll_path = m1 m2 = ebmr_WB2(X, y, use_wb_variance=True, s2_init = m1[0], sb2_init = m1[1], sw2_init = m1[2], binit = m1[3], sigmab_init = m1[4], winit = m1[5], sigmaw_init = m1[6], ignore_s2_sb2_update = True, ignore_sw2_update = True, tol = 1e-3, ) s2, sb2, sw2, mub, sigmab, W, sigmaW, niter, elbo_path, mll_path = m2 bpred = mub * W ypred = np.dot(X, bpred) fig = plt.figure(figsize = (12, 12)) ax1 = fig.add_subplot(221) ax2 = fig.add_subplot(222) ax3 = fig.add_subplot(223) #ax4 = fig.add_subplot(224) yvals = np.log(np.max(elbo_path[1:]) - elbo_path[1:] + 1) ax1.scatter(np.arange(niter), yvals, edgecolor = &#39;black&#39;, facecolor=&#39;white&#39;) ax1.plot(np.arange(niter), yvals) ax1.set_xlabel(&quot;Iterations&quot;) ax1.set_ylabel(&quot;log (max(ELBO) - ELBO[itn] + 1)&quot;) ax2.scatter(np.arange(n), y, edgecolor = &#39;black&#39;, facecolor=&#39;white&#39;) ax2.plot(np.arange(n), ypred, color = &#39;salmon&#39;, label=&quot;Predicted&quot;) ax2.plot(np.arange(n), np.dot(X, btrue), color = &#39;dodgerblue&#39;, label=&quot;True&quot;) ax2.legend() ax2.set_xlabel(&quot;Sample Index&quot;) ax2.set_ylabel(&quot;y&quot;) ax3.scatter(np.arange(p), btrue, edgecolor = &#39;black&#39;, facecolor=&#39;white&#39;, label=&quot;True&quot;) ax3.scatter(np.arange(p), bpred, label=&quot;Predicted&quot;) ax3.legend() ax3.set_xlabel(&quot;Predictor Index&quot;) ax3.set_ylabel(&quot;wb&quot;) # nstep = min(80, niter - 2) # ax4.scatter(np.arange(nstep), mll_path[-nstep:], edgecolor = &#39;black&#39;, facecolor=&#39;white&#39;, label=&quot;Evidence&quot;) # ax4.plot(np.arange(nstep), elbo_path[-nstep:], label=&quot;ELBO&quot;) # ax4.legend() # ax4.set_xlabel(&quot;Iterations&quot;) # ax4.set_ylabel(&quot;ELBO / Evidence&quot;) plt.tight_layout() plt.show() . . Dependence on $ sigma_b$ and $ sigma_w$ . Next, I look at the contour of max(ELBO) at different values of $ sigma_b$ and $ sigma_w$. The optimization at each set of $( sigma_b, sigma_w)$ was performed with a fixed $ sigma^2 = 3.89$ (obtained from the previous optimization) and iterations were stopped with a tolerance of 1e-3. . def optimize_elbo(X, y, s2_init, sb2_init, sw2_init, binit, winit, sigmab_init, sigmaw_init): mres = ebmr_WB2(X, y, use_wb_variance=True, s2_init = s2_init, sb2_init = sb2_init, sw2_init = sw2_init, binit = binit, sigmab_init = sigmab_init, winit = winit, sigmaw_init = sigmaw_init, ignore_s2_sb2_update = True, ignore_sw2_update = True, tol = 1e-3, max_iter = 500, ) s2, sb2, sw2, mub, sigmab, W, sigmaW, niter, elbo_path, mll_path = mres return elbo_path[-1] def plot_contours(ax, X, Y, Z, beta, norm, cstep = 10, xlabel = &quot;&quot;, ylabel = &quot;&quot;, zlabel = &quot;&quot;, showbeta=False, showZmax=False): zmin = np.min(Z) - 1 * np.std(Z) zmax = np.max(Z) + 1 * np.std(Z) ind = np.unravel_index(np.argmax(Z, axis=None), Z.shape) levels = np.linspace(zmin, zmax, 200) clevels = np.linspace(zmin, zmax, 20) cmap = cm.YlOrRd_r if norm: cset1 = ax.contourf(X, Y, Z, levels, norm = norm, cmap=cm.get_cmap(cmap, len(levels) - 1)) else: cset1 = ax.contourf(X, Y, Z, levels, cmap=cm.get_cmap(cmap, len(levels) - 1)) cset2 = ax.contour(X, Y, Z, clevels, colors=&#39;k&#39;) for c in cset2.collections: c.set_linestyle(&#39;solid&#39;) ax.set_aspect(&quot;equal&quot;) if showbeta: ax.scatter(beta[0], beta[1], color = &#39;blue&#39;, s = 100) if showZmax: ax.scatter(X[ind[1]], Y[ind[0]], color = &#39;k&#39;, s = 100) if xlabel: ax.set_xlabel(xlabel) if ylabel: ax.set_ylabel(ylabel) divider = make_axes_locatable(ax) cax = divider.append_axes(&quot;right&quot;, size=&quot;5%&quot;, pad=0.2) cbar = plt.colorbar(cset1, cax=cax) ytickpos = np.arange(int(zmin / cstep) * cstep, zmax, cstep) cbar.set_ticks(ytickpos) if zlabel: cax.set_ylabel(zlabel) #loc = plticker.AutoLocator() #ax.xaxis.set_major_locator(loc) #ax.yaxis.set_major_locator(loc) . . k = 25 sb2_vals = np.logspace(-5, 1, k) sw2_vals = np.logspace(-5, 1, k) s2_init = m1[0] binit = m1[3] winit = m1[5] sigmab_init = m1[4] sigmaw_init = m1[6] ELBO = np.zeros((k, k)) for i, sb2_init in enumerate(sb2_vals): for j, sw2_init in enumerate(sw2_vals): ELBO[j, i] = optimize_elbo(X, y, s2_init, sb2_init, sw2_init, binit, winit, sigmab_init, sigmaw_init) . . fig = plt.figure(figsize = (10, 8)) ax1 = fig.add_subplot(111) norm = cm.colors.Normalize(vmin=np.min(ELBO), vmax=np.max(ELBO)) #norm = cm.colors.TwoSlopeNorm(vmin=np.min(ELBO), vcenter=np.max(ELBO)-100, vmax=np.max(ELBO)) plot_contours(ax1, np.log(sb2_vals), np.log(sw2_vals), ELBO, [-2.3, -6.9], norm = norm, cstep = 100, showbeta = False, showZmax = False, xlabel = r&quot;$ ln( sigma_b^2)$&quot;, ylabel = r&quot;$ ln( sigma_w^2)$&quot;, zlabel = &quot;ln(ELBO)&quot;, ) plt.tight_layout() plt.show() . .",
            "url": "https://banskt.github.io/iridge-notes/2021/01/11/effect-of-sigmas-on-elbo.html",
            "relUrl": "/2021/01/11/effect-of-sigmas-on-elbo.html",
            "date": " • Jan 11, 2021"
        }
        
    
  
    
        ,"post6": {
            "title": "Factorization of var(W) in EBMR with product of normals",
            "content": "About . Here, I am checking whether the factorization of $ mathrm{var} left( mathbf{w} right)$ has any effect on the optimization in the variational approximation of EBMR. Earlier, I found that the variational approximation for the product of two normals leads to severe underfitting (see here). . import numpy as np import pandas as pd from scipy import linalg as sc_linalg import matplotlib.pyplot as plt import sys sys.path.append(&quot;../../ebmrPy/&quot;) from inference.ebmr import EBMR from inference import f_elbo from inference import f_sigma from inference import penalized_em from utils import log_density sys.path.append(&quot;../../utils/&quot;) import mpl_stylesheet mpl_stylesheet.banskt_presentation(fontfamily = &#39;latex-clearsans&#39;, fontsize = 18, colors = &#39;banskt&#39;, dpi = 72) . . Toy example . The same trend-filtering data as used previously. . def standardize(X): Xnorm = (X - np.mean(X, axis = 0)) #Xstd = Xnorm / np.std(Xnorm, axis = 0) Xstd = Xnorm / np.sqrt((Xnorm * Xnorm).sum(axis = 0)) return Xstd def trend_data(n, p, bval = 1.0, sd = 1.0, seed=100): np.random.seed(seed) X = np.zeros((n, p)) for i in range(p): X[i:n, i] = np.arange(1, n - i + 1) #X = standardize(X) btrue = np.zeros(p) idx = int(n / 3) btrue[idx] = bval btrue[idx + 1] = -bval y = np.dot(X, btrue) + np.random.normal(0, sd, n) # y = y / np.std(y) return X, y, btrue . . n = 100 p = 200 bval = 8.0 sd = 2.0 X, y, btrue = trend_data(n, p, bval = bval, sd = sd) fig = plt.figure() ax1 = fig.add_subplot(111) ax1.plot(np.arange(n), np.dot(X, btrue), label = &quot;Xb&quot;) ax1.scatter(np.arange(n), y, edgecolor = &#39;black&#39;, facecolor=&#39;white&#39;, label = &quot;Xb + e&quot;) ax1.legend() ax1.set_xlabel(&quot;Sample index&quot;) ax1.set_ylabel(&quot;y&quot;) plt.show() . . Factorization of var(w) . Here, I am assuming $ mathbf{w}$ is not factorized and I use the expectation of $ mathbf{W}^{ mathsf{T}} mathbf{X}^{ mathsf{T}} mathbf{X} mathbf{W}$ involving the cross terms, then we will keep the off-diagonal terms in $ mathbf{ Lambda}_w$ and there will be corresponding changes in the estimation of all parameters involved. . def ridge_mll(X, y, s2, sb2, W): n, p = X.shape Xscale = np.dot(X, np.diag(W)) XWWtXt = np.dot(Xscale, Xscale.T) sigmay = s2 * (np.eye(n) + sb2 * XWWtXt) muy = np.zeros((n, 1)) return log_density.mgauss(y.reshape(-1,1), muy, sigmay) def grr_step(X, y, s2, sb2, muW, varW, XTX, XTy, useVW=True): n, p = X.shape W = np.diag(muW) WtXtXW = np.linalg.multi_dot([W.T, XTX, W]) VW = np.multiply(XTX, varW) if useVW else np.zeros((p, p)) ExpWtXtXW = WtXtXW + VW sigmabinv = (ExpWtXtXW + np.eye(p) * s2 / sb2) / s2 sigmab = np.linalg.inv(sigmabinv) mub = np.linalg.multi_dot([sigmab, W.T, XTy]) / s2 XWmu = np.linalg.multi_dot([X, W, mub]) mub2 = np.square(mub) s2 = (np.sum(np.square(y - XWmu)) + np.dot(ExpWtXtXW, sigmab).trace() + np.linalg.multi_dot([mub.T, VW, mub])) / n sb2 = (np.sum(mub2) + sigmab.trace()) / p return s2, sb2, mub, sigmab def elbo(X, y, s2, sb2, sw2, mub, sigmab, Wbar, varW, XTX, useVW=True): &#39;&#39;&#39; Wbar is a vector which contains the diagonal elements of the diagonal matrix W W = diag_matrix(Wbar) Wbar = diag(W) -- VW is a vector which contains the diagonal elements of the diagonal matrix V_w &#39;&#39;&#39; n, p = X.shape VW = np.multiply(XTX, varW) if useVW else np.zeros((p, p)) elbo = c_func(n, p, s2, sb2, sw2) + h1_func(X, y, s2, sb2, sw2, mub, Wbar, VW) + h2_func(p, s2, sb2, sw2, XTX, Wbar, sigmab, varW, VW) return elbo def c_func(n, p, s2, sb2, sw2): val = p val += - 0.5 * n * np.log(2.0 * np.pi * s2) val += - 0.5 * p * np.log(sb2) val += - 0.5 * p * np.log(sw2) return val def h1_func(X, y, s2, sb2, sw2, mub, Wbar, VW): XWmu = np.linalg.multi_dot([X, np.diag(Wbar), mub]) val1 = - (0.5 / s2) * np.sum(np.square(y - XWmu)) val2 = - 0.5 * np.linalg.multi_dot([mub.T, VW, mub]) / s2 val3 = - 0.5 * np.sum(np.square(mub)) / sb2 val4 = - 0.5 * np.sum(np.square(Wbar)) / sw2 val = val1 + val2 + val3 + val4 return val def h2_func(p, s2, sb2, sw2, XTX, Wbar, sigmab, sigmaw, VW): (sign, logdetS) = np.linalg.slogdet(sigmab) (sign, logdetV) = np.linalg.slogdet(sigmaw) W = np.diag(Wbar) WtXtXW = np.linalg.multi_dot([W.T, XTX, W]) val = 0.5 * logdetS + 0.5 * logdetV val += - 0.5 * np.trace(sigmab) / sb2 - 0.5 * np.trace(sigmaw) / sw2 val += - 0.5 * np.dot(WtXtXW + VW, sigmab).trace() / s2 return val def ebmr_WB2(X, y, s2_init = 1.0, sb2_init = 1.0, sw2_init = 1.0, binit = None, winit = None, use_wb_variance=True, max_iter = 1000, tol = 1e-8 ): XTX = np.dot(X.T, X) XTy = np.dot(X.T, y) n_samples, n_features = X.shape elbo_path = np.zeros(max_iter + 1) mll_path = np.zeros(max_iter + 1) &#39;&#39;&#39; Iteration 0 &#39;&#39;&#39; niter = 0 s2 = s2_init sb2 = sb2_init sw2 = sw2_init mub = np.ones(n_features) if binit is None else binit muw = np.ones(n_features) if winit is None else winit sigmab = np.zeros((n_features, n_features)) sigmaw = np.zeros((n_features, n_features)) elbo_path[0] = -np.inf mll_path[0] = -np.inf for itn in range(1, max_iter + 1): &#39;&#39;&#39; GRR for b &#39;&#39;&#39; s2, sb2, mub, sigmab = grr_step(X, y, s2, sb2, muw, sigmaw, XTX, XTy, useVW=use_wb_variance) &#39;&#39;&#39; GRR for W &#39;&#39;&#39; __, sw2, muw, sigmaw = grr_step(X, y, s2, sw2, mub, sigmab, XTX, XTy, useVW=use_wb_variance) &#39;&#39;&#39; Convergence &#39;&#39;&#39; niter += 1 elbo_path[itn] = elbo(X, y, s2, sb2, sw2, mub, sigmab, muw, sigmaw, XTX, useVW=use_wb_variance) mll_path[itn] = ridge_mll(X, y, s2, sb2, muw) if elbo_path[itn] - elbo_path[itn - 1] &lt; tol: break #if mll_path[itn] - mll_path[itn - 1] &lt; tol: break return s2, sb2, sw2, mub, sigmab, muw, sigmaw, niter, elbo_path[:niter + 1], mll_path[:niter + 1] . . However, there is still an underfitting. . m2 = ebmr_WB2(X, y) s2, sb2, sw2, mub, sigmab, W, sigmaW, niter, elbo_path, mll_path = m2 bpred = mub * W ypred = np.dot(X, bpred) fig = plt.figure(figsize = (12, 12)) ax1 = fig.add_subplot(221) ax2 = fig.add_subplot(222) ax3 = fig.add_subplot(223) #ax4 = fig.add_subplot(224) yvals = np.log(np.max(elbo_path[1:]) - elbo_path[1:] + 1) ax1.scatter(np.arange(niter), yvals, edgecolor = &#39;black&#39;, facecolor=&#39;white&#39;) ax1.plot(np.arange(niter), yvals) ax1.set_xlabel(&quot;Iterations&quot;) ax1.set_ylabel(&quot;log (max(ELBO) - ELBO[itn] + 1)&quot;) ax2.scatter(np.arange(n), y, edgecolor = &#39;black&#39;, facecolor=&#39;white&#39;) ax2.plot(np.arange(n), ypred, color = &#39;salmon&#39;, label=&quot;Predicted&quot;) ax2.plot(np.arange(n), np.dot(X, btrue), color = &#39;dodgerblue&#39;, label=&quot;True&quot;) ax2.legend() ax2.set_xlabel(&quot;Sample Index&quot;) ax2.set_ylabel(&quot;y&quot;) ax3.scatter(np.arange(p), btrue, edgecolor = &#39;black&#39;, facecolor=&#39;white&#39;, label=&quot;True&quot;) ax3.scatter(np.arange(p), bpred, label=&quot;Predicted&quot;) ax3.legend() ax3.set_xlabel(&quot;Predictor Index&quot;) ax3.set_ylabel(&quot;wb&quot;) # nstep = min(80, niter - 2) # ax4.scatter(np.arange(nstep), mll_path[-nstep:], edgecolor = &#39;black&#39;, facecolor=&#39;white&#39;, label=&quot;Evidence&quot;) # ax4.plot(np.arange(nstep), elbo_path[-nstep:], label=&quot;ELBO&quot;) # ax4.legend() # ax4.set_xlabel(&quot;Iterations&quot;) # ax4.set_ylabel(&quot;ELBO / Evidence&quot;) plt.tight_layout() plt.show() . . As before, if I set $ mathbf{ Lambda}_w = mathbf{ Lambda}_b = mathbf{0}$, then we get back the simple EM updates leading to optimal prediction. . m3 = ebmr_WB2(X, y, use_wb_variance=False) s2, sb2, sw2, mub, sigmab, W, sigmaW, niter, elbo_path, mll_path = m3 bpred = mub * W ypred = np.dot(X, bpred) fig = plt.figure(figsize = (12, 12)) ax1 = fig.add_subplot(221) ax2 = fig.add_subplot(222) ax3 = fig.add_subplot(223) #ax4 = fig.add_subplot(224) yvals = np.log(np.max(elbo_path[1:]) - elbo_path[1:] + 1) ax1.scatter(np.arange(niter), yvals, edgecolor = &#39;black&#39;, facecolor=&#39;white&#39;) ax1.plot(np.arange(niter), yvals) ax1.set_xlabel(&quot;Iterations&quot;) ax1.set_ylabel(&quot;log (max(ELBO) - ELBO[itn] + 1)&quot;) ax2.scatter(np.arange(n), y, edgecolor = &#39;black&#39;, facecolor=&#39;white&#39;) ax2.plot(np.arange(n), ypred, color = &#39;salmon&#39;, label=&quot;Predicted&quot;) ax2.plot(np.arange(n), np.dot(X, btrue), color = &#39;dodgerblue&#39;, label=&quot;True&quot;) ax2.legend() ax2.set_xlabel(&quot;Sample Index&quot;) ax2.set_ylabel(&quot;y&quot;) ax3.scatter(np.arange(p), btrue, edgecolor = &#39;black&#39;, facecolor=&#39;white&#39;, label=&quot;True&quot;) ax3.scatter(np.arange(p), bpred, label=&quot;Predicted&quot;) ax3.legend() ax3.set_xlabel(&quot;Predictor Index&quot;) ax3.set_ylabel(&quot;wb&quot;) # nstep = min(80, niter) # ax4.scatter(np.arange(nstep), mll_path[-nstep:], edgecolor = &#39;black&#39;, facecolor=&#39;white&#39;, label=&quot;Evidence&quot;) # ax4.plot(np.arange(nstep), elbo_path[-nstep:], label=&quot;ELBO&quot;) # ax4.legend() # ax4.set_xlabel(&quot;Iterations&quot;) # ax4.set_ylabel(&quot;ELBO / Evidence&quot;) plt.tight_layout() plt.show() . .",
            "url": "https://banskt.github.io/iridge-notes/2021/01/11/ebmr-factorization-variance.html",
            "relUrl": "/2021/01/11/ebmr-factorization-variance.html",
            "date": " • Jan 11, 2021"
        }
        
    
  
    
        ,"post7": {
            "title": "Check ELBO for VEB with product of two normals in case of simple regression",
            "content": "About . Peter suggested to check whether the ELBO calculation reduces to the correct value if $p=1$. Here, I am calculating the ELBO for some given values of the hyperparameters and variational parameters. I am comparing the numerical values obtained from the ELBO code written earlier, and the simpler version of the ELBO using univariate normals. Both of them are same, at least numerically, in the limit of $p=1$. . import numpy as np import pandas as pd from scipy import linalg as sc_linalg import matplotlib.pyplot as plt import sys sys.path.append(&quot;../../ebmrPy/&quot;) from inference.ebmr import EBMR from inference import f_elbo from inference import f_sigma from inference import penalized_em from utils import log_density sys.path.append(&quot;../../utils/&quot;) import mpl_stylesheet mpl_stylesheet.banskt_presentation(fontfamily = &#39;latex-clearsans&#39;, fontsize = 18, colors = &#39;banskt&#39;, dpi = 72) . . def standardize(X): Xnorm = (X - np.mean(X, axis = 0)) #Xstd = Xnorm / np.std(Xnorm, axis = 0) Xstd = Xnorm / np.sqrt((Xnorm * Xnorm).sum(axis = 0)) return Xstd def lasso_data(nsample, nvar, neff, errsigma, sb2 = 100, seed=100): np.random.seed(seed) X = np.random.normal(0, 1, nsample * nvar).reshape(nsample, nvar) X = standardize(X) btrue = np.zeros(nvar) bidx = np.random.choice(nvar, neff , replace = False) btrue[bidx] = np.random.normal(0, np.sqrt(sb2), neff) y = np.dot(X, btrue) + np.random.normal(0, errsigma, nsample) y = y - np.mean(y) #y = y / np.std(y) return X, y, btrue def lims_xy(ax): lims = [ np.min([ax.get_xlim(), ax.get_ylim()]), # min of both axes np.max([ax.get_xlim(), ax.get_ylim()]), # max of both axes ] return lims def plot_diag(ax): lims = lims_xy(ax) ax.plot(lims, lims, ls=&#39;dotted&#39;, color=&#39;gray&#39;) . . n = 100 p = 1 peff = 1 sb2 = 100.0 sd = 2.0 X, y, btrue = lasso_data(n, p, peff, sd, sb2) fig = plt.figure() ax1 = fig.add_subplot(111) ax1.scatter(np.dot(X,btrue), y) plot_diag(ax1) ax1.set_xlabel(&quot;Xb&quot;) ax1.set_ylabel(&quot;y&quot;) plt.show() . . def elbo(X, y, s2, sb2, sw2, mub, sigmab, Wbar, varW, XTX, useVW=True): &#39;&#39;&#39; Wbar is a vector which contains the diagonal elements of the diagonal matrix W W = diag_matrix(Wbar) Wbar = diag(W) -- VW is a vector which contains the diagonal elements of the diagonal matrix V_w &#39;&#39;&#39; n, p = X.shape VW = np.diag(XTX) * np.diag(varW) if useVW else np.zeros(p) elbo = c_func(n, p, s2, sb2, sw2) + h1_func(X, y, s2, sb2, sw2, mub, Wbar, VW) + h2_func(p, s2, sb2, sw2, XTX, Wbar, sigmab, varW, VW) return elbo def c_func(n, p, s2, sb2, sw2): val = p val += - 0.5 * n * np.log(2.0 * np.pi * s2) val += - 0.5 * p * np.log(sb2) val += - 0.5 * p * np.log(sw2) return val def h1_func(X, y, s2, sb2, sw2, mub, Wbar, VW): XWmu = np.linalg.multi_dot([X, np.diag(Wbar), mub]) val1 = - (0.5 / s2) * np.sum(np.square(y - XWmu)) val2 = - 0.5 * np.sum(np.square(mub) * ((VW / s2) + (1 / sb2))) val3 = - 0.5 * np.sum(np.square(Wbar)) / sw2 val = val1 + val2 + val3 return val def h2_func(p, s2, sb2, sw2, XTX, Wbar, sigmab, sigmaw, VW): (sign, logdetS) = np.linalg.slogdet(sigmab) (sign, logdetV) = np.linalg.slogdet(sigmaw) W = np.diag(Wbar) WtXtXW = np.linalg.multi_dot([W.T, XTX, W]) val = 0.5 * logdetS + 0.5 * logdetV val += - 0.5 * np.trace(sigmab) / sb2 - 0.5 * np.trace(sigmaw) / sw2 val += - 0.5 * np.dot(WtXtXW + np.diag(VW), sigmab).trace() / s2 return val def KL_qp_mvn(p, s2, M, S): (sign, logdetS) = np.linalg.slogdet(S) KL = 0.5 * (np.dot(M.T, M) + np.trace(S)) / s2 KL += - 0.5 * logdetS KL += - 0.5 * p + 0.5 * p * np.log(s2) return KL def elbo_full(X, y, s2, sb2, sw2, mub, sigmab, Wbar, varW, XTX, useVW=True): n, p = X.shape VW = np.diag(XTX) * np.diag(varW) if useVW else np.zeros(p) KLqb = KL_qp_mvn(p, sb2, mub, sigmab) KLqw = KL_qp_mvn(p, sw2, Wbar, varW) XWmu = np.linalg.multi_dot([X, np.diag(Wbar), mub]) W = np.diag(Wbar) WtXtXW = np.linalg.multi_dot([W.T, XTX, W]) t1 = - 0.5 * n * np.log(2.0 * np.pi * s2) t2 = - (0.5 / s2) * np.sum(np.square(y - XWmu)) t3 = - 0.5 * np.sum(np.square(mub) * (VW / s2)) t4 = - 0.5 * np.dot(WtXtXW + np.diag(VW), sigmab).trace() / s2 Eqlnpy = t1 + t2 + t3 + t4 elbo = Eqlnpy - KLqb - KLqw return elbo def elbo_simple(X, y, s2, sb2, sw2, mub, sigmab2, muw, sigmaw2): KLqb = KL_qp_normals(0, sb2, mub, sigmab2) KLqw = KL_qp_normals(0, sw2, muw, sigmaw2) Eb2 = mub * mub + sigmab2 Ew2 = muw * muw + sigmaw2 t1 = - 0.5 * n * np.log(2 * np.pi * s2) bhat = np.repeat(mub * muw, p) t2 = - 0.5 * np.sum(np.square(y - np.dot(X, bhat))) / s2 t3 = - 0.5 * np.sum(np.square(X)) * (Eb2 * Ew2 - mub * mub * muw * muw) / s2 Eqlnpy = t1 + t2 + t3 elbo = Eqlnpy - KLqb - KLqw return elbo def KL_qp_normals(m1, s1sq, m2, s2sq): val = 0.5 * (np.log(s1sq / s2sq) + (s2sq / s1sq) - 1 + (np.square(m1 - m2) / s1sq)) return val . . s2 = sd * sd sw2 = 0.5 * 0.5 muw = 2.0 mub = btrue[0] / muw sigmab2 = 0.2 * 0.2 sigmaw2 = 0.1 * 0.1 . elbo_simple(X, y, s2, sb2, sw2, mub, sigmab2, muw, sigmaw2) . -229.70090826648755 . elbo_full(X, y, s2, sb2, sw2, np.array([mub]), np.eye(p) * sigmab2, np.array([muw]), np.eye(p) * sigmaw2, np.dot(X.T, X)) . -229.70090826648755 . elbo(X, y, s2, sb2, sw2, np.array([mub]), np.eye(p) * sigmab2, np.array([muw]), np.eye(p) * sigmaw2, np.dot(X.T, X)) . -229.70090826648755 .",
            "url": "https://banskt.github.io/iridge-notes/2021/01/11/check-elbo-for-simple-regression.html",
            "relUrl": "/2021/01/11/check-elbo-for-simple-regression.html",
            "date": " • Jan 11, 2021"
        }
        
    
  
    
        ,"post8": {
            "title": "Initialization in EBMR with product of normals",
            "content": "About . Here, I am checking whether the initialization improves the optimization in the variational approximation of EBMR with product of normals. Earlier, I found that the variational approximation for the product of two normals leads to severe underfitting (see here). The sequence of updates, or a symmetric update using $ displaystyle q left( mathbf{b}, mathbf{w} right) = q left( mathbf{b} right) q left( mathbf{w} right) $ does not help. . import numpy as np import pandas as pd from scipy import linalg as sc_linalg import matplotlib.pyplot as plt import sys sys.path.append(&quot;../../ebmrPy/&quot;) from inference.ebmr import EBMR from inference import f_elbo from inference import f_sigma from inference import penalized_em from utils import log_density sys.path.append(&quot;../../utils/&quot;) import mpl_stylesheet mpl_stylesheet.banskt_presentation(fontfamily = &#39;latex-clearsans&#39;, fontsize = 18, colors = &#39;banskt&#39;, dpi = 72) . . Toy example . The same trend-filtering data as used previously. . def standardize(X): Xnorm = (X - np.mean(X, axis = 0)) #Xstd = Xnorm / np.std(Xnorm, axis = 0) Xstd = Xnorm / np.sqrt((Xnorm * Xnorm).sum(axis = 0)) return Xstd def trend_data(n, p, bval = 1.0, sd = 1.0, seed=100): np.random.seed(seed) X = np.zeros((n, p)) for i in range(p): X[i:n, i] = np.arange(1, n - i + 1) #X = standardize(X) btrue = np.zeros(p) idx = int(n / 3) btrue[idx] = bval btrue[idx + 1] = -bval y = np.dot(X, btrue) + np.random.normal(0, sd, n) # y = y / np.std(y) return X, y, btrue . . n = 100 p = 200 bval = 8.0 sd = 2.0 X, y, btrue = trend_data(n, p, bval = bval, sd = sd) fig = plt.figure() ax1 = fig.add_subplot(111) ax1.plot(np.arange(n), np.dot(X, btrue), label = &quot;Xb&quot;) ax1.scatter(np.arange(n), y, edgecolor = &#39;black&#39;, facecolor=&#39;white&#39;, label = &quot;Xb + e&quot;) ax1.legend() ax1.set_xlabel(&quot;Sample index&quot;) ax1.set_ylabel(&quot;y&quot;) plt.show() . . Initialize all parameters to optimal values . First, I will get an optimal fit, using point estimates of $ mathbf{b}$ and $ mathbf{w}$ (without using the contribution of their variance). And then initialize the second optimization (including the contribution of the variances of $ mathbf{b}$ and $ mathbf{w}$) from the optimal initial values of all parameters. . def ridge_mll(X, y, s2, sb2, W): n, p = X.shape Xscale = np.dot(X, np.diag(W)) XWWtXt = np.dot(Xscale, Xscale.T) sigmay = s2 * (np.eye(n) + sb2 * XWWtXt) muy = np.zeros((n, 1)) return log_density.mgauss(y.reshape(-1,1), muy, sigmay) def grr_step(X, y, s2, sb2, muW, varW, XTX, XTy, useVW=True): n, p = X.shape W = np.diag(muW) WtXtXW = np.linalg.multi_dot([W.T, XTX, W]) VW = np.diag(XTX) * np.diag(varW) if useVW else np.zeros(p) sigmabinv = (WtXtXW + np.diag(VW) + np.eye(p) * s2 / sb2) / s2 sigmab = np.linalg.inv(sigmabinv) mub = np.linalg.multi_dot([sigmab, W.T, XTy]) / s2 XWmu = np.linalg.multi_dot([X, W, mub]) mub2 = np.square(mub) s2 = (np.sum(np.square(y - XWmu)) + np.dot((WtXtXW + np.diag(VW)), sigmab).trace() + np.sum(mub2 * VW)) / n sb2 = (np.sum(mub2) + sigmab.trace()) / p return s2, sb2, mub, sigmab def elbo(X, y, s2, sb2, sw2, mub, sigmab, Wbar, varW, XTX, useVW=True): &#39;&#39;&#39; Wbar is a vector which contains the diagonal elements of the diagonal matrix W W = diag_matrix(Wbar) Wbar = diag(W) -- VW is a vector which contains the diagonal elements of the diagonal matrix V_w &#39;&#39;&#39; n, p = X.shape VW = np.diag(XTX) * np.diag(varW) if useVW else np.zeros(p) elbo = c_func(n, p, s2, sb2, sw2) + h1_func(X, y, s2, sb2, sw2, mub, Wbar, VW) + h2_func(p, s2, sb2, sw2, XTX, Wbar, sigmab, varW, VW) return elbo def c_func(n, p, s2, sb2, sw2): val = p val += - 0.5 * n * np.log(2.0 * np.pi * s2) val += - 0.5 * p * np.log(sb2) val += - 0.5 * p * np.log(sw2) return val def h1_func(X, y, s2, sb2, sw2, mub, Wbar, VW): XWmu = np.linalg.multi_dot([X, np.diag(Wbar), mub]) val1 = - (0.5 / s2) * np.sum(np.square(y - XWmu)) val2 = - 0.5 * np.sum(np.square(mub) * ((VW / s2) + (1 / sb2))) val3 = - 0.5 * np.sum(np.square(Wbar)) / sw2 val = val1 + val2 + val3 return val def h2_func(p, s2, sb2, sw2, XTX, Wbar, sigmab, sigmaw, VW): (sign, logdetS) = np.linalg.slogdet(sigmab) (sign, logdetV) = np.linalg.slogdet(sigmaw) W = np.diag(Wbar) WtXtXW = np.linalg.multi_dot([W.T, XTX, W]) val = 0.5 * logdetS + 0.5 * logdetV val += - 0.5 * np.trace(sigmab) / sb2 - 0.5 * np.trace(sigmaw) / sw2 val += - 0.5 * np.dot(WtXtXW + np.diag(VW), sigmab).trace() / s2 return val def ebmr_WB2(X, y, s2_init = 1.0, sb2_init = 1.0, sw2_init = 1.0, binit = None, winit = None, sigmab_init = None, sigmaw_init = None, use_wb_variance = True, ignore_b_update = False, ignore_w_update = False, ignore_s2_update = False, max_iter = 1000, tol = 1e-8 ): XTX = np.dot(X.T, X) XTy = np.dot(X.T, y) n_samples, n_features = X.shape elbo_path = np.zeros(max_iter + 1) mll_path = np.zeros(max_iter + 1) &#39;&#39;&#39; Iteration 0 &#39;&#39;&#39; niter = 0 s2 = s2_init sb2 = sb2_init sw2 = sw2_init mub = np.ones(n_features) if binit is None else binit muw = np.ones(n_features) if winit is None else winit sigmab = np.zeros((n_features, n_features)) if sigmab_init is None else sigmab_init sigmaw = np.zeros((n_features, n_features)) if sigmaw_init is None else sigmaw_init elbo_path[0] = -np.inf mll_path[0] = -np.inf for itn in range(1, max_iter + 1): &#39;&#39;&#39; GRR for b &#39;&#39;&#39; if not ignore_b_update: if ignore_s2_update: #print (&quot;Updating b without s2&quot;) __, sb2, mub, sigmab = grr_step(X, y, s2, sb2, muw, sigmaw, XTX, XTy, useVW=use_wb_variance) else: #print (&quot;Updating b&quot;) s2, sb2, mub, sigmab = grr_step(X, y, s2, sb2, muw, sigmaw, XTX, XTy, useVW=use_wb_variance) &#39;&#39;&#39; GRR for W &#39;&#39;&#39; if not ignore_w_update: #print (&quot;Updating w&quot;) __, sw2, muw, sigmaw = grr_step(X, y, s2, sw2, mub, sigmab, XTX, XTy, useVW=use_wb_variance) &#39;&#39;&#39; Convergence &#39;&#39;&#39; niter += 1 elbo_path[itn] = elbo(X, y, s2, sb2, sw2, mub, sigmab, muw, sigmaw, XTX, useVW=use_wb_variance) mll_path[itn] = ridge_mll(X, y, s2, sb2, muw) if elbo_path[itn] - elbo_path[itn - 1] &lt; tol: break #if mll_path[itn] - mll_path[itn - 1] &lt; tol: break return s2, sb2, sw2, mub, sigmab, muw, sigmaw, niter, elbo_path[:niter + 1], mll_path[:niter + 1] . . It leads to similar underfitting as we have seen before. . m1 = ebmr_WB2(X, y, use_wb_variance=False) # s2, sb2, sw2, mub, sigmab, W, sigmaW, niter, elbo_path, mll_path = m1 m2 = ebmr_WB2(X, y, use_wb_variance=True, s2_init = m1[0], sb2_init = m1[1], sw2_init = m1[2], binit = m1[3], sigmab_init = m1[4], winit = m1[5], sigmaw_init = m1[6] ) s2, sb2, sw2, mub, sigmab, W, sigmaW, niter, elbo_path, mll_path = m2 bpred = mub * W ypred = np.dot(X, bpred) fig = plt.figure(figsize = (12, 12)) ax1 = fig.add_subplot(221) ax2 = fig.add_subplot(222) ax3 = fig.add_subplot(223) #ax4 = fig.add_subplot(224) yvals = np.log(np.max(elbo_path[1:]) - elbo_path[1:] + 1) ax1.scatter(np.arange(niter), yvals, edgecolor = &#39;black&#39;, facecolor=&#39;white&#39;) ax1.plot(np.arange(niter), yvals) ax1.set_xlabel(&quot;Iterations&quot;) ax1.set_ylabel(&quot;log (max(ELBO) - ELBO[itn] + 1)&quot;) ax2.scatter(np.arange(n), y, edgecolor = &#39;black&#39;, facecolor=&#39;white&#39;) ax2.plot(np.arange(n), ypred, color = &#39;salmon&#39;, label=&quot;Predicted&quot;) ax2.plot(np.arange(n), np.dot(X, btrue), color = &#39;dodgerblue&#39;, label=&quot;True&quot;) ax2.legend() ax2.set_xlabel(&quot;Sample Index&quot;) ax2.set_ylabel(&quot;y&quot;) ax3.scatter(np.arange(p), btrue, edgecolor = &#39;black&#39;, facecolor=&#39;white&#39;, label=&quot;True&quot;) ax3.scatter(np.arange(p), bpred, label=&quot;Predicted&quot;) ax3.legend() ax3.set_xlabel(&quot;Predictor Index&quot;) ax3.set_ylabel(&quot;wb&quot;) # nstep = min(80, niter - 2) # ax4.scatter(np.arange(nstep), mll_path[-nstep:], edgecolor = &#39;black&#39;, facecolor=&#39;white&#39;, label=&quot;Evidence&quot;) # ax4.plot(np.arange(nstep), elbo_path[-nstep:], label=&quot;ELBO&quot;) # ax4.legend() # ax4.set_xlabel(&quot;Iterations&quot;) # ax4.set_ylabel(&quot;ELBO / Evidence&quot;) plt.tight_layout() plt.show() . . Fix initial values of $ sigma^2$. . Next, I will use the initial values from the &quot;optimal&quot; fit and fix the value for $ sigma^2$ and allow the updates of all other parameters. . m3 = ebmr_WB2(X, y, use_wb_variance=True, s2_init = m1[0], sb2_init = m1[1], sw2_init = m1[2], binit = m1[3], sigmab_init = m1[4], winit = m1[5], sigmaw_init = m1[6], ignore_s2_update=True, ) s2, sb2, sw2, mub, sigmab, W, sigmaW, niter, elbo_path, mll_path = m3 bpred = mub * W ypred = np.dot(X, bpred) fig = plt.figure(figsize = (12, 12)) ax1 = fig.add_subplot(221) ax2 = fig.add_subplot(222) ax3 = fig.add_subplot(223) #ax4 = fig.add_subplot(224) yvals = np.log(np.max(elbo_path[1:]) - elbo_path[1:] + 1) ax1.scatter(np.arange(niter), yvals, edgecolor = &#39;black&#39;, facecolor=&#39;white&#39;) ax1.plot(np.arange(niter), yvals) ax1.set_xlabel(&quot;Iterations&quot;) ax1.set_ylabel(&quot;log (max(ELBO) - ELBO[itn] + 1)&quot;) ax2.scatter(np.arange(n), y, edgecolor = &#39;black&#39;, facecolor=&#39;white&#39;) ax2.plot(np.arange(n), ypred, color = &#39;salmon&#39;, label=&quot;Predicted&quot;) ax2.plot(np.arange(n), np.dot(X, btrue), color = &#39;dodgerblue&#39;, label=&quot;True&quot;) ax2.legend() ax2.set_xlabel(&quot;Sample Index&quot;) ax2.set_ylabel(&quot;y&quot;) ax3.scatter(np.arange(p), btrue, edgecolor = &#39;black&#39;, facecolor=&#39;white&#39;, label=&quot;True&quot;) ax3.scatter(np.arange(p), bpred, label=&quot;Predicted&quot;) ax3.legend() ax3.set_xlabel(&quot;Predictor Index&quot;) ax3.set_ylabel(&quot;wb&quot;) # nstep = min(80, niter) # ax4.scatter(np.arange(nstep), mll_path[-nstep:], edgecolor = &#39;black&#39;, facecolor=&#39;white&#39;, label=&quot;Evidence&quot;) # ax4.plot(np.arange(nstep), elbo_path[-nstep:], label=&quot;ELBO&quot;) # ax4.legend() # ax4.set_xlabel(&quot;Iterations&quot;) # ax4.set_ylabel(&quot;ELBO / Evidence&quot;) plt.tight_layout() plt.show() . . Fix initial values of $ sigma^2$, $ sigma_b^2$, $ mathbf{m}$ and $ mathbf{S}$. . Next, I will use the initial values from the &quot;optimal&quot; fit and fix the values for the updates of $ mathbf{b}$. . m4 = ebmr_WB2(X, y, use_wb_variance=True, s2_init = m1[0], sb2_init = m1[1], sw2_init = m1[2], binit = m1[3], sigmab_init = m1[4], winit = m1[5], sigmaw_init = m1[6], ignore_b_update=True, ) s2, sb2, sw2, mub, sigmab, W, sigmaW, niter, elbo_path, mll_path = m4 bpred = mub * W ypred = np.dot(X, bpred) fig = plt.figure(figsize = (12, 12)) ax1 = fig.add_subplot(221) ax2 = fig.add_subplot(222) ax3 = fig.add_subplot(223) #ax4 = fig.add_subplot(224) yvals = np.log(np.max(elbo_path[1:]) - elbo_path[1:] + 1) ax1.scatter(np.arange(niter), yvals, edgecolor = &#39;black&#39;, facecolor=&#39;white&#39;) ax1.plot(np.arange(niter), yvals) ax1.set_xlabel(&quot;Iterations&quot;) ax1.set_ylabel(&quot;log (max(ELBO) - ELBO[itn] + 1)&quot;) ax2.scatter(np.arange(n), y, edgecolor = &#39;black&#39;, facecolor=&#39;white&#39;) ax2.plot(np.arange(n), ypred, color = &#39;salmon&#39;, label=&quot;Predicted&quot;) ax2.plot(np.arange(n), np.dot(X, btrue), color = &#39;dodgerblue&#39;, label=&quot;True&quot;) ax2.legend() ax2.set_xlabel(&quot;Sample Index&quot;) ax2.set_ylabel(&quot;y&quot;) ax3.scatter(np.arange(p), btrue, edgecolor = &#39;black&#39;, facecolor=&#39;white&#39;, label=&quot;True&quot;) ax3.scatter(np.arange(p), bpred, label=&quot;Predicted&quot;) ax3.legend() ax3.set_xlabel(&quot;Predictor Index&quot;) ax3.set_ylabel(&quot;wb&quot;) # nstep = min(80, niter) # ax4.scatter(np.arange(nstep), mll_path[-nstep:], edgecolor = &#39;black&#39;, facecolor=&#39;white&#39;, label=&quot;Evidence&quot;) # ax4.plot(np.arange(nstep), elbo_path[-nstep:], label=&quot;ELBO&quot;) # ax4.legend() # ax4.set_xlabel(&quot;Iterations&quot;) # ax4.set_ylabel(&quot;ELBO / Evidence&quot;) plt.tight_layout() plt.show() . . Fix initial values of $ sigma_w^2$, $ mathbf{a}$ and $ mathbf{V}$. . Next, I will use the initial values from the &quot;optimal&quot; fit and fix the values for the updates of $ mathbf{w}$. . m5 = ebmr_WB2(X, y, use_wb_variance=True, s2_init = m1[0], sb2_init = m1[1], sw2_init = m1[2], binit = m1[3], sigmab_init = m1[4], winit = m1[5], sigmaw_init = m1[6], ignore_w_update=True, ) s2, sb2, sw2, mub, sigmab, W, sigmaW, niter, elbo_path, mll_path = m5 bpred = mub * W ypred = np.dot(X, bpred) fig = plt.figure(figsize = (12, 12)) ax1 = fig.add_subplot(221) ax2 = fig.add_subplot(222) ax3 = fig.add_subplot(223) #ax4 = fig.add_subplot(224) yvals = np.log(np.max(elbo_path[1:]) - elbo_path[1:] + 1) ax1.scatter(np.arange(niter), yvals, edgecolor = &#39;black&#39;, facecolor=&#39;white&#39;) ax1.plot(np.arange(niter), yvals) ax1.set_xlabel(&quot;Iterations&quot;) ax1.set_ylabel(&quot;log (max(ELBO) - ELBO[itn] + 1)&quot;) ax2.scatter(np.arange(n), y, edgecolor = &#39;black&#39;, facecolor=&#39;white&#39;) ax2.plot(np.arange(n), ypred, color = &#39;salmon&#39;, label=&quot;Predicted&quot;) ax2.plot(np.arange(n), np.dot(X, btrue), color = &#39;dodgerblue&#39;, label=&quot;True&quot;) ax2.legend() ax2.set_xlabel(&quot;Sample Index&quot;) ax2.set_ylabel(&quot;y&quot;) ax3.scatter(np.arange(p), btrue, edgecolor = &#39;black&#39;, facecolor=&#39;white&#39;, label=&quot;True&quot;) ax3.scatter(np.arange(p), bpred, label=&quot;Predicted&quot;) ax3.legend() ax3.set_xlabel(&quot;Predictor Index&quot;) ax3.set_ylabel(&quot;wb&quot;) # nstep = min(80, niter) # ax4.scatter(np.arange(nstep), mll_path[-nstep:], edgecolor = &#39;black&#39;, facecolor=&#39;white&#39;, label=&quot;Evidence&quot;) # ax4.plot(np.arange(nstep), elbo_path[-nstep:], label=&quot;ELBO&quot;) # ax4.legend() # ax4.set_xlabel(&quot;Iterations&quot;) # ax4.set_ylabel(&quot;ELBO / Evidence&quot;) plt.tight_layout() plt.show() . . The &quot;optimum&quot; initialization and fixed $ sigma^2$ . I found this unexpectedly, but keeping it here if it helps in debug. . Essentially, I obtain &quot;optimum&quot; point estimates of $ mathbf{b}$ and $ mathbf{w}$ (without using the contribution of their variance) and fixing $ sigma^2 = 1.0$. I then use this set of values for initializing the EBMR (including the variances of $ mathbf{b}$ and $ mathbf{w}$) and run the optimization, again with the fixed value of $ sigma^2 = 1.0$. . Although this provides a &quot;visually better&quot; prediction than previous attempts, the coefficients are still estimated poorly. This indicates that I am probably making mistake in the update of $ sigma^2$. . m6 = ebmr_WB2(X, y, use_wb_variance=False, s2_init = 1.0, ignore_s2_update=True, ) m7 = ebmr_WB2(X, y, use_wb_variance=True, s2_init = 1.0, sb2_init = m6[1], sw2_init = m6[2], binit = m6[3], sigmab_init = m6[4], winit = m6[5], sigmaw_init = m6[6], ignore_s2_update = True ) . . s2, sb2, sw2, mub, sigmab, W, sigmaW, niter, elbo_path, mll_path = m7 bpred = mub * W ypred = np.dot(X, bpred) fig = plt.figure(figsize = (12, 12)) ax1 = fig.add_subplot(221) ax2 = fig.add_subplot(222) ax3 = fig.add_subplot(223) #ax4 = fig.add_subplot(224) yvals = np.log(np.max(elbo_path[1:]) - elbo_path[1:] + 1) ax1.scatter(np.arange(niter), yvals, edgecolor = &#39;black&#39;, facecolor=&#39;white&#39;) ax1.plot(np.arange(niter), yvals) ax1.set_xlabel(&quot;Iterations&quot;) ax1.set_ylabel(&quot;log (max(ELBO) - ELBO[itn] + 1)&quot;) ax2.scatter(np.arange(n), y, edgecolor = &#39;black&#39;, facecolor=&#39;white&#39;) ax2.plot(np.arange(n), ypred, color = &#39;salmon&#39;, label=&quot;Predicted&quot;) ax2.plot(np.arange(n), np.dot(X, btrue), color = &#39;dodgerblue&#39;, label=&quot;True&quot;) ax2.legend() ax2.set_xlabel(&quot;Sample Index&quot;) ax2.set_ylabel(&quot;y&quot;) ax3.scatter(np.arange(p), btrue, edgecolor = &#39;black&#39;, facecolor=&#39;white&#39;, label=&quot;True&quot;) ax3.scatter(np.arange(p), bpred, label=&quot;Predicted&quot;) ax3.legend() ax3.set_xlabel(&quot;Predictor Index&quot;) ax3.set_ylabel(&quot;wb&quot;) # nstep = min(80, niter) # ax4.scatter(np.arange(nstep), mll_path[-nstep:], edgecolor = &#39;black&#39;, facecolor=&#39;white&#39;, label=&quot;Evidence&quot;) # ax4.plot(np.arange(nstep), elbo_path[-nstep:], label=&quot;ELBO&quot;) # ax4.legend() # ax4.set_xlabel(&quot;Iterations&quot;) # ax4.set_ylabel(&quot;ELBO / Evidence&quot;) plt.tight_layout() plt.show() . .",
            "url": "https://banskt.github.io/iridge-notes/2021/01/07/initialization-fix-values-ebmr-product-of-normals.html",
            "relUrl": "/2021/01/07/initialization-fix-values-ebmr-product-of-normals.html",
            "date": " • Jan 7, 2021"
        }
        
    
  
    
        ,"post9": {
            "title": "Symmetric updates in EBMR with product of normals",
            "content": "About . Here, I am checking whether the factorization has any effect on the optimization in the variational approximation of EBMR. Earlier, I found that the variational approximation for the product of two normals leads to severe overfitting (see here). . import numpy as np import pandas as pd from scipy import linalg as sc_linalg import matplotlib.pyplot as plt import sys sys.path.append(&quot;../../ebmrPy/&quot;) from inference.ebmr import EBMR from inference import f_elbo from inference import f_sigma from inference import penalized_em from utils import log_density import mpl_stylesheet mpl_stylesheet.banskt_presentation(fontfamily = &#39;latex-clearsans&#39;, fontsize = 18, colors = &#39;banskt&#39;, dpi = 72) . . Toy example . The same trend-filtering data as used previously. . def standardize(X): Xnorm = (X - np.mean(X, axis = 0)) #Xstd = Xnorm / np.std(Xnorm, axis = 0) Xstd = Xnorm / np.sqrt((Xnorm * Xnorm).sum(axis = 0)) return Xstd def trend_data(n, p, bval = 1.0, sd = 1.0, seed=100): np.random.seed(seed) X = np.zeros((n, p)) for i in range(p): X[i:n, i] = np.arange(1, n - i + 1) #X = standardize(X) btrue = np.zeros(p) idx = int(n / 3) btrue[idx] = bval btrue[idx + 1] = -bval y = np.dot(X, btrue) + np.random.normal(0, sd, n) # y = y / np.std(y) return X, y, btrue . . n = 100 p = 200 bval = 8.0 sd = 2.0 X, y, btrue = trend_data(n, p, bval = bval, sd = sd) fig = plt.figure() ax1 = fig.add_subplot(111) ax1.plot(np.arange(n), np.dot(X, btrue), label = &quot;Xb&quot;) ax1.scatter(np.arange(n), y, edgecolor = &#39;black&#39;, facecolor=&#39;white&#39;, label = &quot;Xb + e&quot;) ax1.legend() ax1.set_xlabel(&quot;Sample index&quot;) ax1.set_ylabel(&quot;y&quot;) plt.show() . . Factorization 1 . $ displaystyle q left( mathbf{b}, mathbf{w} right) = q left( mathbf{b} right) prod_{j=1}^{P}q_j left(w_j right) $. . def ridge_mll(X, y, s2, sb2, W): n, p = X.shape Xscale = np.dot(X, np.diag(W)) XWWtXt = np.dot(Xscale, Xscale.T) sigmay = s2 * (np.eye(n) + sb2 * XWWtXt) muy = np.zeros((n, 1)) return log_density.mgauss(y.reshape(-1,1), muy, sigmay) def grr_b(X, y, s2, sb2, Wbar, varWj, XTX, XTy): n, p = X.shape W = np.diag(Wbar) WtXtXW = np.linalg.multi_dot([W.T, XTX, W]) VW = np.diag(XTX) * varWj sigmabinv = (WtXtXW + np.diag(VW) + np.eye(p) * s2 / sb2) / s2 sigmab = np.linalg.inv(sigmabinv) mub = np.linalg.multi_dot([sigmab, W.T, XTy]) / s2 XWmu = np.linalg.multi_dot([X, W, mub]) mub2 = np.square(mub) s2 = (np.sum(np.square(y - XWmu)) + np.dot((WtXtXW + np.diag(VW)), sigmab).trace() + np.sum(mub2 * VW)) / n sb2 = (np.sum(mub2) + sigmab.trace()) / p return s2, sb2, mub, sigmab def grr_W_old(X, y, s2, sw2, mub, sigmab, muWj, XTX, XTy): n, p = X.shape R = np.einsum(&#39;i,j-&gt;ij&#39;, mub, mub) + sigmab XTXRjj = np.array([XTX[j, j] * R[j, j] for j in range(p)]) #wXTXRj = np.array([np.sum(muWj * XTX[:, j] * R[:, j]) - (muWj[j] * XTXRjj[j]) for j in range(p)]) sigmaWj2 = 1 / ((XTXRjj / s2) + (1 / sw2)) for j in range(p): wXTXRj = np.sum(muWj * XTX[:, j] * R[:, j]) - (muWj[j] * XTXRjj[j]) muWj[j] = sigmaWj2[j] * (mub[j] * XTy[j] - 0.5 * wXTXRj) / s2 sw2 = np.sum(np.square(muWj) + sigmaWj2) / p return sw2, muWj, sigmaWj2 def grr_W(X, y, s2, sw2, mub, sigmab, muWj, XTX, XTy): n, p = X.shape R = np.einsum(&#39;i,j-&gt;ij&#39;, mub, mub) + sigmab XTXRjj = np.diag(XTX) * np.diag(R) sigmaWj2inv = (XTXRjj / s2) + (1 / sw2) wXTXRj = np.array([np.sum(muWj * XTX[:, j] * R[:, j]) - (muWj[j] * XTXRjj[j]) for j in range(p)]) sigmaWj2 = 1 / sigmaWj2inv muWj = sigmaWj2 * (mub * XTy - wXTXRj) / s2 sw2 = np.sum(np.square(muWj) + sigmaWj2) / p #sigmaWj2 = np.zeros(p) return sw2, muWj, sigmaWj2 def elbo(X, y, s2, sb2, sw2, mub, sigmab, Wbar, varWj, XTX): &#39;&#39;&#39; Wbar is a vector which contains the diagonal elements of the diagonal matrix W W = diag_matrix(Wbar) Wbar = diag(W) -- VW is a vector which contains the diagonal elements of the diagonal matrix V_w &#39;&#39;&#39; n, p = X.shape VW = np.diag(XTX) * varWj elbo = c_func(n, p, s2, sb2, sw2) + h1_func(X, y, s2, sb2, sw2, mub, Wbar, VW) + h2_func(p, s2, sb2, sw2, XTX, Wbar, sigmab, varWj, VW) return elbo def c_func(n, p, s2, sb2, sw2): val = p val += - 0.5 * n * np.log(2.0 * np.pi * s2) val += - 0.5 * p * np.log(sb2) val += - 0.5 * p * np.log(sw2) return val def h1_func(X, y, s2, sb2, sw2, mub, Wbar, VW): XWmu = np.linalg.multi_dot([X, np.diag(Wbar), mub]) val1 = - (0.5 / s2) * np.sum(np.square(y - XWmu)) val2 = - 0.5 * np.sum(np.square(mub) * ((VW / s2) + (1 / sb2))) val3 = - 0.5 * np.sum(np.square(Wbar)) / sw2 val = val1 + val2 + val3 return val def h2_func(p, s2, sb2, sw2, XTX, Wbar, sigmab, varWj, VW): (sign, logdetS) = np.linalg.slogdet(sigmab) logdetV = np.sum(np.log(varWj)) W = np.diag(Wbar) WtXtXW = np.linalg.multi_dot([W.T, XTX, W]) val = 0.5 * logdetS + 0.5 * logdetV val += - 0.5 * np.trace(sigmab) / sb2 - 0.5 * np.sum(varWj) / sw2 val += - 0.5 * np.dot(WtXtXW + np.diag(VW), sigmab).trace() / s2 return val def ebmr_WB1(X, y, s2_init = 1.0, sb2_init = 1.0, sw2_init = 1.0, binit = None, winit = None, max_iter = 1000, tol = 1e-8 ): XTX = np.dot(X.T, X) XTy = np.dot(X.T, y) n_samples, n_features = X.shape elbo_path = np.zeros(max_iter + 1) mll_path = np.zeros(max_iter + 1) &#39;&#39;&#39; Iteration 0 &#39;&#39;&#39; niter = 0 s2 = s2_init sb2 = sb2_init sw2 = sw2_init mub = np.ones(n_features) if binit is None else binit muWj = np.ones(n_features) if winit is None else winit sigmab = np.zeros((n_features, n_features)) sigmaWj2 = np.zeros(n_features) elbo_path[0] = -np.inf mll_path[0] = -np.inf for itn in range(1, max_iter + 1): &#39;&#39;&#39; GRR for b &#39;&#39;&#39; s2, sb2, mub, sigmab = grr_b(X, y, s2, sb2, muWj, sigmaWj2, XTX, XTy) &#39;&#39;&#39; GRR for W &#39;&#39;&#39; sw2, muWj, sigmaWj2 = grr_W(X, y, s2, sw2, mub, sigmab, muWj, XTX, XTy) &#39;&#39;&#39; Convergence &#39;&#39;&#39; niter += 1 elbo_path[itn] = elbo(X, y, s2, sb2, sw2, mub, sigmab, muWj, sigmaWj2, XTX) mll_path[itn] = ridge_mll(X, y, s2, sb2, muWj) if elbo_path[itn] - elbo_path[itn - 1] &lt; tol: break #if mll_path[itn] - mll_path[itn - 1] &lt; tol: break return s2, sb2, sw2, mub, sigmab, muWj, sigmaWj2, niter, elbo_path[:niter + 1], mll_path[:niter + 1] . . And this leads to overfitting as we have seen previously. . m1 = ebmr_WB1(X, y) s2, sb2, sw2, mub, sigmab, W, sigmaW, niter, elbo_path, mll_path = m1 bpred = mub * W ypred = np.dot(X, bpred) fig = plt.figure(figsize = (12, 12)) ax1 = fig.add_subplot(221) ax2 = fig.add_subplot(222) ax3 = fig.add_subplot(223) #ax4 = fig.add_subplot(224) yvals = np.log(np.max(elbo_path[1:]) - elbo_path[1:] + 1) ax1.scatter(np.arange(niter), yvals, edgecolor = &#39;black&#39;, facecolor=&#39;white&#39;) ax1.plot(np.arange(niter), yvals) ax1.set_xlabel(&quot;Iterations&quot;) ax1.set_ylabel(&quot;log (max(ELBO) - ELBO[itn] + 1)&quot;) ax2.scatter(np.arange(n), y, edgecolor = &#39;black&#39;, facecolor=&#39;white&#39;) ax2.plot(np.arange(n), ypred, color = &#39;salmon&#39;, label=&quot;Predicted&quot;) ax2.plot(np.arange(n), np.dot(X, btrue), color = &#39;dodgerblue&#39;, label=&quot;True&quot;) ax2.legend() ax2.set_xlabel(&quot;Sample Index&quot;) ax2.set_ylabel(&quot;y&quot;) ax3.scatter(np.arange(p), btrue, edgecolor = &#39;black&#39;, facecolor=&#39;white&#39;, label=&quot;True&quot;) ax3.scatter(np.arange(p), bpred, label=&quot;Predicted&quot;) ax3.legend() ax3.set_xlabel(&quot;Predictor Index&quot;) ax3.set_ylabel(&quot;wb&quot;) # nstep = min(80, niter - 2) # ax4.scatter(np.arange(nstep), mll_path[-nstep:], edgecolor = &#39;black&#39;, facecolor=&#39;white&#39;) # ax4.plot(np.arange(nstep), elbo_path[-nstep:]) # ax4.set_xlabel(&quot;Iterations&quot;) # ax4.set_ylabel(&quot;ELBO / Evidence&quot;) plt.tight_layout() plt.show() . . Factorization 2 . $ displaystyle q left( mathbf{b}, mathbf{w} right) = q left( mathbf{b} right) q left( mathbf{w} right) $. . This allows us to use the same update equations for both $ mathbf{b}$ and $ mathbf{w}$. . $q left( mathbf{b} right) = N left( mathbf{b} mid mathbf{m}, mathbf{S} right)$, . $q left( mathbf{w} right) = N left( mathbf{w} mid mathbf{a}, mathbf{V} right)$, . $ displaystyle mathbf{S} = left[ frac{1}{ sigma^2} left( mathbf{A}^{ mathsf{T}} mathbf{X}^{ mathsf{T}} mathbf{X} mathbf{A} + mathbf{ Lambda}_w right) + frac{1}{ sigma_b^2} mathbb{I} right]^{-1}$, . $ displaystyle mathbf{m} = frac{1}{ sigma^2} mathbf{S} mathbf{A}^{ mathsf{T}} mathbf{X}^{ mathsf{T}} mathbf{y}$, . $ displaystyle mathbf{V} = left[ frac{1}{ sigma^2} left( mathbf{M}^{ mathsf{T}} mathbf{X}^{ mathsf{T}} mathbf{X} mathbf{M} + mathbf{ Lambda}_b right) + frac{1}{ sigma_w^2} mathbb{I} right]^{-1}$, . $ displaystyle mathbf{a} = frac{1}{ sigma^2} mathbf{V} mathbf{M}^{ mathsf{T}} mathbf{X}^{ mathsf{T}} mathbf{y}$, . $ mathbf{A}$, $ mathbf{M}$, $ mathbf{ Lambda}_w$ and $ mathbf{ Lambda}_b$ are diagonal matrices whose diagonal elements are given by . $ mathbf{A}_{jj} := a_j$, . $( mathbf{ Lambda}_w)_{jj} := ( mathbf{X}^{ mathsf{T}} mathbf{X})_{jj} mathbf{V}_{jj}^2$, . $ mathbf{M}_{jj} := m_j$, . $( mathbf{ Lambda}_b)_{jj} := ( mathbf{X}^{ mathsf{T}} mathbf{X})_{jj} mathbf{S}_{jj}^2$. . The updates of $ sigma^2$, $ sigma_b^2$ and $ sigma_w^2$ can be obtained by taking the derivative of $F(q( mathbf{b}, mathbf{W}))$ and setting them to 0 respectively. This yields . $ displaystyle sigma^2 = frac{1}{N} left { left Vert mathbf{y} - mathbf{X} mathbf{A} mathbf{m} right Vert_{2}^{2} + mathrm{Tr} left( left( mathbf{A}^{ mathsf{T}} mathbf{X}^{ mathsf{T}} mathbf{X} mathbf{A} + mathbf{ Lambda}_w right) mathbf{S} right) + mathbf{m}^{ mathsf{T}} mathbf{ Lambda}_w mathbf{m} right }$, . $ displaystyle sigma_b^2 = frac{1}{P} mathrm{Tr} left( mathbf{m} mathbf{m}^{ mathsf{T}} + mathbf{S} right)$ and . $ displaystyle sigma_w^2 = frac{1}{P} mathrm{Tr} left( mathbf{a} mathbf{a}^{ mathsf{T}} + mathbf{V} right)$. . The iteration should stop when the ELBO $F(q( mathbf{b}, mathbf{w}))$ converges to a tolerance value (tol). The ELBO can be calculated as . $ displaystyle F(q) = P - frac{N}{2} ln(2 pi sigma^2) - frac{P}{2} ln( sigma_b^2) - frac{P}{2} ln( sigma_w^2) - frac{1}{2 sigma^2} left { left Vert mathbf{y} - mathbf{X} mathbf{A} mathbf{m} right Vert_2^2 + mathrm{Tr} left( left( mathbf{A}^{ mathsf{T}} mathbf{X}^{ mathsf{T}} mathbf{X} mathbf{A} + mathbf{ Lambda}_w right) mathbf{S} right) + mathbf{m}^{ mathsf{T}} mathbf{ Lambda}_w mathbf{m} right } - frac{1}{2 sigma_b^2} mathrm{Tr} left( mathbf{m} mathbf{m}^{ mathsf{T}} + mathbf{S} right) - frac{1}{2 sigma_w^2} mathrm{Tr} left( mathbf{a} mathbf{a}^{ mathsf{T}} + mathbf{V} right) + frac{1}{2} ln left lvert mathbf{S} right rvert + frac{1}{2} ln left lvert mathbf{V} right rvert$ . def ridge_mll(X, y, s2, sb2, W): n, p = X.shape Xscale = np.dot(X, np.diag(W)) XWWtXt = np.dot(Xscale, Xscale.T) sigmay = s2 * (np.eye(n) + sb2 * XWWtXt) muy = np.zeros((n, 1)) return log_density.mgauss(y.reshape(-1,1), muy, sigmay) def grr_step(X, y, s2, sb2, muW, varW, XTX, XTy, useVW=True): n, p = X.shape W = np.diag(muW) WtXtXW = np.linalg.multi_dot([W.T, XTX, W]) VW = np.diag(XTX) * np.diag(varW) if useVW else np.zeros(p) sigmabinv = (WtXtXW + np.diag(VW) + np.eye(p) * s2 / sb2) / s2 sigmab = np.linalg.inv(sigmabinv) mub = np.linalg.multi_dot([sigmab, W.T, XTy]) / s2 XWmu = np.linalg.multi_dot([X, W, mub]) mub2 = np.square(mub) s2 = (np.sum(np.square(y - XWmu)) + np.dot((WtXtXW + np.diag(VW)), sigmab).trace() + np.sum(mub2 * VW)) / n sb2 = (np.sum(mub2) + sigmab.trace()) / p return s2, sb2, mub, sigmab def elbo(X, y, s2, sb2, sw2, mub, sigmab, Wbar, varW, XTX, useVW=True): &#39;&#39;&#39; Wbar is a vector which contains the diagonal elements of the diagonal matrix W W = diag_matrix(Wbar) Wbar = diag(W) -- VW is a vector which contains the diagonal elements of the diagonal matrix V_w &#39;&#39;&#39; n, p = X.shape VW = np.diag(XTX) * np.diag(varW) if useVW else np.zeros(p) elbo = c_func(n, p, s2, sb2, sw2) + h1_func(X, y, s2, sb2, sw2, mub, Wbar, VW) + h2_func(p, s2, sb2, sw2, XTX, Wbar, sigmab, varW, VW) return elbo def c_func(n, p, s2, sb2, sw2): val = p val += - 0.5 * n * np.log(2.0 * np.pi * s2) val += - 0.5 * p * np.log(sb2) val += - 0.5 * p * np.log(sw2) return val def h1_func(X, y, s2, sb2, sw2, mub, Wbar, VW): XWmu = np.linalg.multi_dot([X, np.diag(Wbar), mub]) val1 = - (0.5 / s2) * np.sum(np.square(y - XWmu)) val2 = - 0.5 * np.sum(np.square(mub) * ((VW / s2) + (1 / sb2))) val3 = - 0.5 * np.sum(np.square(Wbar)) / sw2 val = val1 + val2 + val3 return val def h2_func(p, s2, sb2, sw2, XTX, Wbar, sigmab, sigmaw, VW): (sign, logdetS) = np.linalg.slogdet(sigmab) (sign, logdetV) = np.linalg.slogdet(sigmaw) W = np.diag(Wbar) WtXtXW = np.linalg.multi_dot([W.T, XTX, W]) val = 0.5 * logdetS + 0.5 * logdetV val += - 0.5 * np.trace(sigmab) / sb2 - 0.5 * np.trace(sigmaw) / sw2 val += - 0.5 * np.dot(WtXtXW + np.diag(VW), sigmab).trace() / s2 return val def ebmr_WB2(X, y, s2_init = 1.0, sb2_init = 1.0, sw2_init = 1.0, binit = None, winit = None, use_wb_variance=True, max_iter = 1000, tol = 1e-8 ): XTX = np.dot(X.T, X) XTy = np.dot(X.T, y) n_samples, n_features = X.shape elbo_path = np.zeros(max_iter + 1) mll_path = np.zeros(max_iter + 1) &#39;&#39;&#39; Iteration 0 &#39;&#39;&#39; niter = 0 s2 = s2_init sb2 = sb2_init sw2 = sw2_init mub = np.ones(n_features) if binit is None else binit muw = np.ones(n_features) if winit is None else winit sigmab = np.zeros((n_features, n_features)) sigmaw = np.zeros((n_features, n_features)) elbo_path[0] = -np.inf mll_path[0] = -np.inf for itn in range(1, max_iter + 1): &#39;&#39;&#39; GRR for b &#39;&#39;&#39; s2, sb2, mub, sigmab = grr_step(X, y, s2, sb2, muw, sigmaw, XTX, XTy, useVW=use_wb_variance) &#39;&#39;&#39; GRR for W &#39;&#39;&#39; s2, sw2, muw, sigmaw = grr_step(X, y, s2, sw2, mub, sigmab, XTX, XTy, useVW=use_wb_variance) &#39;&#39;&#39; Convergence &#39;&#39;&#39; niter += 1 elbo_path[itn] = elbo(X, y, s2, sb2, sw2, mub, sigmab, muw, sigmaw, XTX, useVW=use_wb_variance) mll_path[itn] = ridge_mll(X, y, s2, sb2, muw) if elbo_path[itn] - elbo_path[itn - 1] &lt; tol: break #if mll_path[itn] - mll_path[itn - 1] &lt; tol: break return s2, sb2, sw2, mub, sigmab, muw, sigmaw, niter, elbo_path[:niter + 1], mll_path[:niter + 1] . . However, there is still an overfitting. . m2 = ebmr_WB2(X, y) s2, sb2, sw2, mub, sigmab, W, sigmaW, niter, elbo_path, mll_path = m2 bpred = mub * W ypred = np.dot(X, bpred) fig = plt.figure(figsize = (12, 12)) ax1 = fig.add_subplot(221) ax2 = fig.add_subplot(222) ax3 = fig.add_subplot(223) #ax4 = fig.add_subplot(224) yvals = np.log(np.max(elbo_path[1:]) - elbo_path[1:] + 1) ax1.scatter(np.arange(niter), yvals, edgecolor = &#39;black&#39;, facecolor=&#39;white&#39;) ax1.plot(np.arange(niter), yvals) ax1.set_xlabel(&quot;Iterations&quot;) ax1.set_ylabel(&quot;log (max(ELBO) - ELBO[itn] + 1)&quot;) ax2.scatter(np.arange(n), y, edgecolor = &#39;black&#39;, facecolor=&#39;white&#39;) ax2.plot(np.arange(n), ypred, color = &#39;salmon&#39;, label=&quot;Predicted&quot;) ax2.plot(np.arange(n), np.dot(X, btrue), color = &#39;dodgerblue&#39;, label=&quot;True&quot;) ax2.legend() ax2.set_xlabel(&quot;Sample Index&quot;) ax2.set_ylabel(&quot;y&quot;) ax3.scatter(np.arange(p), btrue, edgecolor = &#39;black&#39;, facecolor=&#39;white&#39;, label=&quot;True&quot;) ax3.scatter(np.arange(p), bpred, label=&quot;Predicted&quot;) ax3.legend() ax3.set_xlabel(&quot;Predictor Index&quot;) ax3.set_ylabel(&quot;wb&quot;) # nstep = min(80, niter - 2) # ax4.scatter(np.arange(nstep), mll_path[-nstep:], edgecolor = &#39;black&#39;, facecolor=&#39;white&#39;, label=&quot;Evidence&quot;) # ax4.plot(np.arange(nstep), elbo_path[-nstep:], label=&quot;ELBO&quot;) # ax4.legend() # ax4.set_xlabel(&quot;Iterations&quot;) # ax4.set_ylabel(&quot;ELBO / Evidence&quot;) plt.tight_layout() plt.show() . . However, if I set $ mathbf{ Lambda}_w = mathbf{ Lambda}_b = mathbf{0}$, then we get back the simple EM updates leading to optimal prediction. . m3 = ebmr_WB2(X, y, use_wb_variance=False) s2, sb2, sw2, mub, sigmab, W, sigmaW, niter, elbo_path, mll_path = m3 bpred = mub * W ypred = np.dot(X, bpred) fig = plt.figure(figsize = (12, 12)) ax1 = fig.add_subplot(221) ax2 = fig.add_subplot(222) ax3 = fig.add_subplot(223) #ax4 = fig.add_subplot(224) yvals = np.log(np.max(elbo_path[1:]) - elbo_path[1:] + 1) ax1.scatter(np.arange(niter), yvals, edgecolor = &#39;black&#39;, facecolor=&#39;white&#39;) ax1.plot(np.arange(niter), yvals) ax1.set_xlabel(&quot;Iterations&quot;) ax1.set_ylabel(&quot;log (max(ELBO) - ELBO[itn] + 1)&quot;) ax2.scatter(np.arange(n), y, edgecolor = &#39;black&#39;, facecolor=&#39;white&#39;) ax2.plot(np.arange(n), ypred, color = &#39;salmon&#39;, label=&quot;Predicted&quot;) ax2.plot(np.arange(n), np.dot(X, btrue), color = &#39;dodgerblue&#39;, label=&quot;True&quot;) ax2.legend() ax2.set_xlabel(&quot;Sample Index&quot;) ax2.set_ylabel(&quot;y&quot;) ax3.scatter(np.arange(p), btrue, edgecolor = &#39;black&#39;, facecolor=&#39;white&#39;, label=&quot;True&quot;) ax3.scatter(np.arange(p), bpred, label=&quot;Predicted&quot;) ax3.legend() ax3.set_xlabel(&quot;Predictor Index&quot;) ax3.set_ylabel(&quot;wb&quot;) # nstep = min(80, niter) # ax4.scatter(np.arange(nstep), mll_path[-nstep:], edgecolor = &#39;black&#39;, facecolor=&#39;white&#39;, label=&quot;Evidence&quot;) # ax4.plot(np.arange(nstep), elbo_path[-nstep:], label=&quot;ELBO&quot;) # ax4.legend() # ax4.set_xlabel(&quot;Iterations&quot;) # ax4.set_ylabel(&quot;ELBO / Evidence&quot;) plt.tight_layout() plt.show() . . Toy model without noise . Here, I am just trying another toy model to check how the method performs in a simpler setting, without any noise in the data, but with sparse predictor (all coefficients are zero, except 10 out of 200 predictors). . def lasso_data(nsample, nvar, neff, errsigma, sb2 = 100, seed=200): np.random.seed(seed) X = np.random.normal(0, 1, nsample * nvar).reshape(nsample, nvar) X = standardize(X) btrue = np.zeros(nvar) bidx = np.random.choice(nvar, neff , replace = False) btrue[bidx] = np.random.normal(0, np.sqrt(sb2), neff) y = np.dot(X, btrue) + np.random.normal(0, errsigma, nsample) y = y - np.mean(y) #y = y / np.std(y) return X, y, btrue def lims_xy(ax): lims = [ np.min([ax.get_xlim(), ax.get_ylim()]), # min of both axes np.max([ax.get_xlim(), ax.get_ylim()]), # max of both axes ] return lims def plot_diag(ax): lims = lims_xy(ax) ax.plot(lims, lims, ls=&#39;dotted&#39;, color=&#39;gray&#39;) . . peff = 10 sb2 = 100.0 sd = 0.0001 X, y, btrue = lasso_data(n, p, peff, sd, sb2) fig = plt.figure() ax1 = fig.add_subplot(111) ax1.scatter(np.dot(X,btrue), y) plot_diag(ax1) ax1.set_xlabel(&quot;Xb&quot;) ax1.set_ylabel(&quot;y&quot;) plt.show() . It gets the coefficients almost correctly, but there is a systematic tendency to underestimate the coefficients. . m4 = ebmr_WB2(X, y) s2, sb2, sw2, mub, sigmab, W, sigmaW, niter, elbo_path, mll_path = m4 bpred = mub * W ypred = np.dot(X, bpred) fig = plt.figure(figsize = (12, 12)) ax1 = fig.add_subplot(221) ax2 = fig.add_subplot(222) ax3 = fig.add_subplot(223) #ax4 = fig.add_subplot(224) yvals = np.log(np.max(elbo_path[1:]) - elbo_path[1:] + 1) ax1.scatter(np.arange(niter), yvals, edgecolor = &#39;black&#39;, facecolor=&#39;white&#39;) ax1.plot(np.arange(niter), yvals) ax1.set_xlabel(&quot;Iterations&quot;) ax1.set_ylabel(&quot;log (max(ELBO) - ELBO[itn] + 1)&quot;) ax2.scatter(np.dot(X,btrue), y, edgecolor = &#39;black&#39;, facecolor=&#39;white&#39;, label=&quot;True&quot;) ax2.scatter(np.dot(X,btrue), ypred, color = &#39;salmon&#39;, label=&quot;Predicted&quot;) plot_diag(ax2) ax2.legend() ax2.set_xlabel(&quot;Xb&quot;) ax2.set_ylabel(&quot;y&quot;) ax3.scatter(np.arange(p), btrue, edgecolor = &#39;black&#39;, facecolor=&#39;white&#39;, label=&quot;True&quot;) ax3.scatter(np.arange(p), bpred, label=&quot;Predicted&quot;) ax3.legend() ax3.set_xlabel(&quot;Predictor Index&quot;) ax3.set_ylabel(&quot;wb&quot;) # nstep = min(80, niter) # ax4.scatter(np.arange(nstep), mll_path[-nstep:], edgecolor = &#39;black&#39;, facecolor=&#39;white&#39;, label=&quot;Evidence&quot;) # ax4.plot(np.arange(nstep), elbo_path[-nstep:], label=&quot;ELBO&quot;) # ax4.legend() # ax4.set_xlabel(&quot;Iterations&quot;) # ax4.set_ylabel(&quot;ELBO / Evidence&quot;) plt.tight_layout() plt.show() . . sigmab.trace() . 95.26544609444606 . sigmaW.trace() . 60.98925911204604 . np.diag(sigmab)[btrue != 0] . array([0.05601364, 0.08098608, 0.08084723, 0.0840591 , 0.045118 , 0.1231217 , 0.49224221, 0.05510448, 0.49224221, 0.22969095]) . np.diag(sigmaW)[btrue != 0] . array([0.03586012, 0.05184756, 0.05175867, 0.05381492, 0.0288847 , 0.07882293, 0.31513512, 0.03527807, 0.31513512, 0.14704892]) . s2 . 0.4024155770439133 . sb2 . 0.801017889123893 . sw2 . 0.512814349102875 . (mub * W)[btrue != 0] . array([-8.95029733e+00, 6.52191422e+00, 5.71359528e+00, 5.40759037e+00, 1.10498294e+01, -3.79645146e+00, 6.82826917e-52, -9.02268238e+00, -6.15810037e-90, 1.49639826e+00]) .",
            "url": "https://banskt.github.io/iridge-notes/2021/01/06/ebmr-symmetric-updates.html",
            "relUrl": "/2021/01/06/ebmr-symmetric-updates.html",
            "date": " • Jan 6, 2021"
        }
        
    
  
    
        ,"post10": {
            "title": "Sequence of updates in EBMR with product of normals",
            "content": "About . Here, I am simply checking whether the sequence of updates has any effect on the optimization in the variational approximation of EBMR. Earlier, I found that the variational approximation for the product of two normals leads to severe overfitting (see here). . import numpy as np import pandas as pd from scipy import linalg as sc_linalg import matplotlib.pyplot as plt import sys sys.path.append(&quot;../../ebmrPy/&quot;) from inference.ebmr import EBMR from inference import f_elbo from inference import f_sigma from inference import penalized_em from utils import log_density sys.path.append(&quot;../../utils/&quot;) import mpl_stylesheet mpl_stylesheet.banskt_presentation(fontfamily = &#39;latex-clearsans&#39;, fontsize = 18, colors = &#39;banskt&#39;, dpi = 72) . . Toy example . The same trend-filtering data as used previously. . def standardize(X): Xnorm = (X - np.mean(X, axis = 0)) #Xstd = Xnorm / np.std(Xnorm, axis = 0) Xstd = Xnorm / np.sqrt((Xnorm * Xnorm).sum(axis = 0)) return Xstd def trend_data(n, p, bval = 1.0, sd = 1.0, seed=100): np.random.seed(seed) X = np.zeros((n, p)) for i in range(p): X[i:n, i] = np.arange(1, n - i + 1) btrue = np.zeros(p) idx = int(n / 3) btrue[idx] = bval btrue[idx + 1] = -bval y = np.dot(X, btrue) + np.random.normal(0, sd, n) # y = y / np.std(y) return X, y, btrue . . n = 100 p = 200 bval = 8.0 sd = 2.0 X, y, btrue = trend_data(n, p, bval = bval, sd = sd) fig = plt.figure() ax1 = fig.add_subplot(111) ax1.plot(np.arange(n), np.dot(X, btrue), label = &quot;Xb&quot;) ax1.scatter(np.arange(n), y, edgecolor = &#39;black&#39;, facecolor=&#39;white&#39;, label = &quot;Xb + e&quot;) ax1.legend() ax1.set_xlabel(&quot;Sample index&quot;) ax1.set_ylabel(&quot;y&quot;) plt.show() . . Sequence 1 . Here, I am updating $ mathbf{S}$, $ mathbf{m}$, $ sigma^2$, $ sigma_b^2$, $ {v_j^2 }$, $ {a_j }$ and $ sigma_w^2$ in that order. . def ridge_mll(X, y, s2, sb2, W): n, p = X.shape Xscale = np.dot(X, np.diag(W)) XWWtXt = np.dot(Xscale, Xscale.T) sigmay = s2 * (np.eye(n) + sb2 * XWWtXt) muy = np.zeros((n, 1)) return log_density.mgauss(y.reshape(-1,1), muy, sigmay) def grr_b(X, y, s2, sb2, Wbar, varWj, XTX, XTy): n, p = X.shape W = np.diag(Wbar) WtXtXW = np.linalg.multi_dot([W.T, XTX, W]) VW = np.diag(XTX) * varWj sigmabinv = (WtXtXW + np.diag(VW) + np.eye(p) * s2 / sb2) / s2 sigmab = np.linalg.inv(sigmabinv) mub = np.linalg.multi_dot([sigmab, W.T, XTy]) / s2 XWmu = np.linalg.multi_dot([X, W, mub]) mub2 = np.square(mub) s2 = (np.sum(np.square(y - XWmu)) + np.dot((WtXtXW + np.diag(VW)), sigmab).trace() + np.sum(mub2 * VW)) / n sb2 = (np.sum(mub2) + sigmab.trace()) / p return s2, sb2, mub, sigmab def grr_W_old(X, y, s2, sw2, mub, sigmab, muWj, XTX, XTy): n, p = X.shape R = np.einsum(&#39;i,j-&gt;ij&#39;, mub, mub) + sigmab XTXRjj = np.array([XTX[j, j] * R[j, j] for j in range(p)]) #wXTXRj = np.array([np.sum(muWj * XTX[:, j] * R[:, j]) - (muWj[j] * XTXRjj[j]) for j in range(p)]) sigmaWj2 = 1 / ((XTXRjj / s2) + (1 / sw2)) for j in range(p): wXTXRj = np.sum(muWj * XTX[:, j] * R[:, j]) - (muWj[j] * XTXRjj[j]) muWj[j] = sigmaWj2[j] * (mub[j] * XTy[j] - 0.5 * wXTXRj) / s2 sw2 = np.sum(np.square(muWj) + sigmaWj2) / p return sw2, muWj, sigmaWj2 def grr_W(X, y, s2, sw2, mub, sigmab, muWj, XTX, XTy): n, p = X.shape R = np.einsum(&#39;i,j-&gt;ij&#39;, mub, mub) + sigmab XTXRjj = np.diag(XTX) * np.diag(R) sigmaWj2inv = (XTXRjj / s2) + (1 / sw2) wXTXRj = np.array([np.sum(muWj * XTX[:, j] * R[:, j]) - (muWj[j] * XTXRjj[j]) for j in range(p)]) sigmaWj2 = 1 / sigmaWj2inv muWj = sigmaWj2 * (mub * XTy - wXTXRj) / s2 sw2 = np.sum(np.square(muWj) + sigmaWj2) / p #sigmaWj2 = np.zeros(p) return sw2, muWj, sigmaWj2 def elbo(X, y, s2, sb2, sw2, mub, sigmab, Wbar, varWj, XTX): &#39;&#39;&#39; Wbar is a vector which contains the diagonal elements of the diagonal matrix W W = diag_matrix(Wbar) Wbar = diag(W) -- VW is a vector which contains the diagonal elements of the diagonal matrix V_w &#39;&#39;&#39; n, p = X.shape VW = np.diag(XTX) * varWj elbo = c_func(n, p, s2, sb2, sw2) + h1_func(X, y, s2, sb2, sw2, mub, Wbar, VW) + h2_func(p, s2, sb2, sw2, XTX, Wbar, sigmab, varWj, VW) return elbo def c_func(n, p, s2, sb2, sw2): val = p val += - 0.5 * n * np.log(2.0 * np.pi * s2) val += - 0.5 * p * np.log(sb2) val += - 0.5 * p * np.log(sw2) return val def h1_func(X, y, s2, sb2, sw2, mub, Wbar, VW): XWmu = np.linalg.multi_dot([X, np.diag(Wbar), mub]) val1 = - (0.5 / s2) * np.sum(np.square(y - XWmu)) val2 = - 0.5 * np.sum(np.square(mub) * ((VW / s2) + (1 / sb2))) val3 = - 0.5 * np.sum(np.square(Wbar)) / sw2 val = val1 + val2 + val3 return val def h2_func(p, s2, sb2, sw2, XTX, Wbar, sigmab, varWj, VW): (sign, logdetS) = np.linalg.slogdet(sigmab) logdetV = np.sum(np.log(varWj)) W = np.diag(Wbar) WtXtXW = np.linalg.multi_dot([W.T, XTX, W]) val = 0.5 * logdetS + 0.5 * logdetV val += - 0.5 * np.trace(sigmab) / sb2 - 0.5 * np.sum(varWj) / sw2 val += - 0.5 * np.dot(WtXtXW + np.diag(VW), sigmab).trace() / s2 return val def ebmr_WB1(X, y, s2_init = 1.0, sb2_init = 1.0, sw2_init = 1.0, binit = None, winit = None, max_iter = 1000, tol = 1e-8 ): XTX = np.dot(X.T, X) XTy = np.dot(X.T, y) n_samples, n_features = X.shape elbo_path = np.zeros(max_iter + 1) mll_path = np.zeros(max_iter + 1) &#39;&#39;&#39; Iteration 0 &#39;&#39;&#39; niter = 0 s2 = s2_init sb2 = sb2_init sw2 = sw2_init mub = np.ones(n_features) if binit is None else binit muWj = np.ones(n_features) if winit is None else winit sigmab = np.zeros((n_features, n_features)) sigmaWj2 = np.zeros(n_features) elbo_path[0] = -np.inf mll_path[0] = -np.inf for itn in range(1, max_iter + 1): &#39;&#39;&#39; GRR for b &#39;&#39;&#39; s2, sb2, mub, sigmab = grr_b(X, y, s2, sb2, muWj, sigmaWj2, XTX, XTy) &#39;&#39;&#39; GRR for W &#39;&#39;&#39; sw2, muWj, sigmaWj2 = grr_W(X, y, s2, sw2, mub, sigmab, muWj, XTX, XTy) &#39;&#39;&#39; Convergence &#39;&#39;&#39; niter += 1 elbo_path[itn] = elbo(X, y, s2, sb2, sw2, mub, sigmab, muWj, sigmaWj2, XTX) mll_path[itn] = ridge_mll(X, y, s2, sb2, muWj) if elbo_path[itn] - elbo_path[itn - 1] &lt; tol: break #if mll_path[itn] - mll_path[itn - 1] &lt; tol: break return s2, sb2, sw2, mub, sigmab, muWj, sigmaWj2, niter, elbo_path[:niter + 1], mll_path[:niter + 1] . . And this leads to overfitting as we have seen previously. . m1 = ebmr_WB1(X, y) s2, sb2, sw2, mub, sigmab, W, sigmaW, niter, elbo_path, mll_path = m1 bpred = mub * W ypred = np.dot(X, bpred) fig = plt.figure(figsize = (12, 12)) ax1 = fig.add_subplot(221) ax2 = fig.add_subplot(222) ax3 = fig.add_subplot(223) ax4 = fig.add_subplot(224) ax1.scatter(np.arange(niter-1), elbo_path[2:], edgecolor = &#39;black&#39;, facecolor=&#39;white&#39;) ax1.plot(np.arange(niter-1), elbo_path[2:]) ax1.set_xlabel(&quot;Iterations&quot;) ax1.set_ylabel(&quot;ELBO&quot;) ax2.scatter(np.arange(n), y, edgecolor = &#39;black&#39;, facecolor=&#39;white&#39;) ax2.plot(np.arange(n), ypred, color = &#39;salmon&#39;, label=&quot;Predicted&quot;) ax2.plot(np.arange(n), np.dot(X, btrue), color = &#39;dodgerblue&#39;, label=&quot;True&quot;) ax2.legend() ax2.set_xlabel(&quot;Sample Index&quot;) ax2.set_ylabel(&quot;y&quot;) ax3.scatter(np.arange(p), btrue, edgecolor = &#39;black&#39;, facecolor=&#39;white&#39;, label=&quot;True&quot;) ax3.scatter(np.arange(p), bpred, label=&quot;Predicted&quot;) ax3.legend() ax3.set_xlabel(&quot;Predictor Index&quot;) ax3.set_ylabel(&quot;wb&quot;) nstep = min(80, niter - 2) ax4.scatter(np.arange(nstep), mll_path[-nstep:], edgecolor = &#39;black&#39;, facecolor=&#39;white&#39;) ax4.plot(np.arange(nstep), elbo_path[-nstep:]) ax4.set_xlabel(&quot;Iterations&quot;) ax4.set_ylabel(&quot;ELBO / Evidence&quot;) plt.tight_layout() plt.show() . . Sequence 2 . Here, I am updating $ mathbf{S}$, $ mathbf{m}$, $ {v_j^2 }$, $ {a_j }$, $ sigma^2$, $ sigma_b^2$ and $ sigma_w^2$ in that order. . def update_qbw(X, s2, sb2, sw2, mub, sigmab, Wbar, varWj, XTX, XTy): n, p = X.shape W = np.diag(Wbar) WtXtXW = np.linalg.multi_dot([W.T, XTX, W]) VW = np.diag(XTX) * varWj # update mub and sigmab sigmabinv = (WtXtXW + np.diag(VW) + np.eye(p) * s2 / sb2) / s2 sigmab = np.linalg.inv(sigmabinv) mub = np.linalg.multi_dot([sigmab, W.T, XTy]) / s2 # update Wbar and varWj R = np.einsum(&#39;i,j-&gt;ij&#39;, mub, mub) + sigmab XTXRjj = np.diag(XTX) * np.diag(R) wXTXRj = np.array([np.sum(Wbar * XTX[:, j] * R[:, j]) - (Wbar[j] * XTXRjj[j]) for j in range(p)]) varWjinv = (XTXRjj / s2) + (1 / sw2) varWj = 1 / varWjinv for j in range(p): wXTXRj = np.sum(Wbar * XTX[:, j] * R[:, j]) - (Wbar[j] * XTXRjj[j]) Wbar[j] = varWj[j] * (mub[j] * XTy[j] - wXTXRj) / s2 #Wbar = varWj * (mub * XTy - wXTXRj) / s2 return mub, sigmab, Wbar, varWj def update_params(X, y, mub, sigmab, Wbar, varWj, XTX): n, p = X.shape W = np.diag(Wbar) VW = np.diag(XTX) * varWj WtXtXW = np.linalg.multi_dot([W.T, XTX, W]) XWmu = np.linalg.multi_dot([X, W, mub]) mub2 = np.square(mub) # update the parameters s2 = (np.sum(np.square(y - XWmu)) + np.dot((WtXtXW + np.diag(VW)), sigmab).trace() + np.sum(mub2 * VW)) / n sb2 = np.sum(np.square(mub) + np.diag(sigmab)) / p sw2 = np.sum(np.square(Wbar) + varWj) / p return s2, sb2, sw2 def ebmr_WB2(X, y, s2_init = 1.0, sb2_init = 1.0, sw2_init = 1.0, binit = None, winit = None, max_iter = 1000, tol = 1e-8 ): XTX = np.dot(X.T, X) XTy = np.dot(X.T, y) n_samples, n_features = X.shape elbo_path = np.zeros(max_iter + 1) mll_path = np.zeros(max_iter + 1) &#39;&#39;&#39; Iteration 0 &#39;&#39;&#39; niter = 0 s2 = s2_init sb2 = sb2_init sw2 = sw2_init mub = np.ones(n_features) if binit is None else binit muWj = np.ones(n_features) if winit is None else winit sigmab = np.zeros((n_features, n_features)) sigmaWj2 = np.zeros(n_features) elbo_path[0] = -np.inf mll_path[0] = -np.inf for itn in range(1, max_iter + 1): &#39;&#39;&#39; Update q(b, w) &#39;&#39;&#39; mub, sigmab, muWj, sigmaWj2 = update_qbw(X, s2, sb2, sw2, mub, sigmab, muWj, sigmaWj2, XTX, XTy) &#39;&#39;&#39; Update s2, sb2, sw2 &#39;&#39;&#39; s2, sb2, sw2 = update_params(X, y, mub, sigmab, muWj, sigmaWj2, XTX) &#39;&#39;&#39; Convergence &#39;&#39;&#39; niter += 1 elbo_path[itn] = elbo(X, y, s2, sb2, sw2, mub, sigmab, muWj, sigmaWj2, XTX) mll_path[itn] = ridge_mll(X, y, s2, sb2, muWj) if elbo_path[itn] - elbo_path[itn - 1] &lt; tol: break #if mll_path[itn] - mll_path[itn - 1] &lt; tol: break return s2, sb2, sw2, mub, sigmab, muWj, sigmaWj2, niter, elbo_path[:niter + 1], mll_path[:niter + 1] . However, there is still an overfitting. . m2 = ebmr_WB2(X, y) s2, sb2, sw2, mub, sigmab, W, sigmaW, niter, elbo_path, mll_path = m2 bpred = mub * W ypred = np.dot(X, bpred) fig = plt.figure(figsize = (12, 12)) ax1 = fig.add_subplot(221) ax2 = fig.add_subplot(222) ax3 = fig.add_subplot(223) ax4 = fig.add_subplot(224) ax1.scatter(np.arange(niter-1), elbo_path[2:], edgecolor = &#39;black&#39;, facecolor=&#39;white&#39;) ax1.plot(np.arange(niter-1), elbo_path[2:]) ax1.set_xlabel(&quot;Iterations&quot;) ax1.set_ylabel(&quot;ELBO&quot;) ax2.scatter(np.arange(n), y, edgecolor = &#39;black&#39;, facecolor=&#39;white&#39;) ax2.plot(np.arange(n), ypred, color = &#39;salmon&#39;, label=&quot;Predicted&quot;) ax2.plot(np.arange(n), np.dot(X, btrue), color = &#39;dodgerblue&#39;, label=&quot;True&quot;) ax2.legend() ax2.set_xlabel(&quot;Sample Index&quot;) ax2.set_ylabel(&quot;y&quot;) ax3.scatter(np.arange(p), btrue, edgecolor = &#39;black&#39;, facecolor=&#39;white&#39;, label=&quot;True&quot;) ax3.scatter(np.arange(p), bpred, label=&quot;Predicted&quot;) ax3.legend() ax3.set_xlabel(&quot;Predictor Index&quot;) ax3.set_ylabel(&quot;wb&quot;) nstep = min(80, niter - 2) ax4.scatter(np.arange(nstep), mll_path[-nstep:], edgecolor = &#39;black&#39;, facecolor=&#39;white&#39;, label=&quot;Evidence&quot;) ax4.plot(np.arange(nstep), elbo_path[-nstep:], label=&quot;ELBO&quot;) ax4.legend() ax4.set_xlabel(&quot;Iterations&quot;) ax4.set_ylabel(&quot;ELBO / Evidence&quot;) plt.tight_layout() plt.show() . .",
            "url": "https://banskt.github.io/iridge-notes/2021/01/05/ebmr-sequence-of-updates.html",
            "relUrl": "/2021/01/05/ebmr-sequence-of-updates.html",
            "date": " • Jan 5, 2021"
        }
        
    
  
    
        ,"post11": {
            "title": "EBMR with product of coefficients",
            "content": "About . The idea here is to iteratively multiply the coefficients in a ridge regression to induce sparsity. The implementation here is based on the work of Matthew Stephens (see here). . We consider the following hierarchical Empirical Bayes (EB) regression model: . $p left( mathbf{y} mid mathbf{W}, mathbf{b}, sigma right) = N left( mathbf{y} mid mathbf{X} mathbf{W} mathbf{b}, sigma^2 I_n right)$ . $p left( mathbf{b} mid sigma, sigma_b right) = N left( mathbf{b} mid 0, sigma^2 sigma_b^2 right)$ . $p left(w_j mid g right) = g in mathcal{G}.$ . where $ mathbf{W}$ is diagonal matrix whose diagonal elements are given by $(w_1, dots,w_p)$, $ sigma, sigma_b$ are scalars, and $g$ is a prior distribution that is to be estimated. The motivation is that the product of two normals will be distributed as a linear combination of two chi-square random variables. . $wb = displaystyle frac{1}{4}(w + b)^2 - frac{1}{4}(w - b)^2$ . By iteratively multiplying the $b$ coefficients on $w$, we can get sparse coefficients. . import numpy as np import pandas as pd from scipy import linalg as sc_linalg import matplotlib.pyplot as plt import sys sys.path.append(&quot;../../ebmrPy/&quot;) from inference.ebmr import EBMR from inference import f_elbo from inference import f_sigma from inference import penalized_em from utils import log_density sys.path.append(&quot;../../utils/&quot;) import mpl_stylesheet mpl_stylesheet.banskt_presentation(fontfamily = &#39;latex-clearsans&#39;, fontsize = 18, colors = &#39;banskt&#39;, dpi = 72) . . Toy model . I am using the trend-filtering example as a toy model, following the work of Matthew. . def standardize(X): Xnorm = (X - np.mean(X, axis = 0)) #Xstd = Xnorm / np.std(Xnorm, axis = 0) Xstd = Xnorm / np.sqrt((Xnorm * Xnorm).sum(axis = 0)) return Xstd def trend_data(n, p, bval = 1.0, sd = 1.0, seed=100): np.random.seed(seed) X = np.zeros((n, p)) for i in range(p): X[i:n, i] = np.arange(1, n - i + 1) btrue = np.zeros(p) idx = int(n / 3) btrue[idx] = bval btrue[idx + 1] = -bval y = np.dot(X, btrue) + np.random.normal(0, sd, n) # y = y / np.std(y) return X, y, btrue . . n = 100 p = 200 bval = 8.0 sd = 2.0 X, y, btrue = trend_data(n, p, bval = bval, sd = sd) fig = plt.figure() ax1 = fig.add_subplot(111) ax1.plot(np.arange(n), np.dot(X, btrue), label=&quot;Xb&quot;) ax1.scatter(np.arange(n), y, edgecolor = &#39;black&#39;, facecolor=&#39;white&#39;, label=&quot;Xb + e&quot;) ax1.legend() ax1.set_xlabel(&quot;Sample index&quot;) ax1.set_ylabel(&quot;y&quot;) plt.show() . . Ridge regression using EM . First, I will try normal ridge regression: . $p left( mathbf{y} mid mathbf{b}, sigma^2 right) = N left( mathbf{y} mid mathbf{X} mathbf{b}, sigma^2 mathbb{I}_n right)$ . $p left( mathbf{b} mid sigma_b^2 right) = N left( mathbf{b} mid 0, sigma_b^2 mathbb{I}_p right)$ . m1 = penalized_em.ridge(X, y, 1.0, 1.0, max_iter=4000, tol=1e-8, ignore_convergence=False) m1_bpred = m1[2] m1_ypred = np.dot(X, m1_bpred) fig = plt.figure() ax1 = fig.add_subplot(111) ax1.scatter(np.arange(n), y, edgecolor = &#39;black&#39;, facecolor=&#39;white&#39;) ax1.plot(np.arange(n), m1_ypred) ax1.set_xlabel(&quot;Sample index&quot;) ax1.set_ylabel(&quot;y&quot;) plt.show() . . Iterative ridge regression . Next, I will try iterative ridge regression, using: . $p left( mathbf{y} mid mathbf{b}, sigma^2 right) = N left( mathbf{y} mid mathbf{X} mathbf{W} mathbf{b}, sigma^2 mathbb{I}_n right)$ . $p left( mathbf{b} mid sigma_b^2 right) = N left( mathbf{b} mid 0, sigma_b^2 mathbb{I}_p right)$. . $ mathbf{W}$ is a diagonal matrix whose diagonal elements are given by $ mathbf{w} := (w_1, dots,w_p)$. In every step, we perform EM ridge regression using $ mathbf{X} mathbf{W}$ as the predictor variable. We update $ mathbf{w}_{ mathrm{new}} = mathbf{w}_{ mathrm{old}} mathbf{b}$. The iteration stops when the marginal likelihood given by . $p left( mathbf{y} mid mathbf{W}, sigma^2, sigma_b^2 right) = N left( mathbf{y} mid 0, sigma^2 mathbb{I}_n + sigma_b^2 mathbf{X} mathbf{W}( mathbf{X} mathbf{W})^{ mathsf{T}} right)$ . converges to a tolerance value (tol). Here, we obtain a point estimate of $ mathbf{W}$. We have not set any prior on $ mathbf{W}$ explicitly. Since $ mathbf{b}$ has a Gaussian prior, we can think of $ mathbf{W}$ as having an implicit prior of a product of $T$ Gaussians (where $T$ is the number of iterations). . def ridge_step(X, y, s2, sb2, W): n, p = X.shape Xscale = np.dot(X, np.diag(W)) XTX = np.dot(Xscale.T, Xscale) XTy = np.dot(Xscale.T, y) yTy = np.dot(y.T, y) sigmabinv = (XTX + np.eye(p) * (s2 / sb2)) / s2 sigmab = np.linalg.inv(sigmabinv) # posterior variance of b mub = np.dot(sigmab, XTy) / s2 # posterior mean of b mmT = np.einsum(&#39;i,j-&gt;ij&#39;, mub, mub) BTB = mmT + sigmab XWmu = np.dot(Xscale, mub) s2 = (np.sum(np.square(y - XWmu)) + np.trace(np.dot(XTX, sigmab))) / n sb2 = np.trace(BTB) / p return s2, sb2, mub def ridge_mll(X, y, s2, sb2, W): n, p = X.shape Xscale = np.dot(X, np.diag(W)) XWWtXt = np.dot(Xscale, Xscale.T) sigmay = s2 * (np.eye(n) + sb2 * XWWtXt) muy = np.zeros((n, 1)) return log_density.mgauss(y.reshape(-1,1), muy, sigmay) def iterative_ridge(X, y, s2_init = 1.0, sb2_init = 1.0, sw2_init = 1.0, max_iter = 1000, tol = 1e-8): n_samples, n_features = X.shape mll_path = np.zeros(max_iter + 1) W = np.ones(n_features) &#39;&#39;&#39; Iteration 0 &#39;&#39;&#39; niter = 0 s2 = s2_init sb2 = sb2_init mll_path[0] = ridge_mll(X, y, s2, sb2, W) &#39;&#39;&#39; Iterations &#39;&#39;&#39; for itn in range(1, max_iter + 1): s2, sb2, mub = ridge_step(X, y, s2, sb2, W) W *= mub &#39;&#39;&#39; Convergence &#39;&#39;&#39; niter += 1 mll_path[itn] = ridge_mll(X, y, s2, sb2, W) if mll_path[itn] - mll_path[itn - 1] &lt; tol: break return s2, sb2, mub, W, niter, mll_path[:niter + 1] . . This gives a sparse solution and a better prediction than ridge regression. . m2 = iterative_ridge(X, y, s2_init = 3.0) _, _, bpred, W, niter, mll_path = m2 ypred = np.dot(X, W) fig = plt.figure(figsize = (12, 12)) ax1 = fig.add_subplot(221) ax2 = fig.add_subplot(222) ax3 = fig.add_subplot(223) yvals = mll_path[1:] ax1.scatter(np.arange(niter), yvals, edgecolor = &#39;black&#39;, facecolor=&#39;white&#39;) ax1.set_xlabel(&quot;Iterations&quot;) ax1.set_ylabel(&quot;Log(marginal likelihood)&quot;) ax2.scatter(np.arange(n), y, edgecolor = &#39;black&#39;, facecolor=&#39;white&#39;) ax2.plot(np.arange(n), ypred, color = &#39;salmon&#39;, label=&quot;Predicted&quot;) ax2.plot(np.arange(n), np.dot(X, btrue), color = &#39;dodgerblue&#39;, label=&quot;True&quot;) ax2.legend() ax2.set_xlabel(&quot;Sample index&quot;) ax2.set_ylabel(&quot;y&quot;) ax3.scatter(np.arange(p), btrue, edgecolor = &#39;black&#39;, facecolor=&#39;white&#39;, label=&quot;True&quot;) ax3.scatter(np.arange(p), W, label=&quot;Predicted&quot;) ax3.legend() ax3.set_xlabel(&quot;Predictor index&quot;) ax3.set_ylabel(&quot;wb&quot;) plt.tight_layout() plt.show() . . Approximate ELBO in the iterative framework . In the iterative framework, the prior on $ mathbf{W}$ is essentialy a product of $T$ normals, where $T$ is the number of iterations. We can think of the above problem in the framework of variational approximation and try to calculate the ELBO $F left(q( mathbf{b}, mathbf{W}) right)$. . However, we do not know the $ mathbb{E}_{q( mathbf{W})} left[ ln g( mathbf{W}) right]$ and $ mathbb{E}_{q( mathbf{W})} left[ ln q( mathbf{W}) right]$. Hence, I calculated an approximate ELBO by considering $ mathbb{E}_{q(w)} left[ ln (g( mathbf{W}) / q( mathbf{W})) right] = 0$. Derivations are still in my notebook. . I find that the approximate ELBO is very close to the evidence $p left( mathbf{y} mid mathbf{W}, sigma^2, sigma_b^2 right)$. . def elbo(X, y, s2, sb2, mub, sigmab, Wbar, varWj, XTX, KLqW): &#39;&#39;&#39; Wbar is a vector which contains the diagonal elements of the diagonal matrix W W = diag_matrix(Wbar) Wbar = diag(W) -- VW is a vector which contains the diagonal elements of the diagonal matrix V_w &#39;&#39;&#39; n, p = X.shape VW = np.diag(XTX) * varWj elbo = c_func(n, p, s2, sb2) + h1_func(X, y, s2, sb2, mub, Wbar, VW) + h2_func(p, s2, sb2, XTX, sigmab, Wbar, VW) - KLqW return elbo def c_func(n, p, s2, sb2): val = 0.5 * p val += - 0.5 * p * np.log(sb2) val += - 0.5 * n * np.log(2.0 * np.pi * s2) return val def h1_func(X, y, s2, sb2, mub, Wbar, VW): XWmu = np.linalg.multi_dot([X, np.diag(Wbar), mub]) val1 = - (0.5 / s2) * np.sum(np.square(y - XWmu)) val2 = - 0.5 * np.sum(np.square(mub) * ((VW / s2) + (1 / sb2))) val = val1 + val2 return val def h2_func(p, s2, sb2, XTX, sigmab, Wbar, VW): (sign, logdet) = np.linalg.slogdet(sigmab) W = np.diag(Wbar) WtXtXW = np.linalg.multi_dot([W.T, XTX, W]) val = 0.5 * logdet val += - 0.5 * np.dot(WtXtXW + np.diag(VW) + np.eye(p) * (s2 / sb2), sigmab).trace() / s2 return val def grr_b(X, y, s2, sb2, Wbar, varWj, XTX, XTy): n, p = X.shape W = np.diag(Wbar) WtXtXW = np.linalg.multi_dot([W.T, XTX, W]) VW = np.diag(XTX) * varWj sigmabinv = (WtXtXW + np.diag(VW) + np.eye(p) * s2 / sb2) / s2 sigmab = np.linalg.inv(sigmabinv) mub = np.linalg.multi_dot([sigmab, W, XTy]) / s2 XWmu = np.linalg.multi_dot([X, W, mub]) mub2 = np.square(mub) s2 = (np.sum(np.square(y - XWmu)) + np.dot((WtXtXW + np.diag(VW)), sigmab).trace() + np.sum(mub2 * VW)) / n sb2 = (np.sum(mub2) + sigmab.trace()) / p return s2, sb2, mub, sigmab def grr_W_point(X, y, s2, sw2, mub, sigmab, muWj, sigmaWj2, XTX, XTy): muWj = muWj * mub sigmaWj2 = np.zeros(p) #sigmaWj2 = sigmaWj2 * np.diag(sigmab) KLqW = 0.0 return sw2, muWj, sigmaWj2, KLqW def ebmr_iterative_ridge(X, y, s2_init = 1.0, sb2_init = 1.0, sw2_init = 1.0, w_init = None, max_iter = 1000, tol = 1e-8 ): XTX = np.dot(X.T, X) XTy = np.dot(X.T, y) n_samples, n_features = X.shape elbo_path = np.zeros(max_iter + 1) mll_path = np.zeros(max_iter + 1) &#39;&#39;&#39; Iteration 0 &#39;&#39;&#39; niter = 0 s2 = s2_init sb2 = sb2_init sw2 = sw2_init mub = np.zeros(n_features) sigmab = np.zeros((n_features, n_features)) if w_init is None: muWj = np.ones(n_features) else: muWj = w_init sigmaWj2 = np.zeros(n_features) elbo_path[0] = -np.inf mll_path[0] = -np.inf KLqW = 0 for itn in range(1, max_iter + 1): &#39;&#39;&#39; GRR for b &#39;&#39;&#39; s2, sb2, mub, sigmab = grr_b(X, y, s2, sb2, muWj, sigmaWj2, XTX, XTy) #print(itn, s2, sb2, mub[0]) &#39;&#39;&#39; GRR for W &#39;&#39;&#39; sw2, muWj, sigmaWj2, KLqW = grr_W_point(X, y, s2, sw2, mub, sigmab, muWj, sigmaWj2, XTX, XTy) &#39;&#39;&#39; Convergence &#39;&#39;&#39; niter += 1 elbo_path[itn] = elbo(X, y, s2, sb2, mub, sigmab, muWj, sigmaWj2, XTX, KLqW) mll_path[itn] = ridge_mll(X, y, s2, sb2, muWj) #if elbo_path[itn] - elbo_path[itn - 1] &lt; tol: break if mll_path[itn] - mll_path[itn - 1] &lt; tol: break return s2, sb2, sw2, mub, sigmab, muWj, sigmaWj2, niter, elbo_path[:niter + 1], mll_path[:niter + 1] . . This gives a solution similar to the iterative ridge regression, except that the predicted values of the coefficients are better. However, the approximate ELBO behaves strangely, which is quite expected because it is not a true ELBO. . m3 = ebmr_iterative_ridge(X, y, max_iter = 200) _, _, _, mub, sigmab, W, sigmaW, niter, elbo_path, mll_path = m3 ypred = np.dot(X, W) fig = plt.figure(figsize = (12, 12)) ax1 = fig.add_subplot(221) ax2 = fig.add_subplot(222) ax3 = fig.add_subplot(223) ax4 = fig.add_subplot(224) ax1.scatter(np.arange(niter), mll_path[1:], edgecolor = &#39;black&#39;, facecolor=&#39;white&#39;) #ax1.plot(np.arange(niter), elbo_path[1:]) ax1.set_xlabel(&quot;Iterations&quot;) ax1.set_ylabel(&quot;Evidence&quot;) ax2.scatter(np.arange(n), y, edgecolor = &#39;black&#39;, facecolor=&#39;white&#39;) ax2.plot(np.arange(n), ypred, color = &#39;salmon&#39;, label=&quot;Predicted&quot;) ax2.plot(np.arange(n), np.dot(X, btrue), color = &#39;dodgerblue&#39;, label=&quot;True&quot;) ax2.legend() ax2.set_xlabel(&quot;Sample index&quot;) ax2.set_ylabel(&quot;y&quot;) ax3.scatter(np.arange(p), btrue, edgecolor = &#39;black&#39;, facecolor=&#39;white&#39;, label=&quot;True&quot;) ax3.scatter(np.arange(p), W, label=&quot;Predicted&quot;) ax3.legend() ax3.set_xlabel(&quot;Predictor index&quot;) ax3.set_ylabel(&quot;wb&quot;) nstep = min(80, niter - 2) ax4.scatter(np.arange(nstep), mll_path[-nstep:], edgecolor = &#39;black&#39;, facecolor=&#39;white&#39;, label=&quot;Evidence&quot;) ax4.plot(np.arange(nstep), elbo_path[-nstep:], label=&quot;ELBO&quot;) ax4.legend() ax4.set_xlabel(&quot;Iterations&quot;) ax4.set_ylabel(&quot;ELBO / Evidence&quot;) plt.tight_layout() plt.show() . . Product of two normals . Instead of using a product of multiple normals, we can restrict the coefficients $ mathbf{W} mathbf{b}$ to a product of two normals, and iterate between solving $ mathbf{W}$ and $ mathbf{b}$. . $p left( mathbf{y} mid mathbf{b}, sigma^2 right) = N left( mathbf{y} mid mathbf{X} mathbf{W} mathbf{b}, sigma^2 mathbb{I}_n right) = N left( mathbf{y} mid mathbf{X} mathbf{B} mathbf{w}, sigma^2 mathbb{I}_n right)$ . $p left( mathbf{b} mid sigma_b^2 right) = N left( mathbf{b} mid 0, sigma_b^2 mathbb{I}_p right)$ . $ displaystyle p left( mathbf{w} mid sigma_w^2 right) = prod_{j=1}^{P} N left(w_j mid 0, sigma_w^2 mathbb{I}_p right)$. . As before $ mathbf{W}$ is a $p times p$ diagonal matrix whose diagonal elements are given by $w_j$. The vector of $ {w_j }$ is denoted as $ mathbf{w}$. Here I introduced the matrix $ mathbf{B}$ which is required when solving for $ mathbf{w}$. $ mathbf{B}$ is a $p times p$ diagonal matrix whose diagonal elements are given by $b_j$. I used the following factorization: . $ displaystyle q left( mathbf{b}, mathbf{w} right) = q left( mathbf{b} right) prod_{j=1}^{P}q_j left(w_j right) $. . We can separate the problem to two steps. In the first step, we assume $ mathbf{W}$ is known and solve for $ mathbf{b}$. In the second step, we assume $ mathbf{B}$ is known and solve for $ mathbf{w}$. We continue iterating until we reach convergence. . For the first step, the updates should be the same as for normal ridge regression with fixed $ mathbf{W}$: . $p left( mathbf{b} mid mathbf{y}, mathbf{W}, sigma^2, sigma_b^2 right) = N left( mathbf{b} mid mathbf{m}, mathbf{S} right)$, . $ displaystyle mathbf{S} = left[ frac{1}{ sigma^2} mathbf{W}^{ mathsf{T}} mathbf{X}^{ mathsf{T}} mathbf{X} mathbf{W} + frac{1}{ sigma_b^2} mathbb{I} right]^{-1}$, . $ displaystyle mathbf{m} = frac{1}{ sigma^2} mathbf{S} mathbf{W}^{ mathsf{T}} mathbf{X}^{ mathsf{T}} mathbf{y}$, . $ displaystyle sigma^2 = frac{1}{N} left { left Vert mathbf{y} - mathbf{X} mathbf{W} mathbf{m} right Vert_{2}^{2} + mathrm{Tr} left( mathbf{W}^{ mathsf{T}} mathbf{X}^{ mathsf{T}} mathbf{X} mathbf{W} mathbf{S} right) right }$, . $ displaystyle sigma_b^2 = frac{1}{P} mathrm{Tr} left( mathbf{m} mathbf{m}^{ mathsf{T}} + mathbf{S} right)$ . Similarly, we have another ridge regression for $ mathbf{W}$ with . $p left( mathbf{w} mid mathbf{y}, mathbf{B}, sigma^2, sigma_w^2 right) = N left( mathbf{w} mid mathbf{a}, mathbf{V} right)$, . $ displaystyle mathbf{V} = left[ frac{1}{ sigma^2} mathbf{B}^{ mathsf{T}} mathbf{X}^{ mathsf{T}} mathbf{X} mathbf{B} + frac{1}{ sigma_w^2} mathbb{I} right]^{-1}$, . $ displaystyle mathbf{a} = frac{1}{ sigma^2} mathbf{V} mathbf{B}^{ mathsf{T}} mathbf{X}^{ mathsf{T}} mathbf{y}$, . $ displaystyle sigma^2 = frac{1}{N} left { left Vert mathbf{y} - mathbf{X} mathbf{B} mathbf{a} right Vert_{2}^{2} + mathrm{Tr} left( mathbf{B}^{ mathsf{T}} mathbf{X}^{ mathsf{T}} mathbf{X} mathbf{B} mathbf{V} right) right }$, . $ displaystyle sigma_w^2 = frac{1}{P} mathrm{Tr} left( mathbf{a} mathbf{a}^{ mathsf{T}} + mathbf{V} right)$. . The iteration stops when the ELBO $F(q( mathbf{b}, mathbf{W}))$ converges to a tolerance value (tol). . def grr_step(X, y, s2, sb2, Wbar, XTX, XTy): n, p = X.shape W = np.diag(Wbar) WtXtXW = np.linalg.multi_dot([W.T, XTX, W]) #VW = np.diag(XTX) * varWj sigmabinv = (WtXtXW + np.eye(p) * s2 / sb2) / s2 sigmab = np.linalg.inv(sigmabinv) mub = np.linalg.multi_dot([sigmab, W, XTy]) / s2 XWmu = np.linalg.multi_dot([X, W, mub]) mub2 = np.square(mub) s2 = (np.sum(np.square(y - XWmu)) + np.dot(WtXtXW, sigmab).trace()) / n sb2 = (np.sum(mub2) + sigmab.trace()) / p return s2, sb2, mub, sigmab def elbo(X, y, s2, sb2, sw2, mub, sigmab, Wbar, varWj, XTX): &#39;&#39;&#39; Wbar is a vector which contains the diagonal elements of the diagonal matrix W W = diag_matrix(Wbar) Wbar = diag(W) -- VW is a vector which contains the diagonal elements of the diagonal matrix V_w &#39;&#39;&#39; n, p = X.shape VW = np.diag(XTX) * varWj elbo = c_func(n, p, s2, sb2, sw2) + h1_func(X, y, s2, sb2, sw2, mub, Wbar, VW) + h2_func(p, s2, sb2, sw2, XTX, Wbar, sigmab, varWj, VW) return elbo def c_func(n, p, s2, sb2, sw2): val = p + p * np.log(2.0 * np.pi) val += - 0.5 * n * np.log(2.0 * np.pi * s2) val += - 0.5 * p * np.log(2.0 * np.pi * sb2) val += - 0.5 * p * np.log(2.0 * np.pi * sw2) return val def h1_func(X, y, s2, sb2, sw2, mub, Wbar, VW): XWmu = np.linalg.multi_dot([X, np.diag(Wbar), mub]) val1 = - (0.5 / s2) * np.sum(np.square(y - XWmu)) val2 = - 0.5 * np.sum(np.square(mub)) / sb2 val3 = - 0.5 * np.sum(np.square(Wbar)) / sw2 val = val1 + val2 + val3 return val def h2_func(p, s2, sb2, sw2, XTX, Wbar, sigmab, varWj, VW): (sign, logdetS) = np.linalg.slogdet(sigmab) logdetV = np.sum(np.log(varWj)) W = np.diag(Wbar) WtXtXW = np.linalg.multi_dot([W.T, XTX, W]) val = 0.5 * logdetS #+ 0.5 * logdetV val += - 0.5 * np.trace(sigmab) / sb2 - 0.5 * np.sum(varWj) / sw2 val += - 0.5 * np.dot(WtXtXW, sigmab).trace() / s2 return val def ebmr_WB(X, y, s2_init = 1.0, sb2_init = 1.0, sw2_init = 1.0, max_iter = 1000, tol = 1e-8 ): XTX = np.dot(X.T, X) XTy = np.dot(X.T, y) n_samples, n_features = X.shape elbo_path = np.zeros(max_iter + 1) mll_path = np.zeros(max_iter + 1) &#39;&#39;&#39; Iteration 0 &#39;&#39;&#39; niter = 0 s2 = s2_init sb2 = sb2_init sw2 = sw2_init mub = np.ones(n_features) sigmab = np.zeros((n_features, n_features)) muWj = np.ones(n_features) sigmaWj2 = np.zeros(n_features) elbo_path[0] = -np.inf mll_path[0] = -np.inf for itn in range(1, max_iter + 1): &#39;&#39;&#39; GRR for b &#39;&#39;&#39; s2, sb2, mub, sigmab = grr_step(X, y, s2, sb2, muWj, XTX, XTy) &#39;&#39;&#39; GRR for W &#39;&#39;&#39; _, sw2, muWj, sigmaW = grr_step(X, y, s2, sw2, mub, XTX, XTy) sigmaWj2 = np.diag(sigmaW) &#39;&#39;&#39; Convergence &#39;&#39;&#39; niter += 1 elbo_path[itn] = elbo(X, y, s2, sb2, sw2, mub, sigmab, muWj, sigmaWj2, XTX) mll_path[itn] = ridge_mll(X, y, s2, sb2, muWj) if elbo_path[itn] - elbo_path[itn - 1] &lt; tol: break #if mll_path[itn] - mll_path[itn - 1] &lt; tol: break return s2, sb2, sw2, mub, sigmab, muWj, sigmaWj2, niter, elbo_path[:niter + 1], mll_path[:niter + 1] . . m4 = ebmr_WB(X, y, max_iter = 1000) s2, sb2, sw2, mub, sigmab, muW, sigmaW, niter, elbo_path, mll_path = m4 bpred = mub * muW ypred = np.dot(X, bpred) fig = plt.figure(figsize = (12, 12)) ax1 = fig.add_subplot(221) ax2 = fig.add_subplot(222) ax3 = fig.add_subplot(223) ax4 = fig.add_subplot(224) ax1.scatter(np.arange(niter), elbo_path[1:], edgecolor = &#39;black&#39;, facecolor=&#39;white&#39;) #ax1.plot(np.arange(niter), elbo_path[1:]) ax1.set_xlabel(&quot;Iterations&quot;) ax1.set_ylabel(&quot;ELBO&quot;) ax2.scatter(np.arange(n), y, edgecolor = &#39;black&#39;, facecolor=&#39;white&#39;) ax2.plot(np.arange(n), ypred, color = &#39;salmon&#39;, label=&quot;Predicted&quot;) ax2.plot(np.arange(n), np.dot(X, btrue), color = &#39;dodgerblue&#39;, label=&quot;True&quot;) ax2.legend() ax2.set_xlabel(&quot;Sample Index&quot;) ax2.set_ylabel(&quot;y&quot;) ax3.scatter(np.arange(p), btrue, edgecolor = &#39;black&#39;, facecolor=&#39;white&#39;, label=&quot;True&quot;) ax3.scatter(np.arange(p), bpred, label=&quot;Predicted&quot;) ax3.legend() ax3.set_xlabel(&quot;Predictor Index&quot;) ax3.set_ylabel(&quot;wb&quot;) nstep = min(80, niter - 1) ax4.scatter(np.arange(nstep), mll_path[-nstep:], edgecolor = &#39;black&#39;, facecolor=&#39;white&#39;, label=&quot;Evidence&quot;) ax4.plot(np.arange(nstep), elbo_path[-nstep:], label=&quot;ELBO&quot;) ax4.legend() ax4.set_xlabel(&quot;Iterations&quot;) ax4.set_ylabel(&quot;ELBO / Evidence&quot;) plt.tight_layout() plt.show() . . Variational approximation . Finally, I try to cast the problem in a variational approximation framework with a product of two normals. . $p left( mathbf{y} mid mathbf{b}, sigma^2 right) = N left( mathbf{y} mid mathbf{X} mathbf{W} mathbf{b}, sigma^2 mathbb{I}_n right)$ . $p left( mathbf{b} mid sigma_b^2 right) = N left( mathbf{b} mid 0, sigma_b^2 mathbb{I}_p right)$ . $ displaystyle p left( mathbf{w} mid sigma_w^2 right) = prod_{j=1}^{P} N left(w_j mid 0, sigma_w^2 mathbb{I}_p right)$. . As before $ mathbf{W}$ is a $p times p$ diagonal matrix whose diagonal elements are given by $w_j$. The vector of $ {w_j }$ is denoted as $ mathbf{w}$. I used the following factorization: . $ displaystyle q left( mathbf{b}, mathbf{W} right) = q left( mathbf{b} right) prod_{j=1}^{P}q_j left(w_j right) $. . Here we can calulate the exact ELBO and derive the update equations. Derivations are still in my notebook. The updates are as follows: . $q left( mathbf{b} right) = N left( mathbf{b} mid mathbf{m}, mathbf{S} right)$, . $q_j left(w_j right) = N left( w_j mid a_j, v_j^2 right)$, . $ displaystyle mathbf{S} = left[ frac{1}{ sigma^2} left( mathbf{A}^{ mathsf{T}} mathbf{X}^{ mathsf{T}} mathbf{X} mathbf{A} + mathbf{ Lambda}_w right) + frac{1}{ sigma_b^2} mathbb{I} right]^{-1}$, . $ displaystyle mathbf{m} = frac{1}{ sigma^2} mathbf{S} mathbf{A}^{ mathsf{T}} mathbf{X}^{ mathsf{T}} mathbf{y}$, . $ mathbf{A}$ and $ mathbf{ Lambda}_w$ are diagonal matrices whose diagonal elements are given by . $ mathbf{A}_{jj} := a_j$, . $( mathbf{ Lambda}_w)_{jj} = ( mathbf{X}^{ mathsf{T}} mathbf{X})_{jj}v_j^2$. . $ displaystyle v_j^2 = left[ frac{1}{ sigma^2} ( mathbf{X}^{ mathsf{T}} mathbf{X})_{jj} mathbf{R}_{jj} + frac{1}{ sigma_w^2} right]^{-1}$, . $ displaystyle a_j = frac{v_j^2}{ sigma^2} left[m_j ( mathbf{X}^{ mathsf{T}} mathbf{y})_j - sum_{i=1 {i neq j}}^{P}{a_i ( mathbf{X}^{ mathsf{T}} mathbf{X})_{ij} mathbf{R}_{ij}} right]$, . $ mathbf{R} := mathbf{m} mathbf{m}^{ mathsf{T}} + mathbf{S}$. . Finally, the updates of $ sigma^2$, $ sigma_b^2$ and $ sigma_w^2$ can be obtained by taking the derivative of $F(q( mathbf{b}, mathbf{W}))$ and setting them to 0 respectively. This yields . $ displaystyle sigma^2 = frac{1}{N} left { left Vert mathbf{y} - mathbf{X} mathbf{A} mathbf{m} right Vert_{2}^{2} + mathrm{Tr} left( left( mathbf{A}^{ mathsf{T}} mathbf{X}^{ mathsf{T}} mathbf{X} mathbf{A} + mathbf{ Lambda}_w right) mathbf{S} right) + mathbf{m}^{ mathsf{T}} mathbf{ Lambda}_w mathbf{m} right }$, . $ displaystyle sigma_b^2 = frac{1}{P} mathrm{Tr} left( mathbf{m} mathbf{m}^{ mathsf{T}} + mathbf{S} right)$ and . $ displaystyle sigma_w^2 = frac{1}{P} sum_{j=1}^{P} left(a_j^2 + v_j^2 right)$. . The iteration should stop when the ELBO $F(q( mathbf{b}, mathbf{w}))$ converges to a tolerance value (tol). But, there is some mistake in the ELBO calculation and currently, I am not using any convergence criteria. . $ displaystyle F(q) = P - frac{N}{2} ln(2 pi sigma^2) - frac{P}{2} ln( sigma_b^2) - frac{P}{2} ln( sigma_w^2) - frac{1}{2 sigma^2} left { left Vert mathbf{y} - mathbf{X} mathbf{A} mathbf{m} right Vert_2^2 + mathrm{Tr} left( left( mathbf{A}^{ mathsf{T}} mathbf{X}^{ mathsf{T}} mathbf{X} mathbf{A} + mathbf{ Lambda}_w right) mathbf{S} right) + mathbf{m}^{ mathsf{T}} mathbf{ Lambda}_w mathbf{m} right } - frac{1}{2 sigma_b^2} mathrm{Tr} left( mathbf{m} mathbf{m}^{ mathsf{T}} + mathbf{S} right) - frac{1}{2 sigma_w^2} sum_{j=1}^{P} left(a_j^2 + v_j^2 right) + frac{1}{2} ln left lvert mathbf{S} right rvert + frac{1}{2} sum_{j=1}^{P} ln{v_j^2}$ . def grr_b(X, y, s2, sb2, Wbar, varWj, XTX, XTy): n, p = X.shape W = np.diag(Wbar) WtXtXW = np.linalg.multi_dot([W.T, XTX, W]) VW = np.diag(XTX) * varWj sigmabinv = (WtXtXW + np.diag(VW) + np.eye(p) * s2 / sb2) / s2 sigmab = np.linalg.inv(sigmabinv) mub = np.linalg.multi_dot([sigmab, W.T, XTy]) / s2 XWmu = np.linalg.multi_dot([X, W, mub]) mub2 = np.square(mub) s2 = (np.sum(np.square(y - XWmu)) + np.dot((WtXtXW + np.diag(VW)), sigmab).trace() + np.sum(mub2 * VW)) / n sb2 = (np.sum(mub2) + sigmab.trace()) / p return s2, sb2, mub, sigmab def grr_W_old(X, y, s2, sw2, mub, sigmab, muWj, XTX, XTy): n, p = X.shape R = np.einsum(&#39;i,j-&gt;ij&#39;, mub, mub) + sigmab XTXRjj = np.array([XTX[j, j] * R[j, j] for j in range(p)]) #wXTXRj = np.array([np.sum(muWj * XTX[:, j] * R[:, j]) - (muWj[j] * XTXRjj[j]) for j in range(p)]) sigmaWj2 = 1 / ((XTXRjj / s2) + (1 / sw2)) for j in range(p): wXTXRj = np.sum(muWj * XTX[:, j] * R[:, j]) - (muWj[j] * XTXRjj[j]) muWj[j] = sigmaWj2[j] * (mub[j] * XTy[j] - 0.5 * wXTXRj) / s2 sw2 = np.sum(np.square(muWj) + sigmaWj2) / p return sw2, muWj, sigmaWj2 def grr_W(X, y, s2, sw2, mub, sigmab, muWj, XTX, XTy): n, p = X.shape R = np.einsum(&#39;i,j-&gt;ij&#39;, mub, mub) + sigmab XTXRjj = np.diag(XTX) * np.diag(R) sigmaWj2inv = (XTXRjj / s2) + (1 / sw2) wXTXRj = np.array([np.sum(muWj * XTX[:, j] * R[:, j]) - (muWj[j] * XTXRjj[j]) for j in range(p)]) sigmaWj2 = 1 / sigmaWj2inv muWj = sigmaWj2 * (mub * XTy - wXTXRj) / s2 sw2 = np.sum(np.square(muWj) + sigmaWj2) / p #sigmaWj2 = np.zeros(p) return sw2, muWj, sigmaWj2 def elbo(X, y, s2, sb2, sw2, mub, sigmab, Wbar, varWj, XTX): &#39;&#39;&#39; Wbar is a vector which contains the diagonal elements of the diagonal matrix W W = diag_matrix(Wbar) Wbar = diag(W) -- VW is a vector which contains the diagonal elements of the diagonal matrix V_w &#39;&#39;&#39; n, p = X.shape VW = np.diag(XTX) * varWj elbo = c_func(n, p, s2, sb2, sw2) + h1_func(X, y, s2, sb2, sw2, mub, Wbar, VW) + h2_func(p, s2, sb2, sw2, XTX, Wbar, sigmab, varWj, VW) return elbo def c_func(n, p, s2, sb2, sw2): val = p val += - 0.5 * n * np.log(2.0 * np.pi * s2) val += - 0.5 * p * np.log(sb2) val += - 0.5 * p * np.log(sw2) return val def h1_func(X, y, s2, sb2, sw2, mub, Wbar, VW): XWmu = np.linalg.multi_dot([X, np.diag(Wbar), mub]) val1 = - (0.5 / s2) * np.sum(np.square(y - XWmu)) val2 = - 0.5 * np.sum(np.square(mub) * ((VW / s2) + (1 / sb2))) val3 = - 0.5 * np.sum(np.square(Wbar)) / sw2 val = val1 + val2 + val3 return val def h2_func(p, s2, sb2, sw2, XTX, Wbar, sigmab, varWj, VW): (sign, logdetS) = np.linalg.slogdet(sigmab) logdetV = np.sum(np.log(varWj)) W = np.diag(Wbar) WtXtXW = np.linalg.multi_dot([W.T, XTX, W]) val = 0.5 * logdetS + 0.5 * logdetV val += - 0.5 * np.trace(sigmab) / sb2 - 0.5 * np.sum(varWj) / sw2 val += - 0.5 * np.dot(WtXtXW + np.diag(VW), sigmab).trace() / s2 return val def ebmr_WB(X, y, s2_init = 1.0, sb2_init = 1.0, sw2_init = 1.0, binit = None, winit = None, max_iter = 1000, tol = 1e-8 ): XTX = np.dot(X.T, X) XTy = np.dot(X.T, y) n_samples, n_features = X.shape elbo_path = np.zeros(max_iter + 1) mll_path = np.zeros(max_iter + 1) &#39;&#39;&#39; Iteration 0 &#39;&#39;&#39; niter = 0 s2 = s2_init sb2 = sb2_init sw2 = sw2_init mub = np.ones(n_features) if binit is None else binit muWj = np.ones(n_features) if winit is None else winit sigmab = np.zeros((n_features, n_features)) sigmaWj2 = np.zeros(n_features) elbo_path[0] = -np.inf mll_path[0] = -np.inf for itn in range(1, max_iter + 1): &#39;&#39;&#39; GRR for b &#39;&#39;&#39; s2, sb2, mub, sigmab = grr_b(X, y, s2, sb2, muWj, sigmaWj2, XTX, XTy) &#39;&#39;&#39; GRR for W &#39;&#39;&#39; sw2, muWj, sigmaWj2 = grr_W(X, y, s2, sw2, mub, sigmab, muWj, XTX, XTy) &#39;&#39;&#39; Convergence &#39;&#39;&#39; niter += 1 elbo_path[itn] = elbo(X, y, s2, sb2, sw2, mub, sigmab, muWj, sigmaWj2, XTX) mll_path[itn] = ridge_mll(X, y, s2, sb2, muWj) if elbo_path[itn] - elbo_path[itn - 1] &lt; tol: break #if mll_path[itn] - mll_path[itn - 1] &lt; tol: break return s2, sb2, sw2, mub, sigmab, muWj, sigmaWj2, niter, elbo_path[:niter + 1], mll_path[:niter + 1] . . m5 = ebmr_WB(X, y) s2, sb2, sw2, mub, sigmab, W, sigmaW, niter, elbo_path, mll_path = m5 bpred = mub * W ypred = np.dot(X, bpred) fig = plt.figure(figsize = (12, 12)) ax1 = fig.add_subplot(221) ax2 = fig.add_subplot(222) ax3 = fig.add_subplot(223) ax4 = fig.add_subplot(224) ax1.scatter(np.arange(niter-1), elbo_path[2:], edgecolor = &#39;black&#39;, facecolor=&#39;white&#39;) ax1.plot(np.arange(niter-1), elbo_path[2:]) ax1.set_xlabel(&quot;Iterations&quot;) ax1.set_ylabel(&quot;ELBO&quot;) ax2.scatter(np.arange(n), y, edgecolor = &#39;black&#39;, facecolor=&#39;white&#39;) ax2.plot(np.arange(n), ypred, color = &#39;salmon&#39;, label=&quot;Predicted&quot;) ax2.plot(np.arange(n), np.dot(X, btrue), color = &#39;dodgerblue&#39;, label=&quot;True&quot;) ax2.legend() ax2.set_xlabel(&quot;Sample Index&quot;) ax2.set_ylabel(&quot;y&quot;) ax3.scatter(np.arange(p), btrue, edgecolor = &#39;black&#39;, facecolor=&#39;white&#39;, label=&quot;True&quot;) ax3.scatter(np.arange(p), bpred, label=&quot;Predicted&quot;) ax3.legend() ax3.set_xlabel(&quot;Predictor Index&quot;) ax3.set_ylabel(&quot;wb&quot;) nstep = min(80, niter - 2) ax4.scatter(np.arange(nstep), mll_path[-nstep:], edgecolor = &#39;black&#39;, facecolor=&#39;white&#39;, label=&quot;Evidence&quot;) ax4.plot(np.arange(nstep), elbo_path[-nstep:], label=&quot;ELBO&quot;) ax4.legend() ax4.set_xlabel(&quot;Iterations&quot;) ax4.set_ylabel(&quot;ELBO / Evidence&quot;) plt.tight_layout() plt.show() . .",
            "url": "https://banskt.github.io/iridge-notes/2020/12/30/ebmr-with-product-of-coefficients.html",
            "relUrl": "/2020/12/30/ebmr-with-product-of-coefficients.html",
            "date": " • Dec 30, 2020"
        }
        
    
  
    
        ,"post12": {
            "title": "EBMR with adaptive shrinkage prior",
            "content": "About . We consider the following hierarchical Empirical Bayes (EB) regression model: . $p left( mathbf{y} mid s, mathbf{b} right) = N left( mathbf{y} mid mathbf{X} mathbf{b}, s^2 I_n right)$ . $p left( mathbf{b} mid s_b, s, mathbf{W} right) = N left( mathbf{b} mid 0,s_b^2 s^2 mathbf{W} right)$ . $p left(w_j mid g right) = g in mathcal{G}.$ . where $ mathbf{W}= mathrm{diag}(w_1, dots,w_p)$ is a diagonal matrix of prior variances, $s, s_b$ are scalars, and $g$ is a prior distribution that is to be estimated. We refer to the model as the Empirical Bayes Multiple Regression (EBMR). We split this model into two overlapping parts: . The first two equations define the Generalized Ridge Regression (GRR) model. | We call the combination of the last two equations as the &quot;Empirical Bayes Normal Variances&quot; (EBNV) model. | Here, we introduce three different priors for $g$ in the EBNV model and solve GRR using the EM-SVD method (see here). The three priors used for EBNV are: . Point Mass $p left(w_j right) = delta(w_j - lambda_k)$. This corresponds to ridge regression. | Exponential $p left(w_j right) = lambda exp(- lambda w_j)$. This corresponds to Lasso. | Mixture of point mass $p left(w_j right) = sum_{k=1}^{K} pi_k delta(w_j - lambda_k)$. This corresponds to the adaptive shrinkage prior (ash). We consider $ lambda_k$ as known inputs and solve for $ pi_k$ in the EBNV step. | The derivations for the point mass and the exponential prior are provided by Matthew in the corresponding Overleaf document, while some handwritten notes for the mixture of point mass is here. . import numpy as np import pandas as pd from scipy import linalg as sc_linalg import matplotlib.pyplot as plt import ebmrPy from ebmrPy.inference.ebmr import EBMR from ebmrPy.inference import f_elbo from ebmrPy.inference import f_sigma from ebmrPy.inference import penalized_em from ebmrPy.utils import log_density from pymir import mpl_stylesheet mpl_stylesheet.banskt_presentation(splinecolor = &#39;black&#39;, dpi = 72, fontsize = 18) . . Model Setup . We use a simple simulation to evaluate the three priors. . def standardize(X): Xnorm = (X - np.mean(X, axis = 0)) Xstd = Xnorm / np.sqrt((Xnorm * Xnorm).sum(axis = 0)) return Xstd def ridge_data(n, p, sd=5.0, sb2=100.0, seed=100): np.random.seed(seed) X = np.random.normal(0, 1, n * p).reshape(n, p) X = standardize(X) btrue = np.random.normal(0, np.sqrt(sb2), p) y = np.dot(X, btrue) + np.random.normal(0, sd, n) y = y - np.mean(y) #y = y / np.std(y) return X, y, btrue def sparse_data(nsample, nvar, neff, errsigma, sb2=100, seed=200): np.random.seed(seed) X = np.random.normal(0, 1, nsample * nvar).reshape(nsample, nvar) X = standardize(X) btrue = np.zeros(nvar) bidx = np.random.choice(nvar, neff , replace = False) btrue[bidx] = np.random.normal(0, np.sqrt(sb2), neff) y = np.dot(X, btrue) + np.random.normal(0, errsigma, nsample) y = y - np.mean(y) #y = y / np.std(y) return X, y, btrue def test_data(nsample, btrue, errsigma): nvar = btrue.shape[0] X = np.random.normal(0, 1, nsample * nvar).reshape(nsample, nvar) X = standardize(X) y = np.dot(X, btrue) + np.random.normal(0, errsigma, nsample) y = y - np.mean(y) #y = y / np.std(y) return X, y . . n = 50 p = 100 peff = 5 sb = 5.0 sd = 10.0 sb2 = sb * sb X, y, btrue = ridge_data(n, p, sd, sb2, seed=100) #X, y, btrue = sparse_data(n, p, peff, sd, sb2, seed = 200) Xtest, ytest = test_data(200, btrue, sd) . . yvar = np.var(y) residual_var = np.var(y - np.dot(X, btrue)) explained_var = yvar - residual_var print(f&quot;Total variance of y is {yvar:.3f} and the residual variance is {residual_var:.3f}&quot;) print(f&quot;Hence, PVE is {(yvar - residual_var) / yvar:.3f}&quot;) . . Total variance of y is 151.364 and the residual variance is 112.677 Hence, PVE is 0.256 . EBMR . We use the Python implementation of EBMR, see here. I have switched off the convergence criteria, so that we can monitor how the ELBO evolves with iteration. This will evaluate the results over all the max_iter steps and does not gurantee the best solution (if the convergence criteria has not been met after max_iter steps). . priors = [&#39;point&#39;, &#39;dexp&#39;, &#39;mix_point&#39;] #priors = [&#39;point&#39;] mcolors = {&#39;point&#39;: &#39;#2D69C4&#39;, &#39;dexp&#39; : &#39;#93AA00&#39;, &#39;mix_point&#39;: &#39;#CC2529&#39; } mlabels = {&#39;point&#39;: &#39;Ridge&#39;, &#39;dexp&#39; : &#39;Lasso&#39;, &#39;mix_point&#39;: &#39;Ash&#39; } ebmr_ridge = dict() wks = np.array([0.001, 1.0, 2.0, 3.0, 4.0]) for mprior in priors: mix_prior = None if mprior == &#39;mix_point&#39;: mix_prior = wks if mprior == &#39;dexp&#39;:# or mprior == &#39;mix_point&#39;: grr_method = &#39;mle&#39; else: grr_method = &#39;em_svd&#39; ebmr_ridge[mprior] = EBMR(X, y, prior=mprior, grr = grr_method, sigma = &#39;full&#39;, inverse = &#39;direct&#39;, s2_init = 1, sb2_init = 1, max_iter = 100, tol = 1e-8, mll_calc = True, mix_point_w = mix_prior, ignore_convergence = True ) ebmr_ridge[mprior].update() . 2021-03-29 12:25:32,959 | ebmrPy.inference.ebmr | DEBUG | EBMR using point prior, em_svd grr, full b posterior variance, direct inversion 2021-03-29 12:25:33,183 | ebmrPy.inference.ebmr | DEBUG | EBMR using dexp prior, mle grr, full b posterior variance, direct inversion 2021-03-29 12:25:33,263 | ebmrPy.inference.ebmr | DEBUG | EBMR using mix_point prior, em_svd grr, full b posterior variance, direct inversion . We note the ELBO at the last step is similar for point prior (Ridge) and mix_point prior (Ash). . for mprior in priors: print(f&quot;ELBO for {mprior} prior: {ebmr_ridge[mprior].elbo:.4f}&quot;) . . ELBO for point prior: -194.8095 ELBO for dexp prior: -217.3764 ELBO for mix_point prior: -194.0409 . Here are the optimal values of $s^2$, $s_b^2$ and $ bar{w_0}$ (strictly, they are values obtained after the last step and I assume we have reached convergence). There are $p$ elements in the diagonal vector $ bar{ mathbf{W}}$, of which $w_0$ is the first element. . data = [[ebmr_ridge[x].s2, ebmr_ridge[x].sb2, ebmr_ridge[x].Wbar[0], ebmr_ridge[x].s2 * ebmr_ridge[x].sb2 * ebmr_ridge[x].Wbar[0]] for x in priors] colnames = [&#39;s2&#39;, &#39;sb2&#39;, &#39;w_0&#39;, &#39;s2 * sb2 * w_0&#39;] rownames = priors.copy() df = pd.DataFrame.from_records(data, columns = colnames, index = rownames) df.style.format(&quot;{:.3f}&quot;) . . s2 sb2 w_0 s2 * sb2 * w_0 . point 67.007 | 1.000 | 0.640 | 42.898 | . dexp 14.379 | 1.000 | 2.808 | 40.370 | . mix_point 53.139 | 0.325 | 2.815 | 48.585 | . Finally, here are the mixtures coefficients estimated by EBMR for the ash regression. . data = [wks, wks * ebmr_ridge[&#39;mix_point&#39;].sb2, ebmr_ridge[&#39;mix_point&#39;].mixcoef] rownames = [&#39;w_k&#39;, &#39;sb2 * w_k&#39;, &#39;pi_k&#39;] df = pd.DataFrame.from_records(data, index = rownames) # https://pandas.pydata.org/pandas-docs/stable/user_guide/style.html df.style.format(&quot;{:.3f}&quot;) . . 0 1 2 3 4 . w_k 0.001 | 1.000 | 2.000 | 3.000 | 4.000 | . sb2 * w_k 0.000 | 0.325 | 0.650 | 0.974 | 1.299 | . pi_k 0.200 | 0.200 | 0.200 | 0.200 | 0.200 | . The ELBO is decreasing, which is wrong. However asymptotically, the iteration updates lead to exactly same results for ridge regression and ash regression. . fig = plt.figure() ax1 = fig.add_subplot(111) for mprior in priors: mres = ebmr_ridge[mprior] xvals = np.arange(mres.n_iter) ax1.scatter(xvals, mres.elbo_path[1:], color = mcolors[mprior], s=6) ax1.plot(xvals, mres.elbo_path[1:], color = mcolors[mprior], label = mlabels[mprior]) legend1 = ax1.legend(loc = &#39;center right&#39;, bbox_to_anchor = (0.95, 0.3), frameon = False, handlelength = 1.0) #legend1._legend_box.align = &quot;left&quot; #lframe = legend1.get_frame() #lframe.set_linewidth(0) ax1.set_xlabel(&quot;Iteration step&quot;) ax1.set_ylabel(&quot;ELBO&quot;) plt.tight_layout() plt.show() . . The plot on the left shows the prediction of the different methods on a separate test data (plot on the left). The plot on the right compares the expectation of the coefficients of the variables ($ mathbf{b}$) for the different methods. The colors are same as in the plot above. The ridge regression and the ash regression gives identical results. . def lims_xy(ax): lims = [ np.min([ax.get_xlim(), ax.get_ylim()]), # min of both axes np.max([ax.get_xlim(), ax.get_ylim()]), # max of both axes ] return lims def plot_diag(ax): lims = lims_xy(ax) ax.plot(lims, lims, ls=&#39;dotted&#39;, color=&#39;gray&#39;) fig = plt.figure(figsize = (12, 6)) ax1 = fig.add_subplot(121) ax2 = fig.add_subplot(122) #ax2.scatter(np.arange(p), btrue, color = &#39;black&#39;) for mprior in priors: mres = ebmr_ridge[mprior] ypred = np.dot(Xtest, mres.mu) ax1.scatter(ytest, ypred, color = mcolors[mprior], alpha = 0.5) #ax2.scatter(np.arange(p), mres.mu, color = mcolors[mprior], alpha = 0.5) ax2.scatter(btrue, mres.mu, color = mcolors[mprior], alpha = 0.5) plot_diag(ax1) plot_diag(ax2) ax1.set_xlabel(&quot;y&quot;) ax1.set_ylabel(&quot;y_pred&quot;) ax2.set_xlabel(&quot;b&quot;) ax2.set_ylabel(&quot;b_pred&quot;) plt.tight_layout() plt.show() . Compare ELBO with evidence . To check if the ELBOs are correct, I compare the ELBO with the the marginal log likelihood $p left( mathbf{y} mid s^2, s_b^2 right)$ (also called the evidence), calculated at every step for the last 20 iterations. The ELBO is shown with the colored points while the evidence is the black dotted line. . fig = plt.figure(figsize = (18, 6)) ax = [None for mprior in priors] nstep = 20 for i, mprior in enumerate(priors): ax[i] = fig.add_subplot(1, 3, i+1) mres = ebmr_ridge[mprior] xvals = np.arange(mres.n_iter+1)[-nstep:] #ax[i].scatter(mres.elbo_path[2:], mres.mll_path[2:], color = mcolors[mprior], s = 20) ax[i].plot(xvals, mres.mll_path[-nstep:], color = &#39;black&#39;, ls=&#39;dotted&#39;, label = &quot;Evidence&quot;) ax[i].scatter(xvals, mres.elbo_path[-nstep:], color = mcolors[mprior], s=50) #ax[i].plot(xvals, mres.elbo_path[-nstep:], color = mcolors[mprior], lw=1, label = &quot;ELBO&quot;) ax[i].text(0.7, 0.2, mlabels[mprior], transform=ax[i].transAxes) ax[i].set_xlabel(&quot;Iteration&quot;) ax[i].set_ylabel(&quot;ELBO / Evidence&quot;) plt.tight_layout() plt.show() . .",
            "url": "https://banskt.github.io/iridge-notes/2020/12/14/ebmr-ridge-lasso-ash.html",
            "relUrl": "/2020/12/14/ebmr-ridge-lasso-ash.html",
            "date": " • Dec 14, 2020"
        }
        
    
  
    
        ,"post13": {
            "title": "Bayes Lasso using EBMR",
            "content": "About . A sanity check for the Bayes Lasso method using EBMR . import numpy as np import pandas as pd from scipy import linalg as sc_linalg import matplotlib.pyplot as plt import sys sys.path.append(&quot;../../ebmrPy/&quot;) from utils import log_density from inference import f_elbo from inference import penalized_em from inference.ebmr import EBMR import ipdb sys.path.append(&quot;../../utils/&quot;) import mpl_stylesheet mpl_stylesheet.banskt_presentation(fontfamily = &#39;latex-clearsans&#39;, fontsize = 18, colors = &#39;banskt&#39;, dpi = 72) . def standardize(X): Xnorm = (X - np.mean(X, axis = 0)) Xstd = Xnorm / np.sqrt((Xnorm * Xnorm).sum(axis = 0)) return Xstd def lasso_data(nsample, nvar, neff, errsigma, sb2 = 100, seed=200): np.random.seed(seed) X = np.random.normal(0, 1, nsample * nvar).reshape(nsample, nvar) X = standardize(X) btrue = np.zeros(nvar) bidx = np.random.choice(nvar, neff , replace = False) btrue[bidx] = np.random.normal(0, np.sqrt(sb2), neff) y = np.dot(X, btrue) + np.random.normal(0, errsigma, nsample) y = y - np.mean(y) #y = y / np.std(y) return X, y, btrue def lims_xy(ax): lims = [ np.min([ax.get_xlim(), ax.get_ylim()]), # min of both axes np.max([ax.get_xlim(), ax.get_ylim()]), # max of both axes ] return lims def plot_diag(ax): lims = lims_xy(ax) ax.plot(lims, lims, ls=&#39;dotted&#39;, color=&#39;gray&#39;) . n = 50 p = 100 peff = 10 sb2 = 100.0 sd = 2.0 X, y, btrue = lasso_data(n, p, peff, sd, sb2) . fig = plt.figure() ax1 = fig.add_subplot(111) ax1.scatter(np.dot(X,btrue), y) plot_diag(ax1) plt.show() . eblasso = EBMR(X, y, prior=&#39;dexp&#39;, grr=&#39;em&#39;, sigma=&#39;full&#39;, inverse=&#39;direct&#39;, max_iter = 1000, tol=1e-8) ebridge = EBMR(X, y, prior=&#39;point&#39;, grr=&#39;em&#39;, sigma=&#39;full&#39;, inverse=&#39;direct&#39;, max_iter = 1000, tol=1e-8) . 2020-12-01 12:55:43,172 | inference.ebmr | DEBUG | EBMR using dexp prior, em grr, full b posterior variance, direct inversion 2020-12-01 12:55:43,173 | inference.ebmr | DEBUG | EBMR using point prior, em grr, full b posterior variance, direct inversion . eblasso.update() ebridge.update() . data = {&#39;s2&#39;: [eblasso.s2, ebridge.s2], &#39;sb2&#39;: [eblasso.sb2, ebridge.sb2], &#39;s2 x sb2&#39;: [eblasso.s2 * eblasso.sb2, ebridge.s2 * ebridge.sb2], &#39;ELBO&#39;: [eblasso.elbo, ebridge.elbo], } resdf = pd.DataFrame.from_dict(data) resdf.index = [&#39;dexp&#39;, &#39;point&#39;] resdf.round(decimals=3) . s2 sb2 s2 x sb2 ELBO . dexp 0.975 | 6.257 | 6.102 | -158.132 | . point 4.243 | 1.092 | 4.633 | -139.934 | . eblasso.mll_path . array([ -inf, -162.5211116 , -159.397836 , -158.49845882, -158.25413555, -158.17937266, -158.15315927, -158.14270437, -158.13802668, -158.1358067 , -158.13452536, -158.13384434, -158.1334242 , -158.13317589, -158.13305632, -158.13294788, -158.1328467 , -158.13275135, -158.13266116, -158.13257572, -158.13249471, -158.13241787, -158.13234497, -158.13227578, -158.13221009, -158.13214772]) . eblasso.elbo_path . array([ -inf, -165.0350422 , -160.95177667, -159.17931935, -158.55105265, -158.31597417, -158.21862614, -158.17454958, -158.15315208, -158.1421157 , -158.13627118, -158.13311766, -158.13132908, -158.13045503, -158.13032989, -158.13007672, -158.12985772, -158.12969343, -158.1295773 , -158.12949848, -158.1294471 , -158.12941533, -158.12939738, -158.1293891 , -158.12938758, -158.12939083]) . fig = plt.figure() ax1 = fig.add_subplot(111) ypred_lasso = np.dot(X, eblasso.mu) ypred_ridge = np.dot(X, ebridge.mu) ax1.scatter(y, ypred_lasso, color=&#39;salmon&#39;) ax1.scatter(y, ypred_ridge, color=&#39;dodgerblue&#39;) plot_diag(ax1) plt.show() . fig = plt.figure() ax1 = fig.add_subplot(111) ax1.scatter(np.arange(p), btrue, color = &#39;black&#39;, s = 10) ax1.scatter(np.arange(p), eblasso.mu, color=&#39;salmon&#39;) ax1.scatter(np.arange(p), ebridge.mu, color=&#39;dodgerblue&#39;) plt.show() .",
            "url": "https://banskt.github.io/iridge-notes/2020/12/01/lasso-ebmr.html",
            "relUrl": "/2020/12/01/lasso-ebmr.html",
            "date": " • Dec 1, 2020"
        }
        
    
  
    
        ,"post14": {
            "title": "Basic comparison of ridge regression methods",
            "content": "About . A sanity check that all ridge regression methods perform similarly. . import numpy as np import scipy from scipy import linalg as sc_linalg from scipy import sparse as sc_sparse from sklearn.linear_model import Ridge import glmnet_python from glmnet import glmnet from glmnetPrint import glmnetPrint from glmnetCoef import glmnetCoef from glmnetPredict import glmnetPredict import matplotlib.pyplot as plt import sys sys.path.append(&quot;../../utils/&quot;) import mpl_stylesheet mpl_stylesheet.banskt_presentation(fontfamily = &#39;latex-clearsans&#39;, fontsize = 18, colors = &#39;banskt&#39;, dpi = 72) . ../../utils/mpl_stylesheet.py:26: MatplotlibDeprecationWarning: Support for setting the &#39;text.latex.preamble&#39; or &#39;pgf.preamble&#39; rcParam to a list of strings is deprecated since 3.3 and will be removed two minor releases later; set it to a single string instead. matplotlib.rcParams[&#39;text.latex.preamble&#39;] = [r&#39; usepackage[scaled=.86]{ClearSans}&#39;, . def standardize(X): Xnorm = (X - np.mean(X, axis = 0)) #Xstd = Xnorm / np.std(Xnorm, axis = 0) Xstd = Xnorm / np.sqrt((Xnorm * Xnorm).sum(axis = 0)) return Xstd def ridge_data(nsample, nvar, errsigma): X = np.random.normal(0, 1, nsample * nvar).reshape(nsample, nvar) X = standardize(X) btrue = np.random.normal(0, 1, nvar) y = np.dot(X, btrue) + np.random.normal(0, errsigma, nsample) y = y - np.mean(y) y = y / np.std(y) return X, y, btrue . def rsquare(ytrue, ypred): sst = np.sum(np.square(ytrue - np.mean(ytrue))) sse = np.sum(np.square(ytrue - ypred)) rsq = 1 - (sse / sst) return rsq . def logpdf_multivariate_gauss(x, mu, cov): &#39;&#39;&#39; Caculate the multivariate normal density (pdf) Keyword arguments: x = numpy array of a &quot;d x 1&quot; sample vector mu = numpy array of a &quot;d x 1&quot; mean vector cov = &quot;numpy array of a d x d&quot; covariance matrix &#39;&#39;&#39; assert(mu.shape[0] &gt; mu.shape[1]), &#39;mu must be a row vector&#39; assert(x.shape[0] &gt; x.shape[1]), &#39;x must be a row vector&#39; assert(cov.shape[0] == cov.shape[1]), &#39;covariance matrix must be square&#39; assert(mu.shape[0] == cov.shape[0]), &#39;cov_mat and mu_vec must have the same dimensions&#39; assert(mu.shape[0] == x.shape[0]), &#39;mu and x must have the same dimensions&#39; part1 = - nsample * 0.5 * np.log(2. * np.pi) - 0.5 * np.linalg.slogdet(cov)[1] xlm = x - mu part2 = - 0.5 * np.dot(xlm.T, np.dot(np.linalg.inv(cov), xlm)) return float(part1 + part2) def ridge_em(X, Y, s2, sb2, niter = 10): XTX = np.dot(X.T, X) XTY = np.dot(X.T, Y) YTY = np.dot(Y.T, Y) nsample = X.shape[0] nvar = X.shape[1] loglik = np.zeros(niter) i = 0 while i &lt; niter: V = XTX + np.eye(nvar) * (s2 / sb2) Vinv = sc_linalg.cho_solve(sc_linalg.cho_factor(V, lower=True), np.eye(nvar)) SigmaY = sb2 * np.dot(X, X.T) + np.eye(nsample) * s2 loglik[i] = logpdf_multivariate_gauss(Y.reshape(-1, 1), np.zeros((nsample, 1)), SigmaY) Sigmab = s2 * Vinv # posterior variance of b mub = np.dot(Vinv, XTY) # posterior mean of b b2m = np.einsum(&#39;i,j-&gt;ij&#39;, mub, mub) + Sigmab s2 = (YTY + np.dot(XTX, b2m).trace() - 2 * np.dot(XTY, mub)) / nsample sb2 = np.sum(np.square(mub) + np.diag(Sigmab)) / nvar i += 1 return s2, sb2, loglik, mub.reshape(-1), Sigmab . def ridge_ols(X, Y, lmbda): XTX = np.dot(X.T, X) XTY = np.dot(X.T, Y) nvar = X.shape[1] V = XTX + np.eye(nvar) * lmbda Vinv = sc_linalg.cho_solve(sc_linalg.cho_factor(V, lower=True), np.eye(nvar)) bhat = np.dot(Vinv, XTY) return bhat . def svd2XTX(svd): U = svd[0] S = svd[1] Vh = svd[2] nmax = max(S.shape[0], Vh.shape[0]) Sdiag = np.zeros((nmax, nmax)) Sdiag[np.diag_indices(S.shape[0])] = np.square(S) return np.dot(Vh.T, np.dot(Sdiag, Vh)) def c_func(nsample, s2, ElogW): val = - 0.5 * nsample * np.log(2. * np.pi * s2) val += - 0.5 * np.sum(ElogW) return val def h1_func(X, Y, s2, mu, Wbar): val = - (0.5 / s2) * (np.sum(np.square(Y - np.dot(X, mu))) + np.sum(np.square(mu) / Wbar)) return val def h2_func(svd, Sigma, Wbar): XTX = svd2XTX(svd) (sign, logdet) = np.linalg.slogdet(Sigma) val = - 0.5 * np.trace(np.dot(XTX + np.diag(1 / Wbar), Sigma)) + 0.5 * logdet return val def ebmr_initialize(X, Y): svd = sc_linalg.svd(X) XTY = np.dot(X.T, Y) mu = np.zeros(nvar) Sigma = np.zeros((nvar, nvar)) return svd, XTY, mu, Sigma def update_Sigma(svd, Wbar, nvar): XTX = svd2XTX(svd) Sigma = sc_linalg.cho_solve(sc_linalg.cho_factor(XTX + np.diag(1 / Wbar), lower=True), np.eye(nvar)) return Sigma def update_mu(Sigma, XTY): return np.dot(Sigma, XTY) def update_s2(X, Y, mu, Wbar, nsample): A = np.sum(np.square(Y - np.dot(X, mu))) s2 = (A + np.sum(np.square(mu) / Wbar)) / nsample return s2 def update_wg_ridge(mu, Sigma, s2, nvar): bj2 = np.square(mu) + np.diag(Sigma) * s2 W = np.repeat(np.sum(bj2) / s2 / nvar, nvar) KLW = 0. return W, KLW def update_elbo(X, Y, s2, mu, Sigma, Wbar, KLw, svd, nsample, nvar): ElogW = np.log(Wbar) elbo = c_func(nsample, s2, ElogW) + h1_func(X, Y, s2, mu, Wbar) + h2_func(svd, Sigma, Wbar) + KLw return elbo def ebmr(X, Y, niter = 10, tol = 1e-4): nvar = X.shape[1] nsample = X.shape[0] svdX, XTY, mu, Sigma = ebmr_initialize(X, Y) s2 = np.var(Y) Wbar = np.ones(nvar) elbo = -np.inf i = 0 while i &lt; niter: #print(i) #Sigma = update_Sigma(svdX, Wbar, nvar) XTX = svd2XTX(svdX) Sigma = sc_linalg.cho_solve(sc_linalg.cho_factor(XTX + np.diag(1 / Wbar), lower=True), np.eye(nvar)) mu = update_mu(Sigma, XTY) s2 = update_s2(X, Y, mu, Wbar, nsample) Wbar, KLw = update_wg_ridge(mu, Sigma, s2, nvar) elbo_new = update_elbo(X, Y, s2, mu, Sigma, Wbar, KLw, svdX, nsample, nvar) if elbo_new - elbo &lt; tol: break elbo = elbo_new i += 1 return s2, mu, Sigma, Wbar . nsample = 50 nvar = 100 nsim = 20 errsigmas = np.logspace(-0.1, 1, 5) r2 = [None for i in errsigmas] for i, sd in enumerate(errsigmas): lmbda = np.square(sd) r2[i] = dict() r2[i][&#39;ridge_mle&#39;] = list() r2[i][&#39;ridge_em&#39;] = list() r2[i][&#39;ebmr&#39;] = list() r2[i][&#39;sklearn&#39;] = list() r2[i][&#39;sp_lsqr&#39;] = list() r2[i][&#39;glmnet&#39;] = list() for isim in range(nsim): X, y, btrue = ridge_data(nsample, nvar, sd) # Ridge_OLS b_ridge_ols = ridge_ols(X, y, lmbda) y_ridge_ols = np.dot(X, b_ridge_ols) r2[i][&#39;ridge_mle&#39;].append(rsquare(y, y_ridge_ols)) #r2[i][&#39;ridge_ols&#39;].append(y_ridge_ols) #r2[i][&#39;ridge_ols&#39;].append(np.square(y - y_ridge_ols)) #r2[i][&#39;ridge_ols&#39;].append(y) # Ridge EM _, _, _, b_ridge_em, _ = ridge_em(X, y, 1, 1, 500) r2[i][&#39;ridge_em&#39;].append(rsquare(y, np.dot(X, b_ridge_em))) # EBMR _, b_ebmr, _, _ = ebmr(X, y, 1000) y_ebmr = np.dot(X, b_ebmr) r2[i][&#39;ebmr&#39;].append(rsquare(y, y_ebmr)) #Sklearn Ridge clf = Ridge(alpha=lmbda, fit_intercept = False, normalize = False, solver = &#39;sparse_cg&#39;) clf.fit(X, y) b_sklearn = clf.coef_ y_sklearn = np.dot(X, b_sklearn) r2[i][&#39;sklearn&#39;].append(rsquare(y, y_sklearn)) #Sparse Lsqr b_sp_lsqr = sc_sparse.linalg.lsqr(X, y, damp=np.sqrt(lmbda))[0] #b_sp_lsqr = my_lsqr(X, y, damp=1.0)[0] y_sp_lsqr = np.dot(X, b_sp_lsqr) r2[i][&#39;sp_lsqr&#39;].append(rsquare(y, y_sp_lsqr)) #r2[i][&#39;sp_lsqr&#39;].append(y_sp_lsqr) #r2[i][&#39;sp_lsqr&#39;].append(np.square(y - y_sp_lsqr)) #r2[i][&#39;sp_lsqr&#39;].append(y) #glmnet lmbda_glmnet = lmbda / X.shape[0] fit = glmnet(x = X.copy(), y = y.copy(), family = &#39;gaussian&#39;, alpha = 0.0, intr = False, standardize = False, lambdau = np.array([lmbda_glmnet, 1.0])) b_glmnet = glmnetCoef(fit, s = np.float64([lmbda_glmnet]), exact = False)[1:].reshape(-1) y_glmnet = np.dot(X, b_glmnet) r2[i][&#39;glmnet&#39;].append(rsquare(y, y_glmnet)) #r2[i][&#39;glmnet&#39;].append(y_glmnet) #r2[i][&#39;glmnet&#39;].append(np.square(y - y_glmnet)) #r2[i][&#39;glmnet&#39;].append(y) . fig = plt.figure(figsize = (16,6)) ax1 = fig.add_subplot(111) colors = {&#39;ridge_em&#39;: &#39;#2D69C4&#39;, &#39;ebmr&#39;: &#39;#CC2529&#39;, &#39;sklearn&#39;: &#39;#93AA00&#39;, &#39;sp_lsqr&#39;: &#39;#535154&#39;, &#39;glmnet&#39;: &#39;#6B4C9A&#39;, &#39;ridge_mle&#39;: &#39;#FFB300&#39;} facecolors = {&#39;ridge_em&#39;: &#39;#719ad8&#39;, &#39;ebmr&#39;: &#39;#f2888b&#39;, &#39;sklearn&#39;: &#39;#c4d64f&#39;, &#39;sp_lsqr&#39;: &#39;#a6a3a7&#39;, &#39;glmnet&#39;: &#39;#a98fd2&#39;, &#39;ridge_mle&#39;: &#39;#fbd67e&#39;} barwidth = 0.1 nsigma = len(errsigmas) xpos = [(k+1)*2 for k in range(nsigma)] plot_methods = [&#39;ridge_mle&#39;, &#39;sklearn&#39;, &#39;sp_lsqr&#39;, &#39;ridge_em&#39;, &#39;ebmr&#39;, &#39;glmnet&#39;] bxplt = [None for x in plot_methods] for i, method in enumerate(plot_methods): #for i, method in enumerate([&#39;ridge_ols&#39;, &#39;sp_lsqr&#39;, &#39;glmnet&#39;]): #pdata = [np.hstack(r2[k][method]) for k in range(nsigma)] pdata = [r2[k][method] for k in range(nsigma)] xloc = [x + (i * barwidth) + (i * barwidth / 3) for x in xpos] medianprops = dict(linewidth=2, color = colors[method]) whiskerprops = dict(linewidth=2, color = facecolors[method]) boxprops = dict(linewidth=2, color = colors[method], facecolor = facecolors[method]) bxplt[i] = ax1.boxplot(pdata, positions = xloc, showfliers = False, showcaps = False, widths=barwidth, patch_artist=True, notch = False, boxprops = boxprops, medianprops = medianprops, whiskerprops = whiskerprops, ) leghandles = [x[&quot;boxes&quot;][0] for x in bxplt] ax1.legend(leghandles, plot_methods, loc=&#39;lower left&#39;, handlelength = 1.2, labelspacing = 0.2,) ax1.set_xticks(xpos) ax1.set_xticklabels([f&#39;{x:.2f}&#39; for x in errsigmas]) ax1.set_xlim(min(xpos) - 1, max(xpos) + 1) ax1.set_xlabel(r&#39;Prior $ sigma$&#39;) ax1.set_ylabel(r&#39;$R^2$&#39;) ax1.set_title(r&#39;n = 50, p = 100, fixed $ lambda$ estimated from prior&#39;) #plt.savefig(&#39;compare_ridge_methods.png&#39;, bbox_inches=&#39;tight&#39;, facecolor=&#39;white&#39;, transparent=True) plt.tight_layout() plt.show() . def jitter(arr): stdev = .1 * (max(arr) - min(arr)) return arr + abs(np.random.randn(len(arr)) * stdev) fig = plt.figure(figsize = (8, 8)) ax1 = fig.add_subplot(111) nshow = 2 for i, method in enumerate(plot_methods): ydata = r2[nshow][method] if method == &#39;sklearn&#39; or method == &#39;sp_lsqr&#39; or method == &#39;glmnet&#39;: ydata = jitter(ydata) ax1.scatter(r2[nshow][&#39;ridge_mle&#39;], ydata, color = colors[method]) ax1.set_title(f&#39;Comparison of $R^2$ ($ sigma$ = {errsigmas[nshow]:.2f})&#39;, pad = 20) ax1.set_xlabel(&#39;ridge_mle&#39;) ax1.set_ylabel(&#39;All ridge regression methods&#39;) #ax1.set_xticks([0.03, 0.04, 0.05]) ax1.set_xlim([0.25, 0.45]) ax1.set_ylim([0, 1.05]) ax1.plot([0,1],[0,1], ls = &#39;dashed&#39;, color = &#39;gray&#39;) #plt.savefig(&#39;compare_ridge_methods_scatter.png&#39;, bbox_inches=&#39;tight&#39;, facecolor=&#39;white&#39;, transparent=True) plt.show() .",
            "url": "https://banskt.github.io/iridge-notes/2020/11/02/basic-comparison-ridge-regression-methods.html",
            "relUrl": "/2020/11/02/basic-comparison-ridge-regression-methods.html",
            "date": " • Nov 2, 2020"
        }
        
    
  
    
        ,"post15": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://banskt.github.io/iridge-notes/2020/01/14/test-markdown-post.html",
            "relUrl": "/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About",
          "content": "This project aims to explore and improve empirical Bayes multiple regression methods. The notebooks in this corner mostly include occassional unrefined ones from my daily work in order to keep track of progress. For my other lab notebooks, visit here. . The website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://banskt.github.io/iridge-notes/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  
      ,"page6": {
          "title": "Tags",
          "content": "{% if site.categories.size &gt; 0 %} . {% assign categories = “” | split:”” %} {% for c in site.categories %} {% assign categories = categories | push: c[0] %} {% endfor %} {% assign categories = categories | sort_natural %} . This project aims to explore empirical Bayes multiple regression methods. Here is a collection of occassional unrefined Jupyter Notebooks from my daily work. For my other notebook collections, visit here. . . {% for category in categories %} &lt;h3 id =&quot;{{ category }}&quot;&gt;&lt;/i&gt; {{ category }}&lt;/h3&gt; {% for post in site.categories[category] %} {% if post.hide != true %} {%- assign date_format = site.minima.date_format | default: “%b %-d, %Y” -%} &lt;article class=&quot;archive-item&quot;&gt; &lt;p class=&quot;post-meta post-meta-title&quot;&gt;{{post.title}} • {{ post.date | date: date_format }}&lt;/p&gt; &lt;/article&gt; {% endif %} {% endfor %} {% endfor %} . {% endif %} .",
          "url": "https://banskt.github.io/iridge-notes/categories/",
          "relUrl": "/categories/",
          "date": ""
      }
      
  

  
  

  
  

  

  
  

  
      ,"page11": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://banskt.github.io/iridge-notes/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}