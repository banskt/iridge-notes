{
  
    
        "post0": {
            "title": "Sequence of updates in EBMR with product of normals",
            "content": "About . Here, I am simply checking whether the sequence of updates has any effect on the optimization in the variational approximation of EBMR. Earlier, I found that the variational approximation for the product of two normals leads to severe overfitting (see here). . import numpy as np import pandas as pd from scipy import linalg as sc_linalg import matplotlib.pyplot as plt import sys sys.path.append(&quot;../../ebmrPy/&quot;) from inference.ebmr import EBMR from inference import f_elbo from inference import f_sigma from inference import penalized_em from utils import log_density sys.path.append(&quot;../../utils/&quot;) import mpl_stylesheet mpl_stylesheet.banskt_presentation(fontfamily = &#39;latex-clearsans&#39;, fontsize = 18, colors = &#39;banskt&#39;, dpi = 72) . . Toy example . The same trend-filtering data as used previously. . def standardize(X): Xnorm = (X - np.mean(X, axis = 0)) #Xstd = Xnorm / np.std(Xnorm, axis = 0) Xstd = Xnorm / np.sqrt((Xnorm * Xnorm).sum(axis = 0)) return Xstd def trend_data(n, p, bval = 1.0, sd = 1.0, seed=100): np.random.seed(seed) X = np.zeros((n, p)) for i in range(p): X[i:n, i] = np.arange(1, n - i + 1) btrue = np.zeros(p) idx = int(n / 3) btrue[idx] = bval btrue[idx + 1] = -bval y = np.dot(X, btrue) + np.random.normal(0, sd, n) # y = y / np.std(y) return X, y, btrue . . n = 100 p = 200 bval = 8.0 sd = 2.0 X, y, btrue = trend_data(n, p, bval = bval, sd = sd) fig = plt.figure() ax1 = fig.add_subplot(111) ax1.plot(np.arange(n), np.dot(X, btrue), label = &quot;Xb&quot;) ax1.scatter(np.arange(n), y, edgecolor = &#39;black&#39;, facecolor=&#39;white&#39;, label = &quot;Xb + e&quot;) ax1.legend() ax1.set_xlabel(&quot;Sample index&quot;) ax1.set_ylabel(&quot;y&quot;) plt.show() . . Sequence 1 . Here, I am updating $ mathbf{S}$, $ mathbf{m}$, $ sigma^2$, $ sigma_b^2$, $ {v_j^2 }$, $ {a_j }$ and $ sigma_w^2$ in that order. . def ridge_mll(X, y, s2, sb2, W): n, p = X.shape Xscale = np.dot(X, np.diag(W)) XWWtXt = np.dot(Xscale, Xscale.T) sigmay = s2 * (np.eye(n) + sb2 * XWWtXt) muy = np.zeros((n, 1)) return log_density.mgauss(y.reshape(-1,1), muy, sigmay) def grr_b(X, y, s2, sb2, Wbar, varWj, XTX, XTy): n, p = X.shape W = np.diag(Wbar) WtXtXW = np.linalg.multi_dot([W.T, XTX, W]) VW = np.diag(XTX) * varWj sigmabinv = (WtXtXW + np.diag(VW) + np.eye(p) * s2 / sb2) / s2 sigmab = np.linalg.inv(sigmabinv) mub = np.linalg.multi_dot([sigmab, W.T, XTy]) / s2 XWmu = np.linalg.multi_dot([X, W, mub]) mub2 = np.square(mub) s2 = (np.sum(np.square(y - XWmu)) + np.dot((WtXtXW + np.diag(VW)), sigmab).trace() + np.sum(mub2 * VW)) / n sb2 = (np.sum(mub2) + sigmab.trace()) / p return s2, sb2, mub, sigmab def grr_W_old(X, y, s2, sw2, mub, sigmab, muWj, XTX, XTy): n, p = X.shape R = np.einsum(&#39;i,j-&gt;ij&#39;, mub, mub) + sigmab XTXRjj = np.array([XTX[j, j] * R[j, j] for j in range(p)]) #wXTXRj = np.array([np.sum(muWj * XTX[:, j] * R[:, j]) - (muWj[j] * XTXRjj[j]) for j in range(p)]) sigmaWj2 = 1 / ((XTXRjj / s2) + (1 / sw2)) for j in range(p): wXTXRj = np.sum(muWj * XTX[:, j] * R[:, j]) - (muWj[j] * XTXRjj[j]) muWj[j] = sigmaWj2[j] * (mub[j] * XTy[j] - 0.5 * wXTXRj) / s2 sw2 = np.sum(np.square(muWj) + sigmaWj2) / p return sw2, muWj, sigmaWj2 def grr_W(X, y, s2, sw2, mub, sigmab, muWj, XTX, XTy): n, p = X.shape R = np.einsum(&#39;i,j-&gt;ij&#39;, mub, mub) + sigmab XTXRjj = np.diag(XTX) * np.diag(R) sigmaWj2inv = (XTXRjj / s2) + (1 / sw2) wXTXRj = np.array([np.sum(muWj * XTX[:, j] * R[:, j]) - (muWj[j] * XTXRjj[j]) for j in range(p)]) sigmaWj2 = 1 / sigmaWj2inv muWj = sigmaWj2 * (mub * XTy - wXTXRj) / s2 sw2 = np.sum(np.square(muWj) + sigmaWj2) / p #sigmaWj2 = np.zeros(p) return sw2, muWj, sigmaWj2 def elbo(X, y, s2, sb2, sw2, mub, sigmab, Wbar, varWj, XTX): &#39;&#39;&#39; Wbar is a vector which contains the diagonal elements of the diagonal matrix W W = diag_matrix(Wbar) Wbar = diag(W) -- VW is a vector which contains the diagonal elements of the diagonal matrix V_w &#39;&#39;&#39; n, p = X.shape VW = np.diag(XTX) * varWj elbo = c_func(n, p, s2, sb2, sw2) + h1_func(X, y, s2, sb2, sw2, mub, Wbar, VW) + h2_func(p, s2, sb2, sw2, XTX, Wbar, sigmab, varWj, VW) return elbo def c_func(n, p, s2, sb2, sw2): val = p val += - 0.5 * n * np.log(2.0 * np.pi * s2) val += - 0.5 * p * np.log(sb2) val += - 0.5 * p * np.log(sw2) return val def h1_func(X, y, s2, sb2, sw2, mub, Wbar, VW): XWmu = np.linalg.multi_dot([X, np.diag(Wbar), mub]) val1 = - (0.5 / s2) * np.sum(np.square(y - XWmu)) val2 = - 0.5 * np.sum(np.square(mub) * ((VW / s2) + (1 / sb2))) val3 = - 0.5 * np.sum(np.square(Wbar)) / sw2 val = val1 + val2 + val3 return val def h2_func(p, s2, sb2, sw2, XTX, Wbar, sigmab, varWj, VW): (sign, logdetS) = np.linalg.slogdet(sigmab) logdetV = np.sum(np.log(varWj)) W = np.diag(Wbar) WtXtXW = np.linalg.multi_dot([W.T, XTX, W]) val = 0.5 * logdetS + 0.5 * logdetV val += - 0.5 * np.trace(sigmab) / sb2 - 0.5 * np.sum(varWj) / sw2 val += - 0.5 * np.dot(WtXtXW + np.diag(VW), sigmab).trace() / s2 return val def ebmr_WB1(X, y, s2_init = 1.0, sb2_init = 1.0, sw2_init = 1.0, binit = None, winit = None, max_iter = 1000, tol = 1e-8 ): XTX = np.dot(X.T, X) XTy = np.dot(X.T, y) n_samples, n_features = X.shape elbo_path = np.zeros(max_iter + 1) mll_path = np.zeros(max_iter + 1) &#39;&#39;&#39; Iteration 0 &#39;&#39;&#39; niter = 0 s2 = s2_init sb2 = sb2_init sw2 = sw2_init mub = np.ones(n_features) if binit is None else binit muWj = np.ones(n_features) if winit is None else winit sigmab = np.zeros((n_features, n_features)) sigmaWj2 = np.zeros(n_features) elbo_path[0] = -np.inf mll_path[0] = -np.inf for itn in range(1, max_iter + 1): &#39;&#39;&#39; GRR for b &#39;&#39;&#39; s2, sb2, mub, sigmab = grr_b(X, y, s2, sb2, muWj, sigmaWj2, XTX, XTy) &#39;&#39;&#39; GRR for W &#39;&#39;&#39; sw2, muWj, sigmaWj2 = grr_W(X, y, s2, sw2, mub, sigmab, muWj, XTX, XTy) &#39;&#39;&#39; Convergence &#39;&#39;&#39; niter += 1 elbo_path[itn] = elbo(X, y, s2, sb2, sw2, mub, sigmab, muWj, sigmaWj2, XTX) mll_path[itn] = ridge_mll(X, y, s2, sb2, muWj) if elbo_path[itn] - elbo_path[itn - 1] &lt; tol: break #if mll_path[itn] - mll_path[itn - 1] &lt; tol: break return s2, sb2, sw2, mub, sigmab, muWj, sigmaWj2, niter, elbo_path[:niter + 1], mll_path[:niter + 1] . . And this leads to overfitting as we have seen previously. . m1 = ebmr_WB1(X, y) s2, sb2, sw2, mub, sigmab, W, sigmaW, niter, elbo_path, mll_path = m1 bpred = mub * W ypred = np.dot(X, bpred) fig = plt.figure(figsize = (12, 12)) ax1 = fig.add_subplot(221) ax2 = fig.add_subplot(222) ax3 = fig.add_subplot(223) ax4 = fig.add_subplot(224) ax1.scatter(np.arange(niter-1), elbo_path[2:], edgecolor = &#39;black&#39;, facecolor=&#39;white&#39;) ax1.plot(np.arange(niter-1), elbo_path[2:]) ax1.set_xlabel(&quot;Iterations&quot;) ax1.set_ylabel(&quot;ELBO&quot;) ax2.scatter(np.arange(n), y, edgecolor = &#39;black&#39;, facecolor=&#39;white&#39;) ax2.plot(np.arange(n), ypred, color = &#39;salmon&#39;, label=&quot;Predicted&quot;) ax2.plot(np.arange(n), np.dot(X, btrue), color = &#39;dodgerblue&#39;, label=&quot;True&quot;) ax2.legend() ax2.set_xlabel(&quot;Index&quot;) ax2.set_ylabel(&quot;y&quot;) ax3.scatter(np.arange(p), btrue, edgecolor = &#39;black&#39;, facecolor=&#39;white&#39;, label=&quot;True&quot;) ax3.scatter(np.arange(p), bpred, label=&quot;Predicted&quot;) ax3.legend() ax3.set_xlabel(&quot;Index&quot;) ax3.set_ylabel(&quot;wb&quot;) nstep = min(80, niter - 2) ax4.scatter(np.arange(nstep), mll_path[-nstep:], edgecolor = &#39;black&#39;, facecolor=&#39;white&#39;) ax4.plot(np.arange(nstep), elbo_path[-nstep:]) ax4.set_xlabel(&quot;Iterations&quot;) ax4.set_ylabel(&quot;ELBO / Evidence&quot;) plt.tight_layout() plt.show() . . Sequence 2 . Here, I am updating $ mathbf{S}$, $ mathbf{m}$, $ {v_j^2 }$, $ {a_j }$, $ sigma^2$, $ sigma_b^2$ and $ sigma_w^2$ in that order. . def update_qbw(X, s2, sb2, sw2, mub, sigmab, Wbar, varWj, XTX, XTy): n, p = X.shape W = np.diag(Wbar) WtXtXW = np.linalg.multi_dot([W.T, XTX, W]) VW = np.diag(XTX) * varWj # update mub and sigmab sigmabinv = (WtXtXW + np.diag(VW) + np.eye(p) * s2 / sb2) / s2 sigmab = np.linalg.inv(sigmabinv) mub = np.linalg.multi_dot([sigmab, W.T, XTy]) / s2 # update Wbar and varWj R = np.einsum(&#39;i,j-&gt;ij&#39;, mub, mub) + sigmab XTXRjj = np.diag(XTX) * np.diag(R) wXTXRj = np.array([np.sum(Wbar * XTX[:, j] * R[:, j]) - (Wbar[j] * XTXRjj[j]) for j in range(p)]) varWjinv = (XTXRjj / s2) + (1 / sw2) varWj = 1 / varWjinv for j in range(p): wXTXRj = np.sum(Wbar * XTX[:, j] * R[:, j]) - (Wbar[j] * XTXRjj[j]) Wbar[j] = varWj[j] * (mub[j] * XTy[j] - wXTXRj) / s2 #Wbar = varWj * (mub * XTy - wXTXRj) / s2 return mub, sigmab, Wbar, varWj def update_params(X, y, mub, sigmab, Wbar, varWj, XTX): n, p = X.shape W = np.diag(Wbar) VW = np.diag(XTX) * varWj WtXtXW = np.linalg.multi_dot([W.T, XTX, W]) XWmu = np.linalg.multi_dot([X, W, mub]) mub2 = np.square(mub) # update the parameters s2 = (np.sum(np.square(y - XWmu)) + np.dot((WtXtXW + np.diag(VW)), sigmab).trace() + np.sum(mub2 * VW)) / n sb2 = np.sum(np.square(mub) + np.diag(sigmab)) / p sw2 = np.sum(np.square(Wbar) + varWj) / p return s2, sb2, sw2 def ebmr_WB2(X, y, s2_init = 1.0, sb2_init = 1.0, sw2_init = 1.0, binit = None, winit = None, max_iter = 1000, tol = 1e-8 ): XTX = np.dot(X.T, X) XTy = np.dot(X.T, y) n_samples, n_features = X.shape elbo_path = np.zeros(max_iter + 1) mll_path = np.zeros(max_iter + 1) &#39;&#39;&#39; Iteration 0 &#39;&#39;&#39; niter = 0 s2 = s2_init sb2 = sb2_init sw2 = sw2_init mub = np.ones(n_features) if binit is None else binit muWj = np.ones(n_features) if winit is None else winit sigmab = np.zeros((n_features, n_features)) sigmaWj2 = np.zeros(n_features) elbo_path[0] = -np.inf mll_path[0] = -np.inf for itn in range(1, max_iter + 1): &#39;&#39;&#39; Update q(b, w) &#39;&#39;&#39; mub, sigmab, muWj, sigmaWj2 = update_qbw(X, s2, sb2, sw2, mub, sigmab, muWj, sigmaWj2, XTX, XTy) &#39;&#39;&#39; Update s2, sb2, sw2 &#39;&#39;&#39; s2, sb2, sw2 = update_params(X, y, mub, sigmab, muWj, sigmaWj2, XTX) &#39;&#39;&#39; Convergence &#39;&#39;&#39; niter += 1 elbo_path[itn] = elbo(X, y, s2, sb2, sw2, mub, sigmab, muWj, sigmaWj2, XTX) mll_path[itn] = ridge_mll(X, y, s2, sb2, muWj) if elbo_path[itn] - elbo_path[itn - 1] &lt; tol: break #if mll_path[itn] - mll_path[itn - 1] &lt; tol: break return s2, sb2, sw2, mub, sigmab, muWj, sigmaWj2, niter, elbo_path[:niter + 1], mll_path[:niter + 1] . However, there is still an overfitting. . m2 = ebmr_WB2(X, y) s2, sb2, sw2, mub, sigmab, W, sigmaW, niter, elbo_path, mll_path = m2 bpred = mub * W ypred = np.dot(X, bpred) fig = plt.figure(figsize = (12, 12)) ax1 = fig.add_subplot(221) ax2 = fig.add_subplot(222) ax3 = fig.add_subplot(223) ax4 = fig.add_subplot(224) ax1.scatter(np.arange(niter-1), elbo_path[2:], edgecolor = &#39;black&#39;, facecolor=&#39;white&#39;) ax1.plot(np.arange(niter-1), elbo_path[2:]) ax1.set_xlabel(&quot;Iterations&quot;) ax1.set_ylabel(&quot;ELBO&quot;) ax2.scatter(np.arange(n), y, edgecolor = &#39;black&#39;, facecolor=&#39;white&#39;) ax2.plot(np.arange(n), ypred, color = &#39;salmon&#39;, label=&quot;Predicted&quot;) ax2.plot(np.arange(n), np.dot(X, btrue), color = &#39;dodgerblue&#39;, label=&quot;True&quot;) ax2.legend() ax2.set_xlabel(&quot;Index&quot;) ax2.set_ylabel(&quot;y&quot;) ax3.scatter(np.arange(p), btrue, edgecolor = &#39;black&#39;, facecolor=&#39;white&#39;, label=&quot;True&quot;) ax3.scatter(np.arange(p), bpred, label=&quot;Predicted&quot;) ax3.legend() ax3.set_xlabel(&quot;Index&quot;) ax3.set_ylabel(&quot;wb&quot;) nstep = min(80, niter - 2) ax4.scatter(np.arange(nstep), mll_path[-nstep:], edgecolor = &#39;black&#39;, facecolor=&#39;white&#39;) ax4.plot(np.arange(nstep), elbo_path[-nstep:]) ax4.set_xlabel(&quot;Iterations&quot;) ax4.set_ylabel(&quot;ELBO / Evidence&quot;) plt.tight_layout() plt.show() . .",
            "url": "https://banskt.github.io/iridge-notes/jupyter/2021/01/05/ebmr-sequence-of-updates.html",
            "relUrl": "/jupyter/2021/01/05/ebmr-sequence-of-updates.html",
            "date": " • Jan 5, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "EBMR with product of coefficients",
            "content": "About . The idea here is to iteratively multiply the coefficients in a ridge regression to induce sparsity. The implementation here is based on the work of Matthew Stephens (see here). . We consider the following hierarchical Empirical Bayes (EB) regression model: . $p left( mathbf{y} mid mathbf{W}, mathbf{b}, sigma right) = N left( mathbf{y} mid mathbf{X} mathbf{W} mathbf{b}, sigma^2 I_n right)$ . $p left( mathbf{b} mid sigma, sigma_b right) = N left( mathbf{b} mid 0, sigma^2 sigma_b^2 right)$ . $p left(w_j mid g right) = g in mathcal{G}.$ . where $ mathbf{W}$ is diagonal matrix whose diagonal elements are given by $(w_1, dots,w_p)$, $ sigma, sigma_b$ are scalars, and $g$ is a prior distribution that is to be estimated. The motivation is that the product of two normals will be distributed as a linear combination of two chi-square random variables. . $wb = displaystyle frac{1}{4}(w + b)^2 - frac{1}{4}(w - b)^2$ . By iteratively multiplying the $b$ coefficients on $w$, we can get sparse coefficients. . import numpy as np import pandas as pd from scipy import linalg as sc_linalg import matplotlib.pyplot as plt import sys sys.path.append(&quot;../../ebmrPy/&quot;) from inference.ebmr import EBMR from inference import f_elbo from inference import f_sigma from inference import penalized_em from utils import log_density sys.path.append(&quot;../../utils/&quot;) import mpl_stylesheet mpl_stylesheet.banskt_presentation(fontfamily = &#39;latex-clearsans&#39;, fontsize = 18, colors = &#39;banskt&#39;, dpi = 72) . . Toy model . I am using the trend-filtering example as a toy model, following the work of Matthew. . def standardize(X): Xnorm = (X - np.mean(X, axis = 0)) #Xstd = Xnorm / np.std(Xnorm, axis = 0) Xstd = Xnorm / np.sqrt((Xnorm * Xnorm).sum(axis = 0)) return Xstd def trend_data(n, p, bval = 1.0, sd = 1.0, seed=100): np.random.seed(seed) X = np.zeros((n, p)) for i in range(p): X[i:n, i] = np.arange(1, n - i + 1) btrue = np.zeros(p) idx = int(n / 3) btrue[idx] = bval btrue[idx + 1] = -bval y = np.dot(X, btrue) + np.random.normal(0, sd, n) # y = y / np.std(y) return X, y, btrue . . n = 100 p = 200 bval = 8.0 sd = 2.0 X, y, btrue = trend_data(n, p, bval = bval, sd = sd) fig = plt.figure() ax1 = fig.add_subplot(111) ax1.plot(np.arange(n), np.dot(X, btrue), label=&quot;Xb&quot;) ax1.scatter(np.arange(n), y, edgecolor = &#39;black&#39;, facecolor=&#39;white&#39;, label=&quot;Xb + e&quot;) ax1.legend() ax1.set_xlabel(&quot;Sample index&quot;) ax1.set_ylabel(&quot;y&quot;) plt.show() . . Ridge regression using EM . First, I will try normal ridge regression: . $p left( mathbf{y} mid mathbf{b}, sigma^2 right) = N left( mathbf{y} mid mathbf{X} mathbf{b}, sigma^2 mathbb{I}_n right)$ . $p left( mathbf{b} mid sigma_b^2 right) = N left( mathbf{b} mid 0, sigma_b^2 mathbb{I}_p right)$ . m1 = penalized_em.ridge(X, y, 1.0, 1.0, max_iter=4000, tol=1e-8, ignore_convergence=False) m1_bpred = m1[2] m1_ypred = np.dot(X, m1_bpred) fig = plt.figure() ax1 = fig.add_subplot(111) ax1.scatter(np.arange(n), y, edgecolor = &#39;black&#39;, facecolor=&#39;white&#39;) ax1.plot(np.arange(n), m1_ypred) ax1.set_xlabel(&quot;Sample index&quot;) ax1.set_ylabel(&quot;y&quot;) plt.show() . . Iterative ridge regression . Next, I will try iterative ridge regression, using: . $p left( mathbf{y} mid mathbf{b}, sigma^2 right) = N left( mathbf{y} mid mathbf{X} mathbf{W} mathbf{b}, sigma^2 mathbb{I}_n right)$ . $p left( mathbf{b} mid sigma_b^2 right) = N left( mathbf{b} mid 0, sigma_b^2 mathbb{I}_p right)$. . $ mathbf{W}$ is a diagonal matrix whose diagonal elements are given by $ mathbf{w} := (w_1, dots,w_p)$. In every step, we perform EM ridge regression using $ mathbf{X} mathbf{W}$ as the predictor variable. We update $ mathbf{w}_{ mathrm{new}} = mathbf{w}_{ mathrm{old}} mathbf{b}$. The iteration stops when the marginal likelihood given by . $p left( mathbf{y} mid mathbf{W}, sigma^2, sigma_b^2 right) = N left( mathbf{y} mid 0, sigma^2 mathbb{I}_n + sigma_b^2 mathbf{X} mathbf{W}( mathbf{X} mathbf{W})^{ mathsf{T}} right)$ . converges to a tolerance value (tol). Here, we obtain a point estimate of $ mathbf{W}$. We have not set any prior on $ mathbf{W}$ explicitly. Since $ mathbf{b}$ has a Gaussian prior, we can think of $ mathbf{W}$ as having an implicit prior of a product of $T$ Gaussians (where $T$ is the number of iterations). . def ridge_step(X, y, s2, sb2, W): n, p = X.shape Xscale = np.dot(X, np.diag(W)) XTX = np.dot(Xscale.T, Xscale) XTy = np.dot(Xscale.T, y) yTy = np.dot(y.T, y) sigmabinv = (XTX + np.eye(p) * (s2 / sb2)) / s2 sigmab = np.linalg.inv(sigmabinv) # posterior variance of b mub = np.dot(sigmab, XTy) / s2 # posterior mean of b mmT = np.einsum(&#39;i,j-&gt;ij&#39;, mub, mub) BTB = mmT + sigmab XWmu = np.dot(Xscale, mub) s2 = (np.sum(np.square(y - XWmu)) + np.trace(np.dot(XTX, sigmab))) / n sb2 = np.trace(BTB) / p return s2, sb2, mub def ridge_mll(X, y, s2, sb2, W): n, p = X.shape Xscale = np.dot(X, np.diag(W)) XWWtXt = np.dot(Xscale, Xscale.T) sigmay = s2 * (np.eye(n) + sb2 * XWWtXt) muy = np.zeros((n, 1)) return log_density.mgauss(y.reshape(-1,1), muy, sigmay) def iterative_ridge(X, y, s2_init = 1.0, sb2_init = 1.0, sw2_init = 1.0, max_iter = 1000, tol = 1e-8): n_samples, n_features = X.shape mll_path = np.zeros(max_iter + 1) W = np.ones(n_features) &#39;&#39;&#39; Iteration 0 &#39;&#39;&#39; niter = 0 s2 = s2_init sb2 = sb2_init mll_path[0] = ridge_mll(X, y, s2, sb2, W) &#39;&#39;&#39; Iterations &#39;&#39;&#39; for itn in range(1, max_iter + 1): s2, sb2, mub = ridge_step(X, y, s2, sb2, W) W *= mub &#39;&#39;&#39; Convergence &#39;&#39;&#39; niter += 1 mll_path[itn] = ridge_mll(X, y, s2, sb2, W) if mll_path[itn] - mll_path[itn - 1] &lt; tol: break return s2, sb2, mub, W, niter, mll_path[:niter + 1] . . This gives a sparse solution and a better prediction than ridge regression. . m2 = iterative_ridge(X, y, s2_init = 3.0) _, _, bpred, W, niter, mll_path = m2 ypred = np.dot(X, W) fig = plt.figure(figsize = (12, 12)) ax1 = fig.add_subplot(221) ax2 = fig.add_subplot(222) ax3 = fig.add_subplot(223) yvals = mll_path[1:] ax1.scatter(np.arange(niter), yvals, edgecolor = &#39;black&#39;, facecolor=&#39;white&#39;) ax1.set_xlabel(&quot;Iterations&quot;) ax1.set_ylabel(&quot;Log(marginal likelihood)&quot;) ax2.scatter(np.arange(n), y, edgecolor = &#39;black&#39;, facecolor=&#39;white&#39;) ax2.plot(np.arange(n), ypred, color = &#39;salmon&#39;, label=&quot;Predicted&quot;) ax2.plot(np.arange(n), np.dot(X, btrue), color = &#39;dodgerblue&#39;, label=&quot;True&quot;) ax2.legend() ax2.set_xlabel(&quot;Sample index&quot;) ax2.set_ylabel(&quot;y&quot;) ax3.scatter(np.arange(p), btrue, edgecolor = &#39;black&#39;, facecolor=&#39;white&#39;, label=&quot;True&quot;) ax3.scatter(np.arange(p), W, label=&quot;Predicted&quot;) ax3.legend() ax3.set_xlabel(&quot;Predictro index&quot;) ax3.set_ylabel(&quot;wb&quot;) plt.tight_layout() plt.show() . . Approximate ELBO in the iterative framework . In the iterative framework, the prior on $ mathbf{W}$ is essentialy a product of $T$ normals, where $T$ is the number of iterations. We can think of the above problem in the framework of variational approximation and try to calculate the ELBO $F left(q( mathbf{b}, mathbf{W}) right)$. . However, we do not know the $ mathbb{E}_{q( mathbf{W})} left[ ln g( mathbf{W}) right]$ and $ mathbb{E}_{q( mathbf{W})} left[ ln q( mathbf{W}) right]$. Hence, I calculated an approximate ELBO by considering $ mathbb{E}_{q(w)} left[ ln (g( mathbf{W}) / q( mathbf{W})) right] = 0$. Derivations are still in my notebook. . I find that the approximate ELBO is very close to the evidence $p left( mathbf{y} mid mathbf{W}, sigma^2, sigma_b^2 right)$. . def elbo(X, y, s2, sb2, mub, sigmab, Wbar, varWj, XTX, KLqW): &#39;&#39;&#39; Wbar is a vector which contains the diagonal elements of the diagonal matrix W W = diag_matrix(Wbar) Wbar = diag(W) -- VW is a vector which contains the diagonal elements of the diagonal matrix V_w &#39;&#39;&#39; n, p = X.shape VW = np.diag(XTX) * varWj elbo = c_func(n, p, s2, sb2) + h1_func(X, y, s2, sb2, mub, Wbar, VW) + h2_func(p, s2, sb2, XTX, sigmab, Wbar, VW) - KLqW return elbo def c_func(n, p, s2, sb2): val = 0.5 * p val += - 0.5 * p * np.log(sb2) val += - 0.5 * n * np.log(2.0 * np.pi * s2) return val def h1_func(X, y, s2, sb2, mub, Wbar, VW): XWmu = np.linalg.multi_dot([X, np.diag(Wbar), mub]) val1 = - (0.5 / s2) * np.sum(np.square(y - XWmu)) val2 = - 0.5 * np.sum(np.square(mub) * ((VW / s2) + (1 / sb2))) val = val1 + val2 return val def h2_func(p, s2, sb2, XTX, sigmab, Wbar, VW): (sign, logdet) = np.linalg.slogdet(sigmab) W = np.diag(Wbar) WtXtXW = np.linalg.multi_dot([W.T, XTX, W]) val = 0.5 * logdet val += - 0.5 * np.dot(WtXtXW + np.diag(VW) + np.eye(p) * (s2 / sb2), sigmab).trace() / s2 return val def grr_b(X, y, s2, sb2, Wbar, varWj, XTX, XTy): n, p = X.shape W = np.diag(Wbar) WtXtXW = np.linalg.multi_dot([W.T, XTX, W]) VW = np.diag(XTX) * varWj sigmabinv = (WtXtXW + np.diag(VW) + np.eye(p) * s2 / sb2) / s2 sigmab = np.linalg.inv(sigmabinv) mub = np.linalg.multi_dot([sigmab, W, XTy]) / s2 XWmu = np.linalg.multi_dot([X, W, mub]) mub2 = np.square(mub) s2 = (np.sum(np.square(y - XWmu)) + np.dot((WtXtXW + np.diag(VW)), sigmab).trace() + np.sum(mub2 * VW)) / n sb2 = (np.sum(mub2) + sigmab.trace()) / p return s2, sb2, mub, sigmab def grr_W_point(X, y, s2, sw2, mub, sigmab, muWj, sigmaWj2, XTX, XTy): muWj = muWj * mub sigmaWj2 = np.zeros(p) #sigmaWj2 = sigmaWj2 * np.diag(sigmab) KLqW = 0.0 return sw2, muWj, sigmaWj2, KLqW def ebmr_iterative_ridge(X, y, s2_init = 1.0, sb2_init = 1.0, sw2_init = 1.0, w_init = None, max_iter = 1000, tol = 1e-8 ): XTX = np.dot(X.T, X) XTy = np.dot(X.T, y) n_samples, n_features = X.shape elbo_path = np.zeros(max_iter + 1) mll_path = np.zeros(max_iter + 1) &#39;&#39;&#39; Iteration 0 &#39;&#39;&#39; niter = 0 s2 = s2_init sb2 = sb2_init sw2 = sw2_init mub = np.zeros(n_features) sigmab = np.zeros((n_features, n_features)) if w_init is None: muWj = np.ones(n_features) else: muWj = w_init sigmaWj2 = np.zeros(n_features) elbo_path[0] = -np.inf mll_path[0] = -np.inf KLqW = 0 for itn in range(1, max_iter + 1): &#39;&#39;&#39; GRR for b &#39;&#39;&#39; s2, sb2, mub, sigmab = grr_b(X, y, s2, sb2, muWj, sigmaWj2, XTX, XTy) #print(itn, s2, sb2, mub[0]) &#39;&#39;&#39; GRR for W &#39;&#39;&#39; sw2, muWj, sigmaWj2, KLqW = grr_W_point(X, y, s2, sw2, mub, sigmab, muWj, sigmaWj2, XTX, XTy) &#39;&#39;&#39; Convergence &#39;&#39;&#39; niter += 1 elbo_path[itn] = elbo(X, y, s2, sb2, mub, sigmab, muWj, sigmaWj2, XTX, KLqW) mll_path[itn] = ridge_mll(X, y, s2, sb2, muWj) #if elbo_path[itn] - elbo_path[itn - 1] &lt; tol: break if mll_path[itn] - mll_path[itn - 1] &lt; tol: break return s2, sb2, sw2, mub, sigmab, muWj, sigmaWj2, niter, elbo_path[:niter + 1], mll_path[:niter + 1] . . This gives a solution similar to the iterative ridge regression, except that the predicted values of the coefficients are better. However, the approximate ELBO behaves strangely, which is quite expected because it is not a true ELBO. . m3 = ebmr_iterative_ridge(X, y, max_iter = 200) _, _, _, mub, sigmab, W, sigmaW, niter, elbo_path, mll_path = m3 ypred = np.dot(X, W) fig = plt.figure(figsize = (12, 12)) ax1 = fig.add_subplot(221) ax2 = fig.add_subplot(222) ax3 = fig.add_subplot(223) ax4 = fig.add_subplot(224) ax1.scatter(np.arange(niter), mll_path[1:], edgecolor = &#39;black&#39;, facecolor=&#39;white&#39;) #ax1.plot(np.arange(niter), elbo_path[1:]) ax1.set_xlabel(&quot;Iterations&quot;) ax1.set_ylabel(&quot;Evidence&quot;) ax2.scatter(np.arange(n), y, edgecolor = &#39;black&#39;, facecolor=&#39;white&#39;) ax2.plot(np.arange(n), ypred, color = &#39;salmon&#39;, label=&quot;Predicted&quot;) ax2.plot(np.arange(n), np.dot(X, btrue), color = &#39;dodgerblue&#39;, label=&quot;True&quot;) ax2.legend() ax2.set_xlabel(&quot;Sample index&quot;) ax2.set_ylabel(&quot;y&quot;) ax3.scatter(np.arange(p), btrue, edgecolor = &#39;black&#39;, facecolor=&#39;white&#39;, label=&quot;True&quot;) ax3.scatter(np.arange(p), W, label=&quot;Predicted&quot;) ax3.legend() ax3.set_xlabel(&quot;Predictor index&quot;) ax3.set_ylabel(&quot;wb&quot;) nstep = min(80, niter - 2) ax4.scatter(np.arange(nstep), mll_path[-nstep:], edgecolor = &#39;black&#39;, facecolor=&#39;white&#39;, label=&quot;Evidence&quot;) ax4.plot(np.arange(nstep), elbo_path[-nstep:], label=&quot;ELBO&quot;) ax4.legend() ax4.set_xlabel(&quot;Iterations&quot;) ax4.set_ylabel(&quot;ELBO / Evidence&quot;) plt.tight_layout() plt.show() . . Product of two normals . Instead of using a product of multiple normals, we can restrict the coefficients $ mathbf{W} mathbf{b}$ to a product of two normals, and iterate between solving $ mathbf{W}$ and $ mathbf{b}$. . $p left( mathbf{y} mid mathbf{b}, sigma^2 right) = N left( mathbf{y} mid mathbf{X} mathbf{W} mathbf{b}, sigma^2 mathbb{I}_n right) = N left( mathbf{y} mid mathbf{X} mathbf{B} mathbf{w}, sigma^2 mathbb{I}_n right)$ . $p left( mathbf{b} mid sigma_b^2 right) = N left( mathbf{b} mid 0, sigma_b^2 mathbb{I}_p right)$ . $ displaystyle p left( mathbf{w} mid sigma_w^2 right) = prod_{j=1}^{P} N left(w_j mid 0, sigma_w^2 mathbb{I}_p right)$. . As before $ mathbf{W}$ is a $p times p$ diagonal matrix whose diagonal elements are given by $w_j$. The vector of $ {w_j }$ is denoted as $ mathbf{w}$. Here I introduced the matrix $ mathbf{B}$ which is required when solving for $ mathbf{w}$. $ mathbf{B}$ is a $p times p$ diagonal matrix whose diagonal elements are given by $b_j$. I used the following factorization: . $ displaystyle q left( mathbf{b}, mathbf{w} right) = q left( mathbf{b} right) prod_{j=1}^{P}q_j left(w_j right) $. . We can separate the problem to two steps. In the first step, we assume $ mathbf{W}$ is known and solve for $ mathbf{b}$. In the second step, we assume $ mathbf{B}$ is known and solve for $ mathbf{w}$. We continue iterating until we reach convergence. . For the first step, the updates should be the same as for normal ridge regression with fixed $ mathbf{W}$: . $p left( mathbf{b} mid mathbf{y}, mathbf{W}, sigma^2, sigma_b^2 right) = N left( mathbf{b} mid mathbf{m}, mathbf{S} right)$, . $ displaystyle mathbf{S} = left[ frac{1}{ sigma^2} mathbf{W}^{ mathsf{T}} mathbf{X}^{ mathsf{T}} mathbf{X} mathbf{W} + frac{1}{ sigma_b^2} mathbb{I} right]^{-1}$, . $ displaystyle mathbf{m} = frac{1}{ sigma^2} mathbf{S} mathbf{W}^{ mathsf{T}} mathbf{X}^{ mathsf{T}} mathbf{y}$, . $ displaystyle sigma^2 = frac{1}{N} left { left Vert mathbf{y} - mathbf{X} mathbf{W} mathbf{m} right Vert_{2}^{2} + mathrm{Tr} left( mathbf{W}^{ mathsf{T}} mathbf{X}^{ mathsf{T}} mathbf{X} mathbf{W} mathbf{S} right) right }$, . $ displaystyle sigma_b^2 = frac{1}{P} mathrm{Tr} left( mathbf{m} mathbf{m}^{ mathsf{T}} + mathbf{S} right)$ . Similarly, we have another ridge regression for $ mathbf{W}$ with . $p left( mathbf{w} mid mathbf{y}, mathbf{B}, sigma^2, sigma_w^2 right) = N left( mathbf{w} mid mathbf{a}, mathbf{V} right)$, . $ displaystyle mathbf{V} = left[ frac{1}{ sigma^2} mathbf{B}^{ mathsf{T}} mathbf{X}^{ mathsf{T}} mathbf{X} mathbf{B} + frac{1}{ sigma_w^2} mathbb{I} right]^{-1}$, . $ displaystyle mathbf{a} = frac{1}{ sigma^2} mathbf{V} mathbf{B}^{ mathsf{T}} mathbf{X}^{ mathsf{T}} mathbf{y}$, . $ displaystyle sigma^2 = frac{1}{N} left { left Vert mathbf{y} - mathbf{X} mathbf{B} mathbf{a} right Vert_{2}^{2} + mathrm{Tr} left( mathbf{B}^{ mathsf{T}} mathbf{X}^{ mathsf{T}} mathbf{X} mathbf{B} mathbf{V} right) right }$, . $ displaystyle sigma_w^2 = frac{1}{P} mathrm{Tr} left( mathbf{a} mathbf{a}^{ mathsf{T}} + mathbf{V} right)$. . The iteration stops when the ELBO $F(q( mathbf{b}, mathbf{W}))$ converges to a tolerance value (tol). . def grr_step(X, y, s2, sb2, Wbar, XTX, XTy): n, p = X.shape W = np.diag(Wbar) WtXtXW = np.linalg.multi_dot([W.T, XTX, W]) #VW = np.diag(XTX) * varWj sigmabinv = (WtXtXW + np.eye(p) * s2 / sb2) / s2 sigmab = np.linalg.inv(sigmabinv) mub = np.linalg.multi_dot([sigmab, W, XTy]) / s2 XWmu = np.linalg.multi_dot([X, W, mub]) mub2 = np.square(mub) s2 = (np.sum(np.square(y - XWmu)) + np.dot(WtXtXW, sigmab).trace()) / n sb2 = (np.sum(mub2) + sigmab.trace()) / p return s2, sb2, mub, sigmab def elbo(X, y, s2, sb2, sw2, mub, sigmab, Wbar, varWj, XTX): &#39;&#39;&#39; Wbar is a vector which contains the diagonal elements of the diagonal matrix W W = diag_matrix(Wbar) Wbar = diag(W) -- VW is a vector which contains the diagonal elements of the diagonal matrix V_w &#39;&#39;&#39; n, p = X.shape VW = np.diag(XTX) * varWj elbo = c_func(n, p, s2, sb2, sw2) + h1_func(X, y, s2, sb2, sw2, mub, Wbar, VW) + h2_func(p, s2, sb2, sw2, XTX, Wbar, sigmab, varWj, VW) return elbo def c_func(n, p, s2, sb2, sw2): val = p + p * np.log(2.0 * np.pi) val += - 0.5 * n * np.log(2.0 * np.pi * s2) val += - 0.5 * p * np.log(2.0 * np.pi * sb2) val += - 0.5 * p * np.log(2.0 * np.pi * sw2) return val def h1_func(X, y, s2, sb2, sw2, mub, Wbar, VW): XWmu = np.linalg.multi_dot([X, np.diag(Wbar), mub]) val1 = - (0.5 / s2) * np.sum(np.square(y - XWmu)) val2 = - 0.5 * np.sum(np.square(mub)) / sb2 val3 = - 0.5 * np.sum(np.square(Wbar)) / sw2 val = val1 + val2 + val3 return val def h2_func(p, s2, sb2, sw2, XTX, Wbar, sigmab, varWj, VW): (sign, logdetS) = np.linalg.slogdet(sigmab) logdetV = np.sum(np.log(varWj)) W = np.diag(Wbar) WtXtXW = np.linalg.multi_dot([W.T, XTX, W]) val = 0.5 * logdetS #+ 0.5 * logdetV val += - 0.5 * np.trace(sigmab) / sb2 - 0.5 * np.sum(varWj) / sw2 val += - 0.5 * np.dot(WtXtXW, sigmab).trace() / s2 return val def ebmr_WB(X, y, s2_init = 1.0, sb2_init = 1.0, sw2_init = 1.0, max_iter = 1000, tol = 1e-8 ): XTX = np.dot(X.T, X) XTy = np.dot(X.T, y) n_samples, n_features = X.shape elbo_path = np.zeros(max_iter + 1) mll_path = np.zeros(max_iter + 1) &#39;&#39;&#39; Iteration 0 &#39;&#39;&#39; niter = 0 s2 = s2_init sb2 = sb2_init sw2 = sw2_init mub = np.ones(n_features) sigmab = np.zeros((n_features, n_features)) muWj = np.ones(n_features) sigmaWj2 = np.zeros(n_features) elbo_path[0] = -np.inf mll_path[0] = -np.inf for itn in range(1, max_iter + 1): &#39;&#39;&#39; GRR for b &#39;&#39;&#39; s2, sb2, mub, sigmab = grr_step(X, y, s2, sb2, muWj, XTX, XTy) &#39;&#39;&#39; GRR for W &#39;&#39;&#39; _, sw2, muWj, sigmaW = grr_step(X, y, s2, sw2, mub, XTX, XTy) sigmaWj2 = np.diag(sigmaW) &#39;&#39;&#39; Convergence &#39;&#39;&#39; niter += 1 elbo_path[itn] = elbo(X, y, s2, sb2, sw2, mub, sigmab, muWj, sigmaWj2, XTX) mll_path[itn] = ridge_mll(X, y, s2, sb2, muWj) if elbo_path[itn] - elbo_path[itn - 1] &lt; tol: break #if mll_path[itn] - mll_path[itn - 1] &lt; tol: break return s2, sb2, sw2, mub, sigmab, muWj, sigmaWj2, niter, elbo_path[:niter + 1], mll_path[:niter + 1] . . m4 = ebmr_WB(X, y, max_iter = 1000) s2, sb2, sw2, mub, sigmab, muW, sigmaW, niter, elbo_path, mll_path = m4 bpred = mub * muW ypred = np.dot(X, bpred) fig = plt.figure(figsize = (12, 12)) ax1 = fig.add_subplot(221) ax2 = fig.add_subplot(222) ax3 = fig.add_subplot(223) ax4 = fig.add_subplot(224) ax1.scatter(np.arange(niter), elbo_path[1:], edgecolor = &#39;black&#39;, facecolor=&#39;white&#39;) #ax1.plot(np.arange(niter), elbo_path[1:]) ax1.set_xlabel(&quot;Iterations&quot;) ax1.set_ylabel(&quot;ELBO&quot;) ax2.scatter(np.arange(n), y, edgecolor = &#39;black&#39;, facecolor=&#39;white&#39;) ax2.plot(np.arange(n), ypred, color = &#39;salmon&#39;, label=&quot;Predicted&quot;) ax2.plot(np.arange(n), np.dot(X, btrue), color = &#39;dodgerblue&#39;, label=&quot;True&quot;) ax2.legend() ax2.set_xlabel(&quot;Index&quot;) ax2.set_ylabel(&quot;y&quot;) ax3.scatter(np.arange(p), btrue, edgecolor = &#39;black&#39;, facecolor=&#39;white&#39;, label=&quot;True&quot;) ax3.scatter(np.arange(p), bpred, label=&quot;Predicted&quot;) ax3.legend() ax3.set_xlabel(&quot;Index&quot;) ax3.set_ylabel(&quot;wb&quot;) nstep = min(80, niter - 1) ax4.scatter(np.arange(nstep), mll_path[-nstep:], edgecolor = &#39;black&#39;, facecolor=&#39;white&#39;, label=&quot;Evidence&quot;) ax4.plot(np.arange(nstep), elbo_path[-nstep:], label=&quot;ELBO&quot;) ax4.legend() ax4.set_xlabel(&quot;Iterations&quot;) ax4.set_ylabel(&quot;ELBO / Evidence&quot;) plt.tight_layout() plt.show() . . Variational approximation . Finally, I try to cast the problem in a variational approximation framework with a product of two normals. . $p left( mathbf{y} mid mathbf{b}, sigma^2 right) = N left( mathbf{y} mid mathbf{X} mathbf{W} mathbf{b}, sigma^2 mathbb{I}_n right)$ . $p left( mathbf{b} mid sigma_b^2 right) = N left( mathbf{b} mid 0, sigma_b^2 mathbb{I}_p right)$ . $ displaystyle p left( mathbf{w} mid sigma_w^2 right) = prod_{j=1}^{P} N left(w_j mid 0, sigma_w^2 mathbb{I}_p right)$. . As before $ mathbf{W}$ is a $p times p$ diagonal matrix whose diagonal elements are given by $w_j$. The vector of $ {w_j }$ is denoted as $ mathbf{w}$. I used the following factorization: . $ displaystyle q left( mathbf{b}, mathbf{W} right) = q left( mathbf{b} right) prod_{j=1}^{P}q_j left(w_j right) $. . Here we can calulate the exact ELBO and derive the update equations. Derivations are still in my notebook. The updates are as follows: . $q left( mathbf{b} right) = N left( mathbf{b} mid mathbf{m}, mathbf{S} right)$, . $q_j left(w_j right) = N left( w_j mid a_j, v_j^2 right)$, . $ displaystyle mathbf{S} = left[ frac{1}{ sigma^2} left( mathbf{A}^{ mathsf{T}} mathbf{X}^{ mathsf{T}} mathbf{X} mathbf{A} + mathbf{ Lambda}_w right) + frac{1}{ sigma_b^2} mathbb{I} right]^{-1}$, . $ displaystyle mathbf{m} = frac{1}{ sigma^2} mathbf{S} mathbf{A}^{ mathsf{T}} mathbf{X}^{ mathsf{T}} mathbf{y}$, . $ mathbf{A}$ and $ mathbf{ Lambda}_w$ are diagonal matrices whose diagonal elements are given by . $ mathbf{A}_{jj} := a_j$, . $( mathbf{ Lambda}_w)_{jj} = ( mathbf{X}^{ mathsf{T}} mathbf{X})_{jj}v_j^2$. . $ displaystyle v_j^2 = left[ frac{1}{ sigma^2} ( mathbf{X}^{ mathsf{T}} mathbf{X})_{jj} mathbf{R}_{jj} + frac{1}{ sigma_w^2} right]^{-1}$, . $ displaystyle a_j = frac{v_j^2}{ sigma^2} left[m_j ( mathbf{X}^{ mathsf{T}} mathbf{y})_j - sum_{i=1 {i neq j}}^{P}{a_i ( mathbf{X}^{ mathsf{T}} mathbf{X})_{ij} mathbf{R}_{ij}} right]$, . $ mathbf{R} := mathbf{m} mathbf{m}^{ mathsf{T}} + mathbf{S}$. . Finally, the updates of $ sigma^2$, $ sigma_b^2$ and $ sigma_w^2$ can be obtained by taking the derivative of $F(q( mathbf{b}, mathbf{W}))$ and setting them to 0 respectively. This yields . $ displaystyle sigma^2 = frac{1}{N} left { left Vert mathbf{y} - mathbf{X} mathbf{A} mathbf{m} right Vert_{2}^{2} + mathrm{Tr} left( left( mathbf{A}^{ mathsf{T}} mathbf{X}^{ mathsf{T}} mathbf{X} mathbf{A} + mathbf{ Lambda}_w right) mathbf{S} right) + mathbf{m}^{ mathsf{T}} mathbf{ Lambda}_w mathbf{m} right }$, . $ displaystyle sigma_b^2 = frac{1}{P} mathrm{Tr} left( mathbf{m} mathbf{m}^{ mathsf{T}} + mathbf{S} right)$ and . $ displaystyle sigma_w^2 = frac{1}{P} sum_{j=1}^{P} left(a_j^2 + v_j^2 right)$. . The iteration should stop when the ELBO $F(q( mathbf{b}, mathbf{w}))$ converges to a tolerance value (tol). But, there is some mistake in the ELBO calculation and currently, I am not using any convergence criteria. . $ displaystyle F(q) = P - frac{N}{2} ln(2 pi sigma^2) - frac{P}{2} ln( sigma_b^2) - frac{P}{2} ln( sigma_w^2) - frac{1}{2 sigma^2} left { left Vert mathbf{y} - mathbf{X} mathbf{A} mathbf{m} right Vert_2^2 + mathrm{Tr} left( left( mathbf{A}^{ mathsf{T}} mathbf{X}^{ mathsf{T}} mathbf{X} mathbf{A} + mathbf{ Lambda}_w right) mathbf{S} right) + mathbf{m}^{ mathsf{T}} mathbf{ Lambda}_w mathbf{m} right } - frac{1}{2 sigma_b^2} mathrm{Tr} left( mathbf{m} mathbf{m}^{ mathsf{T}} + mathbf{S} right) - frac{1}{2 sigma_w^2} sum_{j=1}^{P} left(a_j^2 + v_j^2 right) + frac{1}{2} ln left lvert mathbf{S} right rvert + frac{1}{2} sum_{j=1}^{P} ln{v_j^2}$ . def grr_b(X, y, s2, sb2, Wbar, varWj, XTX, XTy): n, p = X.shape W = np.diag(Wbar) WtXtXW = np.linalg.multi_dot([W.T, XTX, W]) VW = np.diag(XTX) * varWj sigmabinv = (WtXtXW + np.diag(VW) + np.eye(p) * s2 / sb2) / s2 sigmab = np.linalg.inv(sigmabinv) mub = np.linalg.multi_dot([sigmab, W.T, XTy]) / s2 XWmu = np.linalg.multi_dot([X, W, mub]) mub2 = np.square(mub) s2 = (np.sum(np.square(y - XWmu)) + np.dot((WtXtXW + np.diag(VW)), sigmab).trace() + np.sum(mub2 * VW)) / n sb2 = (np.sum(mub2) + sigmab.trace()) / p return s2, sb2, mub, sigmab def grr_W_old(X, y, s2, sw2, mub, sigmab, muWj, XTX, XTy): n, p = X.shape R = np.einsum(&#39;i,j-&gt;ij&#39;, mub, mub) + sigmab XTXRjj = np.array([XTX[j, j] * R[j, j] for j in range(p)]) #wXTXRj = np.array([np.sum(muWj * XTX[:, j] * R[:, j]) - (muWj[j] * XTXRjj[j]) for j in range(p)]) sigmaWj2 = 1 / ((XTXRjj / s2) + (1 / sw2)) for j in range(p): wXTXRj = np.sum(muWj * XTX[:, j] * R[:, j]) - (muWj[j] * XTXRjj[j]) muWj[j] = sigmaWj2[j] * (mub[j] * XTy[j] - 0.5 * wXTXRj) / s2 sw2 = np.sum(np.square(muWj) + sigmaWj2) / p return sw2, muWj, sigmaWj2 def grr_W(X, y, s2, sw2, mub, sigmab, muWj, XTX, XTy): n, p = X.shape R = np.einsum(&#39;i,j-&gt;ij&#39;, mub, mub) + sigmab XTXRjj = np.diag(XTX) * np.diag(R) sigmaWj2inv = (XTXRjj / s2) + (1 / sw2) wXTXRj = np.array([np.sum(muWj * XTX[:, j] * R[:, j]) - (muWj[j] * XTXRjj[j]) for j in range(p)]) sigmaWj2 = 1 / sigmaWj2inv muWj = sigmaWj2 * (mub * XTy - wXTXRj) / s2 sw2 = np.sum(np.square(muWj) + sigmaWj2) / p #sigmaWj2 = np.zeros(p) return sw2, muWj, sigmaWj2 def elbo(X, y, s2, sb2, sw2, mub, sigmab, Wbar, varWj, XTX): &#39;&#39;&#39; Wbar is a vector which contains the diagonal elements of the diagonal matrix W W = diag_matrix(Wbar) Wbar = diag(W) -- VW is a vector which contains the diagonal elements of the diagonal matrix V_w &#39;&#39;&#39; n, p = X.shape VW = np.diag(XTX) * varWj elbo = c_func(n, p, s2, sb2, sw2) + h1_func(X, y, s2, sb2, sw2, mub, Wbar, VW) + h2_func(p, s2, sb2, sw2, XTX, Wbar, sigmab, varWj, VW) return elbo def c_func(n, p, s2, sb2, sw2): val = p val += - 0.5 * n * np.log(2.0 * np.pi * s2) val += - 0.5 * p * np.log(sb2) val += - 0.5 * p * np.log(sw2) return val def h1_func(X, y, s2, sb2, sw2, mub, Wbar, VW): XWmu = np.linalg.multi_dot([X, np.diag(Wbar), mub]) val1 = - (0.5 / s2) * np.sum(np.square(y - XWmu)) val2 = - 0.5 * np.sum(np.square(mub) * ((VW / s2) + (1 / sb2))) val3 = - 0.5 * np.sum(np.square(Wbar)) / sw2 val = val1 + val2 + val3 return val def h2_func(p, s2, sb2, sw2, XTX, Wbar, sigmab, varWj, VW): (sign, logdetS) = np.linalg.slogdet(sigmab) logdetV = np.sum(np.log(varWj)) W = np.diag(Wbar) WtXtXW = np.linalg.multi_dot([W.T, XTX, W]) val = 0.5 * logdetS + 0.5 * logdetV val += - 0.5 * np.trace(sigmab) / sb2 - 0.5 * np.sum(varWj) / sw2 val += - 0.5 * np.dot(WtXtXW + np.diag(VW), sigmab).trace() / s2 return val def ebmr_WB(X, y, s2_init = 1.0, sb2_init = 1.0, sw2_init = 1.0, binit = None, winit = None, max_iter = 1000, tol = 1e-8 ): XTX = np.dot(X.T, X) XTy = np.dot(X.T, y) n_samples, n_features = X.shape elbo_path = np.zeros(max_iter + 1) mll_path = np.zeros(max_iter + 1) &#39;&#39;&#39; Iteration 0 &#39;&#39;&#39; niter = 0 s2 = s2_init sb2 = sb2_init sw2 = sw2_init mub = np.ones(n_features) if binit is None else binit muWj = np.ones(n_features) if winit is None else winit sigmab = np.zeros((n_features, n_features)) sigmaWj2 = np.zeros(n_features) elbo_path[0] = -np.inf mll_path[0] = -np.inf for itn in range(1, max_iter + 1): &#39;&#39;&#39; GRR for b &#39;&#39;&#39; s2, sb2, mub, sigmab = grr_b(X, y, s2, sb2, muWj, sigmaWj2, XTX, XTy) &#39;&#39;&#39; GRR for W &#39;&#39;&#39; sw2, muWj, sigmaWj2 = grr_W(X, y, s2, sw2, mub, sigmab, muWj, XTX, XTy) &#39;&#39;&#39; Convergence &#39;&#39;&#39; niter += 1 elbo_path[itn] = elbo(X, y, s2, sb2, sw2, mub, sigmab, muWj, sigmaWj2, XTX) mll_path[itn] = ridge_mll(X, y, s2, sb2, muWj) if elbo_path[itn] - elbo_path[itn - 1] &lt; tol: break #if mll_path[itn] - mll_path[itn - 1] &lt; tol: break return s2, sb2, sw2, mub, sigmab, muWj, sigmaWj2, niter, elbo_path[:niter + 1], mll_path[:niter + 1] . . m5 = ebmr_WB(X, y) s2, sb2, sw2, mub, sigmab, W, sigmaW, niter, elbo_path, mll_path = m5 bpred = mub * W ypred = np.dot(X, bpred) fig = plt.figure(figsize = (12, 12)) ax1 = fig.add_subplot(221) ax2 = fig.add_subplot(222) ax3 = fig.add_subplot(223) ax4 = fig.add_subplot(224) ax1.scatter(np.arange(niter-1), elbo_path[2:], edgecolor = &#39;black&#39;, facecolor=&#39;white&#39;) ax1.plot(np.arange(niter-1), elbo_path[2:]) ax1.set_xlabel(&quot;Iterations&quot;) ax1.set_ylabel(&quot;ELBO&quot;) ax2.scatter(np.arange(n), y, edgecolor = &#39;black&#39;, facecolor=&#39;white&#39;) ax2.plot(np.arange(n), ypred, color = &#39;salmon&#39;, label=&quot;Predicted&quot;) ax2.plot(np.arange(n), np.dot(X, btrue), color = &#39;dodgerblue&#39;, label=&quot;True&quot;) ax2.legend() ax2.set_xlabel(&quot;Index&quot;) ax2.set_ylabel(&quot;y&quot;) ax3.scatter(np.arange(p), btrue, edgecolor = &#39;black&#39;, facecolor=&#39;white&#39;, label=&quot;True&quot;) ax3.scatter(np.arange(p), bpred, label=&quot;Predicted&quot;) ax3.legend() ax3.set_xlabel(&quot;Index&quot;) ax3.set_ylabel(&quot;wb&quot;) nstep = min(80, niter - 2) ax4.scatter(np.arange(nstep), mll_path[-nstep:], edgecolor = &#39;black&#39;, facecolor=&#39;white&#39;, label=&quot;Evidence&quot;) ax4.plot(np.arange(nstep), elbo_path[-nstep:], label=&quot;ELBO&quot;) ax4.legend() ax4.set_xlabel(&quot;Iterations&quot;) ax4.set_ylabel(&quot;ELBO / Evidence&quot;) plt.tight_layout() plt.show() . .",
            "url": "https://banskt.github.io/iridge-notes/jupyter/2020/12/30/ebmr-with-product-of-coefficients.html",
            "relUrl": "/jupyter/2020/12/30/ebmr-with-product-of-coefficients.html",
            "date": " • Dec 30, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "EBMR with adaptive shrinkage prior",
            "content": "About . We consider the following hierarchical Empirical Bayes (EB) regression model: . $p left( mathbf{y} mid s, mathbf{b} right) = N left( mathbf{y} mid mathbf{X} mathbf{b}, s^2 I_n right)$ . $p left( mathbf{b} mid s_b, s, mathbf{W} right) = N left( mathbf{b} mid 0,s_b^2 s^2 mathbf{W} right)$ . $p left(w_j mid g right) = g in mathcal{G}.$ . where $ mathbf{W}= mathrm{diag}(w_1, dots,w_p)$ is a diagonal matrix of prior variances, $s, s_b$ are scalars, and $g$ is a prior distribution that is to be estimated. We refer to the model as the Empirical Bayes Multiple Regression (EBMR). We split this model into two overlapping parts: . The first two equations define the Generalized Ridge Regression (GRR) model. | We call the combination of the last two equations as the &quot;Empirical Bayes Normal Variances&quot; (EBNV) model. | Here, we introduce three different priors for $g$ in the EBNV model and solve GRR using the EM-SVD method (see here). The three priors used for EBNV are: . Point Mass $p left(w_j right) = delta(w_j - lambda_k)$. This corresponds to ridge regression. | Exponential $p left(w_j right) = lambda exp(- lambda w_j)$. This corresponds to Lasso. | Mixture of point mass $p left(w_j right) = sum_{k=1}^{K} pi_k delta(w_j - lambda_k)$. This corresponds to the adaptive shrinkage prior (ash). We consider $ lambda_k$ as known inputs and solve for $ pi_k$ in the EBNV step. | The derivations for the point mass and the exponential prior are provided by Matthew in the corresponding Overleaf document, while some handwritten notes for the mixture of point mass is here. . import numpy as np import pandas as pd from scipy import linalg as sc_linalg import matplotlib.pyplot as plt import sys sys.path.append(&quot;../../ebmrPy/&quot;) from inference.ebmr import EBMR from inference import f_elbo from inference import f_sigma from inference import penalized_em from utils import log_density sys.path.append(&quot;../../utils/&quot;) import mpl_stylesheet mpl_stylesheet.banskt_presentation(fontfamily = &#39;latex-clearsans&#39;, fontsize = 18, colors = &#39;banskt&#39;, dpi = 72) . . Model Setup . We use a simple simulation to evaluate the three priors. . def standardize(X): Xnorm = (X - np.mean(X, axis = 0)) Xstd = Xnorm / np.sqrt((Xnorm * Xnorm).sum(axis = 0)) return Xstd def ridge_data(n, p, sd=5.0, sb2=100.0, seed=100): np.random.seed(seed) X = np.random.normal(0, 1, n * p).reshape(n, p) X = standardize(X) btrue = np.random.normal(0, np.sqrt(sb2), p) y = np.dot(X, btrue) + np.random.normal(0, sd, n) y = y - np.mean(y) #y = y / np.std(y) return X, y, btrue def sparse_data(nsample, nvar, neff, errsigma, sb2=100, seed=200): np.random.seed(seed) X = np.random.normal(0, 1, nsample * nvar).reshape(nsample, nvar) X = standardize(X) btrue = np.zeros(nvar) bidx = np.random.choice(nvar, neff , replace = False) btrue[bidx] = np.random.normal(0, np.sqrt(sb2), neff) y = np.dot(X, btrue) + np.random.normal(0, errsigma, nsample) y = y - np.mean(y) #y = y / np.std(y) return X, y, btrue def test_data(nsample, btrue, errsigma): nvar = btrue.shape[0] X = np.random.normal(0, 1, nsample * nvar).reshape(nsample, nvar) X = standardize(X) y = np.dot(X, btrue) + np.random.normal(0, errsigma, nsample) y = y - np.mean(y) #y = y / np.std(y) return X, y . . n = 50 p = 100 peff = 5 sb = 5.0 sd = 10.0 sb2 = sb * sb X, y, btrue = ridge_data(n, p, sd, sb2, seed=100) #X, y, btrue = sparse_data(n, p, peff, sd, sb2, seed = 200) Xtest, ytest = test_data(200, btrue, sd) . . yvar = np.var(y) residual_var = np.var(y - np.dot(X, btrue)) explained_var = yvar - residual_var print(f&quot;Total variance of y is {yvar:.3f} and the residual variance is {residual_var:.3f}&quot;) print(f&quot;Hence, PVE is {(yvar - residual_var) / yvar:.3f}&quot;) . . Total variance of y is 151.364 and the residual variance is 112.677 Hence, PVE is 0.256 . EBMR . We use the Python implementation of EBMR, see here. I have switched off the convergence criteria, so that we can monitor how the ELBO evolves with iteration. This will evaluate the results over all the max_iter steps and does not gurantee the best solution (if the convergence criteria has not been met after max_iter steps). . priors = [&#39;point&#39;, &#39;dexp&#39;, &#39;mix_point&#39;] #priors = [&#39;point&#39;] mcolors = {&#39;point&#39;: &#39;#2D69C4&#39;, &#39;dexp&#39; : &#39;#93AA00&#39;, &#39;mix_point&#39;: &#39;#CC2529&#39; } mlabels = {&#39;point&#39;: &#39;Ridge&#39;, &#39;dexp&#39; : &#39;Lasso&#39;, &#39;mix_point&#39;: &#39;Ash&#39; } ebmr_ridge = dict() wks = np.array([0.001, 1.0, 2.0, 3.0, 4.0]) for mprior in priors: mix_prior = None if mprior == &#39;mix_point&#39;: mix_prior = wks if mprior == &#39;dexp&#39;:# or mprior == &#39;mix_point&#39;: grr_method = &#39;mle&#39; else: grr_method = &#39;em_svd&#39; ebmr_ridge[mprior] = EBMR(X, y, prior=mprior, grr = grr_method, sigma = &#39;full&#39;, inverse = &#39;direct&#39;, s2_init = 1, sb2_init = 1, max_iter = 100, tol = 1e-8, mll_calc = True, mix_point_w = mix_prior, ignore_convergence = True ) ebmr_ridge[mprior].update() . 2020-12-17 15:57:12,922 | inference.ebmr | DEBUG | EBMR using point prior, em_svd grr, full b posterior variance, direct inversion 2020-12-17 15:57:13,123 | inference.ebmr | DEBUG | EBMR using dexp prior, mle grr, full b posterior variance, direct inversion 2020-12-17 15:57:13,191 | inference.ebmr | DEBUG | EBMR using mix_point prior, em_svd grr, full b posterior variance, direct inversion . We note the ELBO at the last step is similar for point prior (Ridge) and mix_point prior (Ash). . for mprior in priors: print(f&quot;ELBO for {mprior} prior: {ebmr_ridge[mprior].elbo:.4f}&quot;) . . ELBO for point prior: -194.8095 ELBO for dexp prior: -217.3764 ELBO for mix_point prior: -194.0409 . Here are the optimal values of $s^2$, $s_b^2$ and $ bar{w_0}$ (strictly, they are values obtained after the last step and I assume we have reached convergence). There are $p$ elements in the diagonal vector $ bar{ mathbf{W}}$, of which $w_0$ is the first element. . data = [[ebmr_ridge[x].s2, ebmr_ridge[x].sb2, ebmr_ridge[x].Wbar[0], ebmr_ridge[x].s2 * ebmr_ridge[x].sb2 * ebmr_ridge[x].Wbar[0]] for x in priors] colnames = [&#39;s2&#39;, &#39;sb2&#39;, &#39;w_0&#39;, &#39;s2 * sb2 * w_0&#39;] rownames = priors.copy() df = pd.DataFrame.from_records(data, columns = colnames, index = rownames) df.style.format(&quot;{:.3f}&quot;) . . s2 sb2 w_0 s2 * sb2 * w_0 . point 67.007 | 1.000 | 0.640 | 42.898 | . dexp 14.379 | 1.000 | 2.808 | 40.370 | . mix_point 53.139 | 0.325 | 2.815 | 48.585 | . Finally, here are the mixtures coefficients estimated by EBMR for the ash regression. . data = [wks, wks * ebmr_ridge[&#39;mix_point&#39;].sb2, ebmr_ridge[&#39;mix_point&#39;].mixcoef] rownames = [&#39;w_k&#39;, &#39;sb2 * w_k&#39;, &#39;pi_k&#39;] df = pd.DataFrame.from_records(data, index = rownames) # https://pandas.pydata.org/pandas-docs/stable/user_guide/style.html df.style.format(&quot;{:.3f}&quot;) . . 0 1 2 3 4 . w_k 0.001 | 1.000 | 2.000 | 3.000 | 4.000 | . sb2 * w_k 0.000 | 0.325 | 0.650 | 0.974 | 1.299 | . pi_k 0.200 | 0.200 | 0.200 | 0.200 | 0.200 | . The ELBO is decreasing, which is wrong. However asymptotically, the iteration updates lead to exactly same results for ridge regression and ash regression. . fig = plt.figure() ax1 = fig.add_subplot(111) for mprior in priors: mres = ebmr_ridge[mprior] xvals = np.arange(mres.n_iter) ax1.scatter(xvals, mres.elbo_path[1:], color = mcolors[mprior], s=6) ax1.plot(xvals, mres.elbo_path[1:], color = mcolors[mprior], label = mlabels[mprior]) legend1 = ax1.legend(loc = &#39;center right&#39;, bbox_to_anchor = (0.95, 0.3), frameon = False, handlelength = 1.0) #legend1._legend_box.align = &quot;left&quot; #lframe = legend1.get_frame() #lframe.set_linewidth(0) ax1.set_xlabel(&quot;Iteration step&quot;) ax1.set_ylabel(&quot;ELBO&quot;) plt.tight_layout() plt.show() . . The plot on the left shows the prediction of the different methods on a separate test data (plot on the left). The plot on the right compares the expectation of the coefficients of the variables ($ mathbf{b}$) for the different methods. The colors are same as in the plot above. The ridge regression and the ash regression gives identical results. . def lims_xy(ax): lims = [ np.min([ax.get_xlim(), ax.get_ylim()]), # min of both axes np.max([ax.get_xlim(), ax.get_ylim()]), # max of both axes ] return lims def plot_diag(ax): lims = lims_xy(ax) ax.plot(lims, lims, ls=&#39;dotted&#39;, color=&#39;gray&#39;) fig = plt.figure(figsize = (12, 6)) ax1 = fig.add_subplot(121) ax2 = fig.add_subplot(122) #ax2.scatter(np.arange(p), btrue, color = &#39;black&#39;) for mprior in priors: mres = ebmr_ridge[mprior] ypred = np.dot(Xtest, mres.mu) ax1.scatter(ytest, ypred, color = mcolors[mprior], alpha = 0.5) #ax2.scatter(np.arange(p), mres.mu, color = mcolors[mprior], alpha = 0.5) ax2.scatter(btrue, mres.mu, color = mcolors[mprior], alpha = 0.5) plot_diag(ax1) plot_diag(ax2) ax1.set_xlabel(&quot;y&quot;) ax1.set_ylabel(&quot;y_pred&quot;) ax2.set_xlabel(&quot;b&quot;) ax2.set_ylabel(&quot;b_pred&quot;) plt.tight_layout() plt.show() . Compare ELBO with evidence . To check if the ELBOs are correct, I compare the ELBO with the the marginal log likelihood $p left( mathbf{y} mid s^2, s_b^2 right)$ (also called the evidence), calculated at every step for the last 20 iterations. The ELBO is shown with the colored points while the evidence is the black dotted line. . fig = plt.figure(figsize = (18, 6)) ax = [None for mprior in priors] nstep = 20 for i, mprior in enumerate(priors): ax[i] = fig.add_subplot(1, 3, i+1) mres = ebmr_ridge[mprior] xvals = np.arange(mres.n_iter+1)[-nstep:] #ax[i].scatter(mres.elbo_path[2:], mres.mll_path[2:], color = mcolors[mprior], s = 20) ax[i].plot(xvals, mres.mll_path[-nstep:], color = &#39;black&#39;, ls=&#39;dotted&#39;, label = &quot;Evidence&quot;) ax[i].scatter(xvals, mres.elbo_path[-nstep:], color = mcolors[mprior], s=50) #ax[i].plot(xvals, mres.elbo_path[-nstep:], color = mcolors[mprior], lw=1, label = &quot;ELBO&quot;) ax[i].text(0.7, 0.2, mlabels[mprior], transform=ax[i].transAxes) ax[i].set_xlabel(&quot;Iteration&quot;) ax[i].set_ylabel(&quot;ELBO / Evidence&quot;) plt.tight_layout() plt.show() . .",
            "url": "https://banskt.github.io/iridge-notes/jupyter/2020/12/14/ebmr_ridge_lasso_ash.html",
            "relUrl": "/jupyter/2020/12/14/ebmr_ridge_lasso_ash.html",
            "date": " • Dec 14, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Bayes Lasso using EBMR",
            "content": "About . A sanity check for the Bayes Lasso method using EBMR . import numpy as np import pandas as pd from scipy import linalg as sc_linalg import matplotlib.pyplot as plt import sys sys.path.append(&quot;../../ebmrPy/&quot;) from utils import log_density from inference import f_elbo from inference import penalized_em from inference.ebmr import EBMR import ipdb sys.path.append(&quot;../../utils/&quot;) import mpl_stylesheet mpl_stylesheet.banskt_presentation(fontfamily = &#39;latex-clearsans&#39;, fontsize = 18, colors = &#39;banskt&#39;, dpi = 72) . def standardize(X): Xnorm = (X - np.mean(X, axis = 0)) Xstd = Xnorm / np.sqrt((Xnorm * Xnorm).sum(axis = 0)) return Xstd def lasso_data(nsample, nvar, neff, errsigma, sb2 = 100, seed=200): np.random.seed(seed) X = np.random.normal(0, 1, nsample * nvar).reshape(nsample, nvar) X = standardize(X) btrue = np.zeros(nvar) bidx = np.random.choice(nvar, neff , replace = False) btrue[bidx] = np.random.normal(0, np.sqrt(sb2), neff) y = np.dot(X, btrue) + np.random.normal(0, errsigma, nsample) y = y - np.mean(y) #y = y / np.std(y) return X, y, btrue def lims_xy(ax): lims = [ np.min([ax.get_xlim(), ax.get_ylim()]), # min of both axes np.max([ax.get_xlim(), ax.get_ylim()]), # max of both axes ] return lims def plot_diag(ax): lims = lims_xy(ax) ax.plot(lims, lims, ls=&#39;dotted&#39;, color=&#39;gray&#39;) . n = 50 p = 100 peff = 10 sb2 = 100.0 sd = 2.0 X, y, btrue = lasso_data(n, p, peff, sd, sb2) . fig = plt.figure() ax1 = fig.add_subplot(111) ax1.scatter(np.dot(X,btrue), y) plot_diag(ax1) plt.show() . eblasso = EBMR(X, y, prior=&#39;dexp&#39;, grr=&#39;em&#39;, sigma=&#39;full&#39;, inverse=&#39;direct&#39;, max_iter = 1000, tol=1e-8) ebridge = EBMR(X, y, prior=&#39;point&#39;, grr=&#39;em&#39;, sigma=&#39;full&#39;, inverse=&#39;direct&#39;, max_iter = 1000, tol=1e-8) . 2020-12-01 12:55:43,172 | inference.ebmr | DEBUG | EBMR using dexp prior, em grr, full b posterior variance, direct inversion 2020-12-01 12:55:43,173 | inference.ebmr | DEBUG | EBMR using point prior, em grr, full b posterior variance, direct inversion . eblasso.update() ebridge.update() . data = {&#39;s2&#39;: [eblasso.s2, ebridge.s2], &#39;sb2&#39;: [eblasso.sb2, ebridge.sb2], &#39;s2 x sb2&#39;: [eblasso.s2 * eblasso.sb2, ebridge.s2 * ebridge.sb2], &#39;ELBO&#39;: [eblasso.elbo, ebridge.elbo], } resdf = pd.DataFrame.from_dict(data) resdf.index = [&#39;dexp&#39;, &#39;point&#39;] resdf.round(decimals=3) . s2 sb2 s2 x sb2 ELBO . dexp 0.975 | 6.257 | 6.102 | -158.132 | . point 4.243 | 1.092 | 4.633 | -139.934 | . eblasso.mll_path . array([ -inf, -162.5211116 , -159.397836 , -158.49845882, -158.25413555, -158.17937266, -158.15315927, -158.14270437, -158.13802668, -158.1358067 , -158.13452536, -158.13384434, -158.1334242 , -158.13317589, -158.13305632, -158.13294788, -158.1328467 , -158.13275135, -158.13266116, -158.13257572, -158.13249471, -158.13241787, -158.13234497, -158.13227578, -158.13221009, -158.13214772]) . eblasso.elbo_path . array([ -inf, -165.0350422 , -160.95177667, -159.17931935, -158.55105265, -158.31597417, -158.21862614, -158.17454958, -158.15315208, -158.1421157 , -158.13627118, -158.13311766, -158.13132908, -158.13045503, -158.13032989, -158.13007672, -158.12985772, -158.12969343, -158.1295773 , -158.12949848, -158.1294471 , -158.12941533, -158.12939738, -158.1293891 , -158.12938758, -158.12939083]) . fig = plt.figure() ax1 = fig.add_subplot(111) ypred_lasso = np.dot(X, eblasso.mu) ypred_ridge = np.dot(X, ebridge.mu) ax1.scatter(y, ypred_lasso, color=&#39;salmon&#39;) ax1.scatter(y, ypred_ridge, color=&#39;dodgerblue&#39;) plot_diag(ax1) plt.show() . fig = plt.figure() ax1 = fig.add_subplot(111) ax1.scatter(np.arange(p), btrue, color = &#39;black&#39;, s = 10) ax1.scatter(np.arange(p), eblasso.mu, color=&#39;salmon&#39;) ax1.scatter(np.arange(p), ebridge.mu, color=&#39;dodgerblue&#39;) plt.show() .",
            "url": "https://banskt.github.io/iridge-notes/jupyter/2020/12/01/lasso-ebmr.html",
            "relUrl": "/jupyter/2020/12/01/lasso-ebmr.html",
            "date": " • Dec 1, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Basic comparison of ridge regression methods",
            "content": "About . A sanity check that all ridge regression methods perform similarly. . import numpy as np import scipy from scipy import linalg as sc_linalg from scipy import sparse as sc_sparse from sklearn.linear_model import Ridge import glmnet_python from glmnet import glmnet from glmnetPrint import glmnetPrint from glmnetCoef import glmnetCoef from glmnetPredict import glmnetPredict import matplotlib.pyplot as plt import sys sys.path.append(&quot;../../utils/&quot;) import mpl_stylesheet mpl_stylesheet.banskt_presentation(fontfamily = &#39;latex-clearsans&#39;, fontsize = 18, colors = &#39;banskt&#39;, dpi = 72) . ../../utils/mpl_stylesheet.py:26: MatplotlibDeprecationWarning: Support for setting the &#39;text.latex.preamble&#39; or &#39;pgf.preamble&#39; rcParam to a list of strings is deprecated since 3.3 and will be removed two minor releases later; set it to a single string instead. matplotlib.rcParams[&#39;text.latex.preamble&#39;] = [r&#39; usepackage[scaled=.86]{ClearSans}&#39;, . def standardize(X): Xnorm = (X - np.mean(X, axis = 0)) #Xstd = Xnorm / np.std(Xnorm, axis = 0) Xstd = Xnorm / np.sqrt((Xnorm * Xnorm).sum(axis = 0)) return Xstd def ridge_data(nsample, nvar, errsigma): X = np.random.normal(0, 1, nsample * nvar).reshape(nsample, nvar) X = standardize(X) btrue = np.random.normal(0, 1, nvar) y = np.dot(X, btrue) + np.random.normal(0, errsigma, nsample) y = y - np.mean(y) y = y / np.std(y) return X, y, btrue . def rsquare(ytrue, ypred): sst = np.sum(np.square(ytrue - np.mean(ytrue))) sse = np.sum(np.square(ytrue - ypred)) rsq = 1 - (sse / sst) return rsq . def logpdf_multivariate_gauss(x, mu, cov): &#39;&#39;&#39; Caculate the multivariate normal density (pdf) Keyword arguments: x = numpy array of a &quot;d x 1&quot; sample vector mu = numpy array of a &quot;d x 1&quot; mean vector cov = &quot;numpy array of a d x d&quot; covariance matrix &#39;&#39;&#39; assert(mu.shape[0] &gt; mu.shape[1]), &#39;mu must be a row vector&#39; assert(x.shape[0] &gt; x.shape[1]), &#39;x must be a row vector&#39; assert(cov.shape[0] == cov.shape[1]), &#39;covariance matrix must be square&#39; assert(mu.shape[0] == cov.shape[0]), &#39;cov_mat and mu_vec must have the same dimensions&#39; assert(mu.shape[0] == x.shape[0]), &#39;mu and x must have the same dimensions&#39; part1 = - nsample * 0.5 * np.log(2. * np.pi) - 0.5 * np.linalg.slogdet(cov)[1] xlm = x - mu part2 = - 0.5 * np.dot(xlm.T, np.dot(np.linalg.inv(cov), xlm)) return float(part1 + part2) def ridge_em(X, Y, s2, sb2, niter = 10): XTX = np.dot(X.T, X) XTY = np.dot(X.T, Y) YTY = np.dot(Y.T, Y) nsample = X.shape[0] nvar = X.shape[1] loglik = np.zeros(niter) i = 0 while i &lt; niter: V = XTX + np.eye(nvar) * (s2 / sb2) Vinv = sc_linalg.cho_solve(sc_linalg.cho_factor(V, lower=True), np.eye(nvar)) SigmaY = sb2 * np.dot(X, X.T) + np.eye(nsample) * s2 loglik[i] = logpdf_multivariate_gauss(Y.reshape(-1, 1), np.zeros((nsample, 1)), SigmaY) Sigmab = s2 * Vinv # posterior variance of b mub = np.dot(Vinv, XTY) # posterior mean of b b2m = np.einsum(&#39;i,j-&gt;ij&#39;, mub, mub) + Sigmab s2 = (YTY + np.dot(XTX, b2m).trace() - 2 * np.dot(XTY, mub)) / nsample sb2 = np.sum(np.square(mub) + np.diag(Sigmab)) / nvar i += 1 return s2, sb2, loglik, mub.reshape(-1), Sigmab . def ridge_ols(X, Y, lmbda): XTX = np.dot(X.T, X) XTY = np.dot(X.T, Y) nvar = X.shape[1] V = XTX + np.eye(nvar) * lmbda Vinv = sc_linalg.cho_solve(sc_linalg.cho_factor(V, lower=True), np.eye(nvar)) bhat = np.dot(Vinv, XTY) return bhat . def svd2XTX(svd): U = svd[0] S = svd[1] Vh = svd[2] nmax = max(S.shape[0], Vh.shape[0]) Sdiag = np.zeros((nmax, nmax)) Sdiag[np.diag_indices(S.shape[0])] = np.square(S) return np.dot(Vh.T, np.dot(Sdiag, Vh)) def c_func(nsample, s2, ElogW): val = - 0.5 * nsample * np.log(2. * np.pi * s2) val += - 0.5 * np.sum(ElogW) return val def h1_func(X, Y, s2, mu, Wbar): val = - (0.5 / s2) * (np.sum(np.square(Y - np.dot(X, mu))) + np.sum(np.square(mu) / Wbar)) return val def h2_func(svd, Sigma, Wbar): XTX = svd2XTX(svd) (sign, logdet) = np.linalg.slogdet(Sigma) val = - 0.5 * np.trace(np.dot(XTX + np.diag(1 / Wbar), Sigma)) + 0.5 * logdet return val def ebmr_initialize(X, Y): svd = sc_linalg.svd(X) XTY = np.dot(X.T, Y) mu = np.zeros(nvar) Sigma = np.zeros((nvar, nvar)) return svd, XTY, mu, Sigma def update_Sigma(svd, Wbar, nvar): XTX = svd2XTX(svd) Sigma = sc_linalg.cho_solve(sc_linalg.cho_factor(XTX + np.diag(1 / Wbar), lower=True), np.eye(nvar)) return Sigma def update_mu(Sigma, XTY): return np.dot(Sigma, XTY) def update_s2(X, Y, mu, Wbar, nsample): A = np.sum(np.square(Y - np.dot(X, mu))) s2 = (A + np.sum(np.square(mu) / Wbar)) / nsample return s2 def update_wg_ridge(mu, Sigma, s2, nvar): bj2 = np.square(mu) + np.diag(Sigma) * s2 W = np.repeat(np.sum(bj2) / s2 / nvar, nvar) KLW = 0. return W, KLW def update_elbo(X, Y, s2, mu, Sigma, Wbar, KLw, svd, nsample, nvar): ElogW = np.log(Wbar) elbo = c_func(nsample, s2, ElogW) + h1_func(X, Y, s2, mu, Wbar) + h2_func(svd, Sigma, Wbar) + KLw return elbo def ebmr(X, Y, niter = 10, tol = 1e-4): nvar = X.shape[1] nsample = X.shape[0] svdX, XTY, mu, Sigma = ebmr_initialize(X, Y) s2 = np.var(Y) Wbar = np.ones(nvar) elbo = -np.inf i = 0 while i &lt; niter: #print(i) #Sigma = update_Sigma(svdX, Wbar, nvar) XTX = svd2XTX(svdX) Sigma = sc_linalg.cho_solve(sc_linalg.cho_factor(XTX + np.diag(1 / Wbar), lower=True), np.eye(nvar)) mu = update_mu(Sigma, XTY) s2 = update_s2(X, Y, mu, Wbar, nsample) Wbar, KLw = update_wg_ridge(mu, Sigma, s2, nvar) elbo_new = update_elbo(X, Y, s2, mu, Sigma, Wbar, KLw, svdX, nsample, nvar) if elbo_new - elbo &lt; tol: break elbo = elbo_new i += 1 return s2, mu, Sigma, Wbar . nsample = 50 nvar = 100 nsim = 20 errsigmas = np.logspace(-0.1, 1, 5) r2 = [None for i in errsigmas] for i, sd in enumerate(errsigmas): lmbda = np.square(sd) r2[i] = dict() r2[i][&#39;ridge_mle&#39;] = list() r2[i][&#39;ridge_em&#39;] = list() r2[i][&#39;ebmr&#39;] = list() r2[i][&#39;sklearn&#39;] = list() r2[i][&#39;sp_lsqr&#39;] = list() r2[i][&#39;glmnet&#39;] = list() for isim in range(nsim): X, y, btrue = ridge_data(nsample, nvar, sd) # Ridge_OLS b_ridge_ols = ridge_ols(X, y, lmbda) y_ridge_ols = np.dot(X, b_ridge_ols) r2[i][&#39;ridge_mle&#39;].append(rsquare(y, y_ridge_ols)) #r2[i][&#39;ridge_ols&#39;].append(y_ridge_ols) #r2[i][&#39;ridge_ols&#39;].append(np.square(y - y_ridge_ols)) #r2[i][&#39;ridge_ols&#39;].append(y) # Ridge EM _, _, _, b_ridge_em, _ = ridge_em(X, y, 1, 1, 500) r2[i][&#39;ridge_em&#39;].append(rsquare(y, np.dot(X, b_ridge_em))) # EBMR _, b_ebmr, _, _ = ebmr(X, y, 1000) y_ebmr = np.dot(X, b_ebmr) r2[i][&#39;ebmr&#39;].append(rsquare(y, y_ebmr)) #Sklearn Ridge clf = Ridge(alpha=lmbda, fit_intercept = False, normalize = False, solver = &#39;sparse_cg&#39;) clf.fit(X, y) b_sklearn = clf.coef_ y_sklearn = np.dot(X, b_sklearn) r2[i][&#39;sklearn&#39;].append(rsquare(y, y_sklearn)) #Sparse Lsqr b_sp_lsqr = sc_sparse.linalg.lsqr(X, y, damp=np.sqrt(lmbda))[0] #b_sp_lsqr = my_lsqr(X, y, damp=1.0)[0] y_sp_lsqr = np.dot(X, b_sp_lsqr) r2[i][&#39;sp_lsqr&#39;].append(rsquare(y, y_sp_lsqr)) #r2[i][&#39;sp_lsqr&#39;].append(y_sp_lsqr) #r2[i][&#39;sp_lsqr&#39;].append(np.square(y - y_sp_lsqr)) #r2[i][&#39;sp_lsqr&#39;].append(y) #glmnet lmbda_glmnet = lmbda / X.shape[0] fit = glmnet(x = X.copy(), y = y.copy(), family = &#39;gaussian&#39;, alpha = 0.0, intr = False, standardize = False, lambdau = np.array([lmbda_glmnet, 1.0])) b_glmnet = glmnetCoef(fit, s = np.float64([lmbda_glmnet]), exact = False)[1:].reshape(-1) y_glmnet = np.dot(X, b_glmnet) r2[i][&#39;glmnet&#39;].append(rsquare(y, y_glmnet)) #r2[i][&#39;glmnet&#39;].append(y_glmnet) #r2[i][&#39;glmnet&#39;].append(np.square(y - y_glmnet)) #r2[i][&#39;glmnet&#39;].append(y) . fig = plt.figure(figsize = (16,6)) ax1 = fig.add_subplot(111) colors = {&#39;ridge_em&#39;: &#39;#2D69C4&#39;, &#39;ebmr&#39;: &#39;#CC2529&#39;, &#39;sklearn&#39;: &#39;#93AA00&#39;, &#39;sp_lsqr&#39;: &#39;#535154&#39;, &#39;glmnet&#39;: &#39;#6B4C9A&#39;, &#39;ridge_mle&#39;: &#39;#FFB300&#39;} facecolors = {&#39;ridge_em&#39;: &#39;#719ad8&#39;, &#39;ebmr&#39;: &#39;#f2888b&#39;, &#39;sklearn&#39;: &#39;#c4d64f&#39;, &#39;sp_lsqr&#39;: &#39;#a6a3a7&#39;, &#39;glmnet&#39;: &#39;#a98fd2&#39;, &#39;ridge_mle&#39;: &#39;#fbd67e&#39;} barwidth = 0.1 nsigma = len(errsigmas) xpos = [(k+1)*2 for k in range(nsigma)] plot_methods = [&#39;ridge_mle&#39;, &#39;sklearn&#39;, &#39;sp_lsqr&#39;, &#39;ridge_em&#39;, &#39;ebmr&#39;, &#39;glmnet&#39;] bxplt = [None for x in plot_methods] for i, method in enumerate(plot_methods): #for i, method in enumerate([&#39;ridge_ols&#39;, &#39;sp_lsqr&#39;, &#39;glmnet&#39;]): #pdata = [np.hstack(r2[k][method]) for k in range(nsigma)] pdata = [r2[k][method] for k in range(nsigma)] xloc = [x + (i * barwidth) + (i * barwidth / 3) for x in xpos] medianprops = dict(linewidth=2, color = colors[method]) whiskerprops = dict(linewidth=2, color = facecolors[method]) boxprops = dict(linewidth=2, color = colors[method], facecolor = facecolors[method]) bxplt[i] = ax1.boxplot(pdata, positions = xloc, showfliers = False, showcaps = False, widths=barwidth, patch_artist=True, notch = False, boxprops = boxprops, medianprops = medianprops, whiskerprops = whiskerprops, ) leghandles = [x[&quot;boxes&quot;][0] for x in bxplt] ax1.legend(leghandles, plot_methods, loc=&#39;lower left&#39;, handlelength = 1.2, labelspacing = 0.2,) ax1.set_xticks(xpos) ax1.set_xticklabels([f&#39;{x:.2f}&#39; for x in errsigmas]) ax1.set_xlim(min(xpos) - 1, max(xpos) + 1) ax1.set_xlabel(r&#39;Prior $ sigma$&#39;) ax1.set_ylabel(r&#39;$R^2$&#39;) ax1.set_title(r&#39;n = 50, p = 100, fixed $ lambda$ estimated from prior&#39;) #plt.savefig(&#39;compare_ridge_methods.png&#39;, bbox_inches=&#39;tight&#39;, facecolor=&#39;white&#39;, transparent=True) plt.tight_layout() plt.show() . def jitter(arr): stdev = .1 * (max(arr) - min(arr)) return arr + abs(np.random.randn(len(arr)) * stdev) fig = plt.figure(figsize = (8, 8)) ax1 = fig.add_subplot(111) nshow = 2 for i, method in enumerate(plot_methods): ydata = r2[nshow][method] if method == &#39;sklearn&#39; or method == &#39;sp_lsqr&#39; or method == &#39;glmnet&#39;: ydata = jitter(ydata) ax1.scatter(r2[nshow][&#39;ridge_mle&#39;], ydata, color = colors[method]) ax1.set_title(f&#39;Comparison of $R^2$ ($ sigma$ = {errsigmas[nshow]:.2f})&#39;, pad = 20) ax1.set_xlabel(&#39;ridge_mle&#39;) ax1.set_ylabel(&#39;All ridge regression methods&#39;) #ax1.set_xticks([0.03, 0.04, 0.05]) ax1.set_xlim([0.25, 0.45]) ax1.set_ylim([0, 1.05]) ax1.plot([0,1],[0,1], ls = &#39;dashed&#39;, color = &#39;gray&#39;) #plt.savefig(&#39;compare_ridge_methods_scatter.png&#39;, bbox_inches=&#39;tight&#39;, facecolor=&#39;white&#39;, transparent=True) plt.show() .",
            "url": "https://banskt.github.io/iridge-notes/jupyter/2020/11/02/basic-comparison-ridge-regression-methods.html",
            "relUrl": "/jupyter/2020/11/02/basic-comparison-ridge-regression-methods.html",
            "date": " • Nov 2, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://banskt.github.io/iridge-notes/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://banskt.github.io/iridge-notes/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://banskt.github.io/iridge-notes/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}