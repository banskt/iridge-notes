{
  
    
        "post0": {
            "title": "EBMR with adaptive shrinkage prior",
            "content": "About . We consider the following hierarchical Empirical Bayes (EB) regression model: . $$ begin{aligned} label{eqn:VEB_iridge1} p left( mathbf{y} mid s, mathbf{b} right) &amp;= N left( mathbf{y} mid mathbf{X} mathbf{b}, s^2 I_n right) label{eqn:VEB_iridge2} p left( mathbf{b} mid s_b, s, mathbf{W} right) &amp;= N left( mathbf{b} mid 0,s_b^2 s^2 mathbf{W} right) label{eqn:VEB_iridge3} p left(w_j mid g right) &amp;= g in mathcal{G}. end{aligned} $$where $ mathbf{W}= mathrm{diag}(w_1, dots,w_p)$ is a diagonal matrix of prior variances, $s, s_b$ are scalars, and $g$ is a prior distribution that is to be estimated. We refer to the model as the Empirical Bayes Multiple Regression (EBMR). We split this model into two overlapping parts: . The first two equations define the Generalized Ridge Regression (GRR) model. | We call the combination of the last two equations as the &quot;Empirical Bayes Normal Variances&quot; (EBNV) model. | Here, we introduce three different priors for $g$ in the EBNV model and solve GRR using the EM-SVD method (see here). The three priors used for EBNV are: . Point Mass $p left(w_j right) = delta(w_j - lambda_k)$. This corresponds to ridge regression. | Exponential $p left(w_j right) = lambda exp(- lambda w_j)$. This corresponds to Lasso. | Mixture of point mass $p left(w_j right) = sum_{k=1}^{K} pi_k delta(w_j - lambda_k)$. This corresponds to the adaptive shrinkage prior (ash). We consider $ lambda_k$ as known inputs and solve for $ pi_k$ in the EBNV step. | The derivations for the point mass and the exponential prior are provided by Matthew in the corresponding Overleaf document, while some handwritten notes for the mixture of point mass is here. . import numpy as np import pandas as pd from scipy import linalg as sc_linalg import matplotlib.pyplot as plt import sys sys.path.append(&quot;../../ebmrPy/&quot;) from inference.ebmr import EBMR from inference import f_elbo from inference import f_sigma from inference import penalized_em from utils import log_density sys.path.append(&quot;../../utils/&quot;) import mpl_stylesheet mpl_stylesheet.banskt_presentation(fontfamily = &#39;latex-clearsans&#39;, fontsize = 18, colors = &#39;banskt&#39;, dpi = 72) . . Model Setup . We use a simple simulation to evaluate the three priors. . def standardize(X): Xnorm = (X - np.mean(X, axis = 0)) Xstd = Xnorm / np.sqrt((Xnorm * Xnorm).sum(axis = 0)) return Xstd def ridge_data(n, p, sd=5.0, sb2=100.0, seed=100): np.random.seed(seed) X = np.random.normal(0, 1, n * p).reshape(n, p) X = standardize(X) btrue = np.random.normal(0, np.sqrt(sb2), p) y = np.dot(X, btrue) + np.random.normal(0, sd, n) y = y - np.mean(y) #y = y / np.std(y) return X, y, btrue def sparse_data(nsample, nvar, neff, errsigma, sb2=100, seed=200): np.random.seed(seed) X = np.random.normal(0, 1, nsample * nvar).reshape(nsample, nvar) X = standardize(X) btrue = np.zeros(nvar) bidx = np.random.choice(nvar, neff , replace = False) btrue[bidx] = np.random.normal(0, np.sqrt(sb2), neff) y = np.dot(X, btrue) + np.random.normal(0, errsigma, nsample) y = y - np.mean(y) #y = y / np.std(y) return X, y, btrue def test_data(nsample, btrue, errsigma): nvar = btrue.shape[0] X = np.random.normal(0, 1, nsample * nvar).reshape(nsample, nvar) X = standardize(X) y = np.dot(X, btrue) + np.random.normal(0, errsigma, nsample) y = y - np.mean(y) #y = y / np.std(y) return X, y . . n = 50 p = 100 peff = 40 sb = 5.0 sd = 10.0 sb2 = sb * sb X, y, btrue = ridge_data(n, p, sd, sb2, seed=100) #X, y, btrue = sparse_data(n, p, peff, sd, sb2, seed = 200) Xtest, ytest = test_data(200, btrue, sd) . . yvar = np.var(y) residual_var = np.var(y - np.dot(X, btrue)) explained_var = yvar - residual_var print(f&quot;Total variance of y is {yvar:.3f} and the residual variance is {residual_var:.3f}&quot;) print(f&quot;Hence, PVE is {(yvar - residual_var) / yvar:.3f}&quot;) . . Total variance of y is 151.364 and the residual variance is 112.677 Hence, PVE is 0.256 . EBMR . We use the Python implementation of EBMR, see here. I have switched off the convergence criteria, so that we can monitor how the ELBO evolves with iteration. This will evaluate the results over all the max_iter steps and does not gurantee the best solution (if the convergence criteria has not been met after max_iter steps). . priors = [&#39;point&#39;, &#39;dexp&#39;, &#39;mix_point&#39;] #priors = [&#39;point&#39;] mcolors = {&#39;point&#39;: &#39;#2D69C4&#39;, &#39;dexp&#39; : &#39;#93AA00&#39;, &#39;mix_point&#39;: &#39;#CC2529&#39; } mlabels = {&#39;point&#39;: &#39;Ridge&#39;, &#39;dexp&#39; : &#39;Lasso&#39;, &#39;mix_point&#39;: &#39;Ash&#39; } ebmr_ridge = dict() wks = np.array([0.001, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]) for mprior in priors: mix_prior = None if mprior == &#39;mix_point&#39;: mix_prior = wks ebmr_ridge[mprior] = EBMR(X, y, prior=mprior, grr = &#39;mle&#39;, sigma = &#39;full&#39;, inverse = &#39;direct&#39;, s2_init = 1, sb2_init = 1, max_iter = 100, tol = 1e-8, mll_calc = True, mix_point_w = mix_prior, ignore_convergence = True ) ebmr_ridge[mprior].update() . 2020-12-16 15:47:24,384 | inference.ebmr | DEBUG | EBMR using point prior, mle grr, full b posterior variance, direct inversion 2020-12-16 15:47:24,458 | inference.ebmr | DEBUG | EBMR using dexp prior, mle grr, full b posterior variance, direct inversion 2020-12-16 15:47:24,510 | inference.ebmr | DEBUG | EBMR using mix_point prior, mle grr, full b posterior variance, direct inversion . We note the ELBO at the last step is similar for point prior (Ridge) and mix_point prior (Ash). . for mprior in priors: print(f&quot;ELBO for {mprior} prior: {ebmr_ridge[mprior].elbo:.4f}&quot;) . . ELBO for point prior: -194.8148 ELBO for dexp prior: -217.3764 ELBO for mix_point prior: -194.8447 . Here are the optimal values of $s^2$, $s_b^2$ and $ bar{w_0}$ (strictly, they are values obtained after the last step and I assume we have reached convergence). There are $p$ elements in the diagonal vector $ bar{ mathbf{W}}$, of which $w_0$ is the first element. . data = [[ebmr_ridge[x].s2, ebmr_ridge[x].sb2, ebmr_ridge[x].Wbar[0], ebmr_ridge[x].s2 * ebmr_ridge[x].sb2 * ebmr_ridge[x].Wbar[0]] for x in priors] colnames = [&#39;s2&#39;, &#39;sb2&#39;, &#39;w_0&#39;, &#39;s2 * sb2 * w_0&#39;] rownames = priors.copy() df = pd.DataFrame.from_records(data, columns = colnames, index = rownames) df.style.format(&quot;{:.3f}&quot;) . . s2 sb2 w_0 s2 * sb2 * w_0 . point 62.715 | 1.000 | 0.729 | 45.707 | . dexp 14.379 | 1.000 | 2.808 | 40.370 | . mix_point 56.061 | 1.000 | 0.900 | 50.446 | . Finally, here are the mixtures coefficients estimated by EBMR for the ash regression. . data = [wks, wks * ebmr_ridge[&#39;mix_point&#39;].sb2, ebmr_ridge[&#39;mix_point&#39;].mixcoef] rownames = [&#39;w_k&#39;, &#39;sb2 * w_k&#39;, &#39;pi_k&#39;] df = pd.DataFrame.from_records(data, index = rownames) # https://pandas.pydata.org/pandas-docs/stable/user_guide/style.html df.style.format(&quot;{:.3f}&quot;) . . 0 1 2 3 4 5 6 7 8 9 . w_k 0.001 | 0.100 | 0.200 | 0.300 | 0.400 | 0.500 | 0.600 | 0.700 | 0.800 | 0.900 | . sb2 * w_k 0.001 | 0.100 | 0.200 | 0.300 | 0.400 | 0.500 | 0.600 | 0.700 | 0.800 | 0.900 | . pi_k 0.000 | 0.000 | 0.000 | 0.000 | 0.000 | 0.000 | 0.000 | 0.000 | 0.002 | 0.998 | . The ELBO updates are somehow wrong, but asymptotically, the iteration updates lead to exactly same results for ridge regression and ash regression. . fig = plt.figure() ax1 = fig.add_subplot(111) for mprior in priors: mres = ebmr_ridge[mprior] xvals = np.arange(mres.n_iter) ax1.scatter(xvals, mres.elbo_path[1:], color = mcolors[mprior], s=6) ax1.plot(xvals, mres.elbo_path[1:], color = mcolors[mprior], label = mlabels[mprior]) legend1 = ax1.legend(loc = &#39;center right&#39;, bbox_to_anchor = (0.95, 0.3), frameon = False, handlelength = 1.0) #legend1._legend_box.align = &quot;left&quot; #lframe = legend1.get_frame() #lframe.set_linewidth(0) ax1.set_xlabel(&quot;Iteration step&quot;) ax1.set_ylabel(&quot;ELBO&quot;) plt.tight_layout() plt.show() . . The plot on the left shows the prediction of the different methods on a separate test data (plot on the left). The plot on the right compares the expectation of the coefficients of the variables ($ mathbf{b}$) for the different methods. The colors are same as in the plot above. The ridge regression and the ash regression gives identical results. . def lims_xy(ax): lims = [ np.min([ax.get_xlim(), ax.get_ylim()]), # min of both axes np.max([ax.get_xlim(), ax.get_ylim()]), # max of both axes ] return lims def plot_diag(ax): lims = lims_xy(ax) ax.plot(lims, lims, ls=&#39;dotted&#39;, color=&#39;gray&#39;) fig = plt.figure(figsize = (12, 6)) ax1 = fig.add_subplot(121) ax2 = fig.add_subplot(122) #ax2.scatter(np.arange(p), btrue, color = &#39;black&#39;) for mprior in priors: mres = ebmr_ridge[mprior] ypred = np.dot(Xtest, mres.mu) ax1.scatter(ytest, ypred, color = mcolors[mprior], alpha = 0.5) #ax2.scatter(np.arange(p), mres.mu, color = mcolors[mprior], alpha = 0.5) ax2.scatter(btrue, mres.mu, color = mcolors[mprior], alpha = 0.5) plot_diag(ax1) plot_diag(ax2) ax1.set_xlabel(&quot;y&quot;) ax1.set_ylabel(&quot;y_pred&quot;) ax2.set_xlabel(&quot;b&quot;) ax2.set_ylabel(&quot;b_pred&quot;) plt.tight_layout() plt.show() . Are the ELBOs correct? . To check if the ELBOs are correct, I compare the ELBO with the the marginal log likelihood $p left( mathbf{y} mid s^2, s_b^2 right)$ (also called the evidence), calculated at every step for the last 100 iterations. The ELBO is shown with the solid colored line (and points) while the evidence is the black dotted line. . fig = plt.figure(figsize = (18, 6)) ax = [None for mprior in priors] nstep = 100 for i, mprior in enumerate(priors): ax[i] = fig.add_subplot(1, 3, i+1) mres = ebmr_ridge[mprior] xvals = np.arange(mres.n_iter+1)[-nstep:] #ax[i].scatter(mres.elbo_path[2:], mres.mll_path[2:], color = mcolors[mprior], s = 20) ax[i].plot(xvals, mres.mll_path[-nstep:], color = &#39;black&#39;, ls=&#39;dotted&#39;, label = &quot;Evidence&quot;) ax[i].scatter(xvals, mres.elbo_path[-nstep:], color = mcolors[mprior], s=20) ax[i].plot(xvals, mres.elbo_path[-nstep:], color = mcolors[mprior], lw=1, label = &quot;ELBO&quot;) ax[i].text(0.7, 0.2, mlabels[mprior], transform=ax[i].transAxes) ax[i].set_xlabel(&quot;Iteration&quot;) ax[i].set_ylabel(&quot;ELBO / Evidence&quot;) plt.tight_layout() plt.show() . .",
            "url": "https://banskt.github.io/iridge-notes/jupyter/2020/12/14/ebmr_ridge_lasso_ash.html",
            "relUrl": "/jupyter/2020/12/14/ebmr_ridge_lasso_ash.html",
            "date": " • Dec 14, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Bayes Lasso using EBMR",
            "content": "About . A sanity check for the Bayes Lasso method using EBMR . import numpy as np import pandas as pd from scipy import linalg as sc_linalg import matplotlib.pyplot as plt import sys sys.path.append(&quot;../../ebmrPy/&quot;) from utils import log_density from inference import f_elbo from inference import penalized_em from inference.ebmr import EBMR import ipdb sys.path.append(&quot;../../utils/&quot;) import mpl_stylesheet mpl_stylesheet.banskt_presentation(fontfamily = &#39;latex-clearsans&#39;, fontsize = 18, colors = &#39;banskt&#39;, dpi = 72) . def standardize(X): Xnorm = (X - np.mean(X, axis = 0)) Xstd = Xnorm / np.sqrt((Xnorm * Xnorm).sum(axis = 0)) return Xstd def lasso_data(nsample, nvar, neff, errsigma, sb2 = 100, seed=200): np.random.seed(seed) X = np.random.normal(0, 1, nsample * nvar).reshape(nsample, nvar) X = standardize(X) btrue = np.zeros(nvar) bidx = np.random.choice(nvar, neff , replace = False) btrue[bidx] = np.random.normal(0, np.sqrt(sb2), neff) y = np.dot(X, btrue) + np.random.normal(0, errsigma, nsample) y = y - np.mean(y) #y = y / np.std(y) return X, y, btrue def lims_xy(ax): lims = [ np.min([ax.get_xlim(), ax.get_ylim()]), # min of both axes np.max([ax.get_xlim(), ax.get_ylim()]), # max of both axes ] return lims def plot_diag(ax): lims = lims_xy(ax) ax.plot(lims, lims, ls=&#39;dotted&#39;, color=&#39;gray&#39;) . n = 50 p = 100 peff = 10 sb2 = 100.0 sd = 2.0 X, y, btrue = lasso_data(n, p, peff, sd, sb2) . fig = plt.figure() ax1 = fig.add_subplot(111) ax1.scatter(np.dot(X,btrue), y) plot_diag(ax1) plt.show() . eblasso = EBMR(X, y, prior=&#39;dexp&#39;, grr=&#39;em&#39;, sigma=&#39;full&#39;, inverse=&#39;direct&#39;, max_iter = 1000, tol=1e-8) ebridge = EBMR(X, y, prior=&#39;point&#39;, grr=&#39;em&#39;, sigma=&#39;full&#39;, inverse=&#39;direct&#39;, max_iter = 1000, tol=1e-8) . 2020-12-01 12:55:43,172 | inference.ebmr | DEBUG | EBMR using dexp prior, em grr, full b posterior variance, direct inversion 2020-12-01 12:55:43,173 | inference.ebmr | DEBUG | EBMR using point prior, em grr, full b posterior variance, direct inversion . eblasso.update() ebridge.update() . data = {&#39;s2&#39;: [eblasso.s2, ebridge.s2], &#39;sb2&#39;: [eblasso.sb2, ebridge.sb2], &#39;s2 x sb2&#39;: [eblasso.s2 * eblasso.sb2, ebridge.s2 * ebridge.sb2], &#39;ELBO&#39;: [eblasso.elbo, ebridge.elbo], } resdf = pd.DataFrame.from_dict(data) resdf.index = [&#39;dexp&#39;, &#39;point&#39;] resdf.round(decimals=3) . s2 sb2 s2 x sb2 ELBO . dexp 0.975 | 6.257 | 6.102 | -158.132 | . point 4.243 | 1.092 | 4.633 | -139.934 | . eblasso.mll_path . array([ -inf, -162.5211116 , -159.397836 , -158.49845882, -158.25413555, -158.17937266, -158.15315927, -158.14270437, -158.13802668, -158.1358067 , -158.13452536, -158.13384434, -158.1334242 , -158.13317589, -158.13305632, -158.13294788, -158.1328467 , -158.13275135, -158.13266116, -158.13257572, -158.13249471, -158.13241787, -158.13234497, -158.13227578, -158.13221009, -158.13214772]) . eblasso.elbo_path . array([ -inf, -165.0350422 , -160.95177667, -159.17931935, -158.55105265, -158.31597417, -158.21862614, -158.17454958, -158.15315208, -158.1421157 , -158.13627118, -158.13311766, -158.13132908, -158.13045503, -158.13032989, -158.13007672, -158.12985772, -158.12969343, -158.1295773 , -158.12949848, -158.1294471 , -158.12941533, -158.12939738, -158.1293891 , -158.12938758, -158.12939083]) . fig = plt.figure() ax1 = fig.add_subplot(111) ypred_lasso = np.dot(X, eblasso.mu) ypred_ridge = np.dot(X, ebridge.mu) ax1.scatter(y, ypred_lasso, color=&#39;salmon&#39;) ax1.scatter(y, ypred_ridge, color=&#39;dodgerblue&#39;) plot_diag(ax1) plt.show() . fig = plt.figure() ax1 = fig.add_subplot(111) ax1.scatter(np.arange(p), btrue, color = &#39;black&#39;, s = 10) ax1.scatter(np.arange(p), eblasso.mu, color=&#39;salmon&#39;) ax1.scatter(np.arange(p), ebridge.mu, color=&#39;dodgerblue&#39;) plt.show() .",
            "url": "https://banskt.github.io/iridge-notes/jupyter/2020/12/01/lasso-ebmr.html",
            "relUrl": "/jupyter/2020/12/01/lasso-ebmr.html",
            "date": " • Dec 1, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Basic comparison of ridge regression methods",
            "content": "About . A sanity check that all ridge regression methods perform similarly. . import numpy as np import scipy from scipy import linalg as sc_linalg from scipy import sparse as sc_sparse from sklearn.linear_model import Ridge import glmnet_python from glmnet import glmnet from glmnetPrint import glmnetPrint from glmnetCoef import glmnetCoef from glmnetPredict import glmnetPredict import matplotlib.pyplot as plt import sys sys.path.append(&quot;../../utils/&quot;) import mpl_stylesheet mpl_stylesheet.banskt_presentation(fontfamily = &#39;latex-clearsans&#39;, fontsize = 18, colors = &#39;banskt&#39;, dpi = 72) . ../../utils/mpl_stylesheet.py:26: MatplotlibDeprecationWarning: Support for setting the &#39;text.latex.preamble&#39; or &#39;pgf.preamble&#39; rcParam to a list of strings is deprecated since 3.3 and will be removed two minor releases later; set it to a single string instead. matplotlib.rcParams[&#39;text.latex.preamble&#39;] = [r&#39; usepackage[scaled=.86]{ClearSans}&#39;, . def standardize(X): Xnorm = (X - np.mean(X, axis = 0)) #Xstd = Xnorm / np.std(Xnorm, axis = 0) Xstd = Xnorm / np.sqrt((Xnorm * Xnorm).sum(axis = 0)) return Xstd def ridge_data(nsample, nvar, errsigma): X = np.random.normal(0, 1, nsample * nvar).reshape(nsample, nvar) X = standardize(X) btrue = np.random.normal(0, 1, nvar) y = np.dot(X, btrue) + np.random.normal(0, errsigma, nsample) y = y - np.mean(y) y = y / np.std(y) return X, y, btrue . def rsquare(ytrue, ypred): sst = np.sum(np.square(ytrue - np.mean(ytrue))) sse = np.sum(np.square(ytrue - ypred)) rsq = 1 - (sse / sst) return rsq . def logpdf_multivariate_gauss(x, mu, cov): &#39;&#39;&#39; Caculate the multivariate normal density (pdf) Keyword arguments: x = numpy array of a &quot;d x 1&quot; sample vector mu = numpy array of a &quot;d x 1&quot; mean vector cov = &quot;numpy array of a d x d&quot; covariance matrix &#39;&#39;&#39; assert(mu.shape[0] &gt; mu.shape[1]), &#39;mu must be a row vector&#39; assert(x.shape[0] &gt; x.shape[1]), &#39;x must be a row vector&#39; assert(cov.shape[0] == cov.shape[1]), &#39;covariance matrix must be square&#39; assert(mu.shape[0] == cov.shape[0]), &#39;cov_mat and mu_vec must have the same dimensions&#39; assert(mu.shape[0] == x.shape[0]), &#39;mu and x must have the same dimensions&#39; part1 = - nsample * 0.5 * np.log(2. * np.pi) - 0.5 * np.linalg.slogdet(cov)[1] xlm = x - mu part2 = - 0.5 * np.dot(xlm.T, np.dot(np.linalg.inv(cov), xlm)) return float(part1 + part2) def ridge_em(X, Y, s2, sb2, niter = 10): XTX = np.dot(X.T, X) XTY = np.dot(X.T, Y) YTY = np.dot(Y.T, Y) nsample = X.shape[0] nvar = X.shape[1] loglik = np.zeros(niter) i = 0 while i &lt; niter: V = XTX + np.eye(nvar) * (s2 / sb2) Vinv = sc_linalg.cho_solve(sc_linalg.cho_factor(V, lower=True), np.eye(nvar)) SigmaY = sb2 * np.dot(X, X.T) + np.eye(nsample) * s2 loglik[i] = logpdf_multivariate_gauss(Y.reshape(-1, 1), np.zeros((nsample, 1)), SigmaY) Sigmab = s2 * Vinv # posterior variance of b mub = np.dot(Vinv, XTY) # posterior mean of b b2m = np.einsum(&#39;i,j-&gt;ij&#39;, mub, mub) + Sigmab s2 = (YTY + np.dot(XTX, b2m).trace() - 2 * np.dot(XTY, mub)) / nsample sb2 = np.sum(np.square(mub) + np.diag(Sigmab)) / nvar i += 1 return s2, sb2, loglik, mub.reshape(-1), Sigmab . def ridge_ols(X, Y, lmbda): XTX = np.dot(X.T, X) XTY = np.dot(X.T, Y) nvar = X.shape[1] V = XTX + np.eye(nvar) * lmbda Vinv = sc_linalg.cho_solve(sc_linalg.cho_factor(V, lower=True), np.eye(nvar)) bhat = np.dot(Vinv, XTY) return bhat . def svd2XTX(svd): U = svd[0] S = svd[1] Vh = svd[2] nmax = max(S.shape[0], Vh.shape[0]) Sdiag = np.zeros((nmax, nmax)) Sdiag[np.diag_indices(S.shape[0])] = np.square(S) return np.dot(Vh.T, np.dot(Sdiag, Vh)) def c_func(nsample, s2, ElogW): val = - 0.5 * nsample * np.log(2. * np.pi * s2) val += - 0.5 * np.sum(ElogW) return val def h1_func(X, Y, s2, mu, Wbar): val = - (0.5 / s2) * (np.sum(np.square(Y - np.dot(X, mu))) + np.sum(np.square(mu) / Wbar)) return val def h2_func(svd, Sigma, Wbar): XTX = svd2XTX(svd) (sign, logdet) = np.linalg.slogdet(Sigma) val = - 0.5 * np.trace(np.dot(XTX + np.diag(1 / Wbar), Sigma)) + 0.5 * logdet return val def ebmr_initialize(X, Y): svd = sc_linalg.svd(X) XTY = np.dot(X.T, Y) mu = np.zeros(nvar) Sigma = np.zeros((nvar, nvar)) return svd, XTY, mu, Sigma def update_Sigma(svd, Wbar, nvar): XTX = svd2XTX(svd) Sigma = sc_linalg.cho_solve(sc_linalg.cho_factor(XTX + np.diag(1 / Wbar), lower=True), np.eye(nvar)) return Sigma def update_mu(Sigma, XTY): return np.dot(Sigma, XTY) def update_s2(X, Y, mu, Wbar, nsample): A = np.sum(np.square(Y - np.dot(X, mu))) s2 = (A + np.sum(np.square(mu) / Wbar)) / nsample return s2 def update_wg_ridge(mu, Sigma, s2, nvar): bj2 = np.square(mu) + np.diag(Sigma) * s2 W = np.repeat(np.sum(bj2) / s2 / nvar, nvar) KLW = 0. return W, KLW def update_elbo(X, Y, s2, mu, Sigma, Wbar, KLw, svd, nsample, nvar): ElogW = np.log(Wbar) elbo = c_func(nsample, s2, ElogW) + h1_func(X, Y, s2, mu, Wbar) + h2_func(svd, Sigma, Wbar) + KLw return elbo def ebmr(X, Y, niter = 10, tol = 1e-4): nvar = X.shape[1] nsample = X.shape[0] svdX, XTY, mu, Sigma = ebmr_initialize(X, Y) s2 = np.var(Y) Wbar = np.ones(nvar) elbo = -np.inf i = 0 while i &lt; niter: #print(i) #Sigma = update_Sigma(svdX, Wbar, nvar) XTX = svd2XTX(svdX) Sigma = sc_linalg.cho_solve(sc_linalg.cho_factor(XTX + np.diag(1 / Wbar), lower=True), np.eye(nvar)) mu = update_mu(Sigma, XTY) s2 = update_s2(X, Y, mu, Wbar, nsample) Wbar, KLw = update_wg_ridge(mu, Sigma, s2, nvar) elbo_new = update_elbo(X, Y, s2, mu, Sigma, Wbar, KLw, svdX, nsample, nvar) if elbo_new - elbo &lt; tol: break elbo = elbo_new i += 1 return s2, mu, Sigma, Wbar . nsample = 50 nvar = 100 nsim = 20 errsigmas = np.logspace(-0.1, 1, 5) r2 = [None for i in errsigmas] for i, sd in enumerate(errsigmas): lmbda = np.square(sd) r2[i] = dict() r2[i][&#39;ridge_mle&#39;] = list() r2[i][&#39;ridge_em&#39;] = list() r2[i][&#39;ebmr&#39;] = list() r2[i][&#39;sklearn&#39;] = list() r2[i][&#39;sp_lsqr&#39;] = list() r2[i][&#39;glmnet&#39;] = list() for isim in range(nsim): X, y, btrue = ridge_data(nsample, nvar, sd) # Ridge_OLS b_ridge_ols = ridge_ols(X, y, lmbda) y_ridge_ols = np.dot(X, b_ridge_ols) r2[i][&#39;ridge_mle&#39;].append(rsquare(y, y_ridge_ols)) #r2[i][&#39;ridge_ols&#39;].append(y_ridge_ols) #r2[i][&#39;ridge_ols&#39;].append(np.square(y - y_ridge_ols)) #r2[i][&#39;ridge_ols&#39;].append(y) # Ridge EM _, _, _, b_ridge_em, _ = ridge_em(X, y, 1, 1, 500) r2[i][&#39;ridge_em&#39;].append(rsquare(y, np.dot(X, b_ridge_em))) # EBMR _, b_ebmr, _, _ = ebmr(X, y, 1000) y_ebmr = np.dot(X, b_ebmr) r2[i][&#39;ebmr&#39;].append(rsquare(y, y_ebmr)) #Sklearn Ridge clf = Ridge(alpha=lmbda, fit_intercept = False, normalize = False, solver = &#39;sparse_cg&#39;) clf.fit(X, y) b_sklearn = clf.coef_ y_sklearn = np.dot(X, b_sklearn) r2[i][&#39;sklearn&#39;].append(rsquare(y, y_sklearn)) #Sparse Lsqr b_sp_lsqr = sc_sparse.linalg.lsqr(X, y, damp=np.sqrt(lmbda))[0] #b_sp_lsqr = my_lsqr(X, y, damp=1.0)[0] y_sp_lsqr = np.dot(X, b_sp_lsqr) r2[i][&#39;sp_lsqr&#39;].append(rsquare(y, y_sp_lsqr)) #r2[i][&#39;sp_lsqr&#39;].append(y_sp_lsqr) #r2[i][&#39;sp_lsqr&#39;].append(np.square(y - y_sp_lsqr)) #r2[i][&#39;sp_lsqr&#39;].append(y) #glmnet lmbda_glmnet = lmbda / X.shape[0] fit = glmnet(x = X.copy(), y = y.copy(), family = &#39;gaussian&#39;, alpha = 0.0, intr = False, standardize = False, lambdau = np.array([lmbda_glmnet, 1.0])) b_glmnet = glmnetCoef(fit, s = np.float64([lmbda_glmnet]), exact = False)[1:].reshape(-1) y_glmnet = np.dot(X, b_glmnet) r2[i][&#39;glmnet&#39;].append(rsquare(y, y_glmnet)) #r2[i][&#39;glmnet&#39;].append(y_glmnet) #r2[i][&#39;glmnet&#39;].append(np.square(y - y_glmnet)) #r2[i][&#39;glmnet&#39;].append(y) . fig = plt.figure(figsize = (16,6)) ax1 = fig.add_subplot(111) colors = {&#39;ridge_em&#39;: &#39;#2D69C4&#39;, &#39;ebmr&#39;: &#39;#CC2529&#39;, &#39;sklearn&#39;: &#39;#93AA00&#39;, &#39;sp_lsqr&#39;: &#39;#535154&#39;, &#39;glmnet&#39;: &#39;#6B4C9A&#39;, &#39;ridge_mle&#39;: &#39;#FFB300&#39;} facecolors = {&#39;ridge_em&#39;: &#39;#719ad8&#39;, &#39;ebmr&#39;: &#39;#f2888b&#39;, &#39;sklearn&#39;: &#39;#c4d64f&#39;, &#39;sp_lsqr&#39;: &#39;#a6a3a7&#39;, &#39;glmnet&#39;: &#39;#a98fd2&#39;, &#39;ridge_mle&#39;: &#39;#fbd67e&#39;} barwidth = 0.1 nsigma = len(errsigmas) xpos = [(k+1)*2 for k in range(nsigma)] plot_methods = [&#39;ridge_mle&#39;, &#39;sklearn&#39;, &#39;sp_lsqr&#39;, &#39;ridge_em&#39;, &#39;ebmr&#39;, &#39;glmnet&#39;] bxplt = [None for x in plot_methods] for i, method in enumerate(plot_methods): #for i, method in enumerate([&#39;ridge_ols&#39;, &#39;sp_lsqr&#39;, &#39;glmnet&#39;]): #pdata = [np.hstack(r2[k][method]) for k in range(nsigma)] pdata = [r2[k][method] for k in range(nsigma)] xloc = [x + (i * barwidth) + (i * barwidth / 3) for x in xpos] medianprops = dict(linewidth=2, color = colors[method]) whiskerprops = dict(linewidth=2, color = facecolors[method]) boxprops = dict(linewidth=2, color = colors[method], facecolor = facecolors[method]) bxplt[i] = ax1.boxplot(pdata, positions = xloc, showfliers = False, showcaps = False, widths=barwidth, patch_artist=True, notch = False, boxprops = boxprops, medianprops = medianprops, whiskerprops = whiskerprops, ) leghandles = [x[&quot;boxes&quot;][0] for x in bxplt] ax1.legend(leghandles, plot_methods, loc=&#39;lower left&#39;, handlelength = 1.2, labelspacing = 0.2,) ax1.set_xticks(xpos) ax1.set_xticklabels([f&#39;{x:.2f}&#39; for x in errsigmas]) ax1.set_xlim(min(xpos) - 1, max(xpos) + 1) ax1.set_xlabel(r&#39;Prior $ sigma$&#39;) ax1.set_ylabel(r&#39;$R^2$&#39;) ax1.set_title(r&#39;n = 50, p = 100, fixed $ lambda$ estimated from prior&#39;) #plt.savefig(&#39;compare_ridge_methods.png&#39;, bbox_inches=&#39;tight&#39;, facecolor=&#39;white&#39;, transparent=True) plt.tight_layout() plt.show() . def jitter(arr): stdev = .1 * (max(arr) - min(arr)) return arr + abs(np.random.randn(len(arr)) * stdev) fig = plt.figure(figsize = (8, 8)) ax1 = fig.add_subplot(111) nshow = 2 for i, method in enumerate(plot_methods): ydata = r2[nshow][method] if method == &#39;sklearn&#39; or method == &#39;sp_lsqr&#39; or method == &#39;glmnet&#39;: ydata = jitter(ydata) ax1.scatter(r2[nshow][&#39;ridge_mle&#39;], ydata, color = colors[method]) ax1.set_title(f&#39;Comparison of $R^2$ ($ sigma$ = {errsigmas[nshow]:.2f})&#39;, pad = 20) ax1.set_xlabel(&#39;ridge_mle&#39;) ax1.set_ylabel(&#39;All ridge regression methods&#39;) #ax1.set_xticks([0.03, 0.04, 0.05]) ax1.set_xlim([0.25, 0.45]) ax1.set_ylim([0, 1.05]) ax1.plot([0,1],[0,1], ls = &#39;dashed&#39;, color = &#39;gray&#39;) #plt.savefig(&#39;compare_ridge_methods_scatter.png&#39;, bbox_inches=&#39;tight&#39;, facecolor=&#39;white&#39;, transparent=True) plt.show() .",
            "url": "https://banskt.github.io/iridge-notes/jupyter/2020/11/02/basic-comparison-ridge-regression-methods.html",
            "relUrl": "/jupyter/2020/11/02/basic-comparison-ridge-regression-methods.html",
            "date": " • Nov 2, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://banskt.github.io/iridge-notes/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://banskt.github.io/iridge-notes/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://banskt.github.io/iridge-notes/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}